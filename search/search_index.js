var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Docs","text":"<p>This site contains the project documentation and serves as your starting point for working with the <code>TAInstCalorimetry</code> packaga made available  on PyPI.</p>"},{"location":"index.html#table-of-contents","title":"Table Of Contents","text":"<p>The documentation consists of the following parts:</p> <ul> <li>How to Plot</li> <li>How to do a Tian Correction</li> <li>How to calculate characteristic values</li> <li>An Older How-To Guide</li> <li>Reference</li> </ul> Warning <p><code>CaloCem</code> has been developed without involvement of TA Instruments and is thus independent from the company  and its software.</p>"},{"location":"how-to-guide.html","title":"How-To Guides","text":""},{"location":"how-to-guide.html#interfacing-with-experimental-results-file-from-tam-air-calorimeters-made-easy","title":"Interfacing with experimental results file from TAM Air calorimeters made easy.","text":"<p>After collecting multiple experimental results files from a TAM Air calorimeter you will be left with multiple .xls-files obtained as exports from the device control software. To achieve a side by side comparison of theses results and some basic extraction of relevant parameters, TAInstCalorimetry is here to get this done smoothly.</p> <p>Note: TAInstCalorimetry has been developed without involvement of TA Instruments and is thus independent from the company and its software.</p>"},{"location":"how-to-guide.html#info-downloads","title":"Info / Downloads","text":""},{"location":"how-to-guide.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Example Usage</li> <li>Basic plotting</li> <li>Getting cumulated heat values</li> <li>Identifying peaks</li> <li>Identifying peak onsets</li> <li>Plotting by Category</li> <li>Installation</li> <li>Contributing</li> </ul>"},{"location":"how-to-guide.html#example-usage","title":"Example Usage","text":"<p>Import the <code>tacalorimetry</code> module from TAInstCalorimetry.</p> <pre><code># import\nimport os\nfrom calocem import tacalorimetry\n</code></pre> <p>Next, we define where the exported files are stored. With this information at hand, a <code>Measurement</code> is initialized. Experimental raw data and the metadata passed in the course of the measurement are retrieved by the methods <code>get_data()</code> and <code>get_information()</code>, respectively.</p> <pre><code># define data path\n# \"mycalodata\" is the subfoldername where the calorimetry\n# data files (both .csv or .xlsx) are stored\n\npathname = os.path.dirname(os.path.realpath(__file__))\npath_to_data = pathname + os.sep + \"mycalodata\"\n\n# Example: if projectfile is at \"C:\\Users\\myname\\myproject\\myproject.py\", then \"mydata\"\n# refers to \"C:\\Users\\myname\\myproject\\mycalodata\" where the data is stored\n\n# load experiments via class, i.e. instantiate tacalorimetry object with data\ntam = tacalorimetry.Measurement(folder=path_to_data)\n\n# get sample and information\ndata = tam.get_data()\ninfo = tam.get_information()\n</code></pre>"},{"location":"how-to-guide.html#basic-plotting","title":"Basic plotting","text":"<p>Furthermore, the <code>Measurement</code> features a <code>plot()</code>-method for readily visualizing the collected results.</p> <pre><code># make plot\ntam.plot()\n# show plot\ntacalorimetry.plt.show()\n</code></pre> <p>Without further options specified, the <code>plot()</code>-method yields the following.</p> <p></p> <p>The <code>plot()</code>-method can also be tuned to show the temporal course of normalized heat. On the one hand, this \"tuning\" refers to the specification of further keyword arguments such as <code>t_unit</code> and <code>y</code>. On the other hand, the <code>plot()</code>-method returns an object of type <code>matplotlib.axes._subplots.AxesSubplot</code>, which can be used to further customize the plot. In the following, a guide-to-the-eye line is introduced next to adjuting the axes limts, which is not provided for via the <code>plot()</code>-method's signature.</p> <p><pre><code># show cumulated heat plot\nax = tam.plot(\n    t_unit=\"h\",\n    y='normalized_heat',\n    y_unit_milli=False\n)\n\n# define target time\ntarget_h = 1.5\n\n# guide to the eye line\nax.axvline(target_h, color=\"gray\", alpha=0.5, linestyle=\":\")\n\n# set upper limits\nax.set_ylim(top=250)\nax.set_xlim(right=6)\n# show plot\ntacalorimetry.plt.show()\n</code></pre> The following plot is obtained:</p> <p></p>"},{"location":"how-to-guide.html#getting-cumulated-heat-values","title":"Getting cumulated heat values","text":"<p>The cumulated heat after a certain period of time <code>target_h</code> from starting the measurement is a relevant quantity for answering different types of questions. For this purpose, the method <code>get_cumulated_heat_at_hours</code> returns an overview of this parameter for all the samples in the specified folder.</p> <pre><code># get table of cumulated heat at certain age\ncumulated_heats = tam.get_cumulated_heat_at_hours(\n          target_h=target_h,\n          cutoff_min=10\n          )\n\n# show result\nprint(cumulated_heats)\n</code></pre> <p>The return value of the method, <code>cumulated_heats</code> is a <code>pd.DataFrame</code>.</p>"},{"location":"how-to-guide.html#identifying-peaks","title":"Identifying peaks","text":"<p>Next to cumulated heat values detected after a certain time frame from starting the reaction, peaks characteristics can be obtained from the experimental data via the <code>get_peaks</code>-method.</p> <pre><code># get peaks\npeaks = tam.get_peaks(\n    show_plot=True,\n    prominence=0.00001,  # \"sensitivity of peak picking\"\n    cutoff_min=60,  # how much to discard at the beginning of the measurement\n    plt_right_s=4e5,\n    plt_top=1e-2,\n    regex=\".*_\\d\"  # filter samples\n    )\n</code></pre> <p>Tweaking some of the available keyword arguments, the following plot is obtained:</p> <p></p> <p>Please keep in mind, that in particular for samples of ordinary Portland cement (OPC) a clear and unambiguous identification/assigment of peaks remains a challenging task which cannot be achieved in each and every case by TAInstCalorimetry. It is left to the user draw meaningful scientific conclusions from the characteristics derived from this method.</p>"},{"location":"how-to-guide.html#identifying-peak-onsets","title":"Identifying peak onsets","text":"<p>Similarly, the peak onset characteristics are accessible via the <code>get_peak_onsets</code>-method. The resulting plot is shown below.</p> <p><pre><code># get onsets\nonsets = tam.get_peak_onsets(\n    gradient_threshold=0.000001,\n    rolling=10,\n    exclude_discarded_time=True,\n    show_plot=True,\n    regex=\"OPC\"\n)\n</code></pre> </p>"},{"location":"how-to-guide.html#plotting-by-category","title":"Plotting by Category","text":"<p>For introducing the idea of plotting calorimetry data \"by category\" another set of experimental data will be introduced. Next to the calorimetry data alone, information on investigated samples is supplied via an additional source file. In the present example via the file <code>mini_metadata.csv</code>.</p> <p>To begin with, a <code>TAInstCalorimetry.tacalorimetry.Measurement</code>-object is initialized for selected files from the specified <code>`path</code>.</p> <pre><code>import pathlib\nfrom CaloCem import tacalorimetry\n\n# path to experimental calorimetry files\npath = pathlib.Path().cwd().parent / \"CaloCem\" / \"DATA\"\n\n# initialize CaloCem.tacalorimetry.Measurement object\ntam_II = tacalorimetry.Measurement(\n    path, regex=\"myexp.*\", show_info=True, cold_start=True, auto_clean=False\n)\n</code></pre> <p>Next, we need to connect the previously defined object to our metadata provided by the <code>mini_metadata.csv</code>-file. To establish this mapping between experimental results and metadata, the file location, i.e. path, and the column name containing the exact(!) names of the calorimetry files needs to be passed to the <code>add_metadata_source</code>-method. In our case, we declare the column <code>experiment_nr</code> for this purpose</p> <pre><code># add metadata\ntam.add_metadata_source(\"mini_metadata.csv\", \"experiment_nr\")\n</code></pre> <p>Finally, a plotting by category can be carried out by one or multiple categories as shown in the following.</p> <pre><code># define action by one category\ncategorize_by = \"cement_name\"  # 'date', 'cement_amount_g', 'water_amount_g'\n\n# # define action by two or more categories\ncategorize_by = [\"date\", \"cement_name\"]\n\n# loop through plots via generator\nfor this_plot in tam.plot_by_category(categorize_by):\n    # extract parts obtained from generator\n    category_value, ax = this_plot\n    # fine tuning of plot/cosmetics\n    ax.set_ylim(0, 3)\n    # show plot\n    tacalorimetry.plt.show()\n</code></pre> <p>This yields plots of the following kind.</p> <p></p> <p></p>"},{"location":"how-to-guide.html#installation","title":"Installation","text":"<p>Use the package manager pip to install TAInstCalorimetry.</p> <pre><code>pip install CaloCem\n</code></pre>"},{"location":"how-to-guide.html#contributing","title":"Contributing","text":"<p>Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.</p> <p>Please make sure to update tests as appropriate.</p> <p>List of contributors - mj-hofmann - tgaedt</p>"},{"location":"how-to-guide.html#license","title":"License","text":"<p>GNU GPLv3</p>"},{"location":"how-to-guide.html#test","title":"Test","text":""},{"location":"how-to-guide.html#code-styling","title":"Code Styling","text":""},{"location":"plotting.html","title":"Plotting","text":"<p>There are many different use cases for isothermal calorimetry. Here, we focus on the application of isothermal heat flow calorimetry for the hydration of cementitious materials.</p>"},{"location":"plotting.html#basic-plotting-of-calorimetry-data","title":"Basic Plotting of Calorimetry Data","text":"<p>Assume that your calorimetry data is found inside a folder called <code>calo_data</code> and your Python script <code>myscript.py</code>is in the working directory. <pre><code>.\n\u251c\u2500\u2500 myscript.py\n\u2514\u2500\u2500 calo_data\n    \u251c\u2500\u2500 calofile1.csv\n    \u2514\u2500\u2500 calofile2.csv\n</code></pre></p> <p>It is very easy to load the calorimetry files and to plot them. The file <code>myscript.py</code> could read like this. First, we create a Path object <code>datapath</code> using the pathlib package that is directed at the folder which contains the raw instrument data (<code>calo_data</code> in this example). The advantage of using the <code>pathlib</code> package is that we do not have to worry if the user of our code is running Linux, MacOS, or Windows. The <code>Path()</code> object ensures that the path definition always works. In our example, <code>Path(__file__).parent</code> contains the absolute path to the folder in which the script (here <code>myscript.py</code>) is located, independent of the operating system. By writing <code>Path(__file__).parent / \"calo_data\"</code> we create a <code>Path()</code> object which contains the absolute path to <code>calo_data</code>.</p> <p>After we have obtained the path, we pass it to <code>ta.Measurement()</code>. Besides the <code>Path</code> object, we can pass further arguments such as the option <code>show_info=True</code> which prints the names of the calo files being loaded in the terminal.</p> <p><pre><code>from calocem import tacalorimetry as ta\nfrom pathlib import Path\n\ndatapath = Path(__file__).parent / \"calo_data\"\n\n# create the calorimetry Measurement object\ntam = ta.Measurement(\n    folder=datapath,\n    show_info=True,\n    auto_clean=False,\n)\n\n# plot the data\ntam.plot()\n</code></pre> This would yield something like the following plot:</p> <p></p> <p>The plot has at least three issues:</p> <ul> <li>the y-axis and the x-axis are automatically scaled to include the maximumum values</li> <li>the legend is not visible</li> <li>by default the plot method plots the normalized heat flow in mW/g, maybe another parameters is desired</li> </ul>"},{"location":"plotting.html#customizing-the-plot","title":"Customizing the plot","text":""},{"location":"plotting.html#choosing-different-variables-for-the-y-axis","title":"Choosing different variables for the y-axis","text":"<p>If only a different y-axis variable is desired, this can simply be achieved by defining the name of the desired parameter:</p> <pre><code>tam.plot(y=\"heat_j_g\")\n</code></pre>"},{"location":"plotting.html#full-customization","title":"Full customization","text":"<p>The <code>plot()</code> method returns a Matplotlib axes object.  Therefore, we can manipulate the plot as normal, e.g., by defining the limits of both axes or by defining the location of the legend (as shown in the code below).</p> <pre><code>ax = tam.plot(\n    y=\"normalized_heat_flow_w_g\",\n    t_unit=\"h\",  # time axis in hours\n    y_unit_milli=True,\n)\n\n# set upper limits\nax.set_ylim(0, 6)\nax.set_xlim(0, 48)\nax.legend(bbox_to_anchor=(1., 1), loc=\"upper right\")\n</code></pre>"},{"location":"plotting.html#plotting-heat-flow-and-heat-in-subplots","title":"Plotting Heat Flow and Heat in Subplots","text":"<p>Often, both the initial phase of hydration is of interest and also both the heat flow and the heat are relevant.  Here is code which allows plotting such data to a neat 2x2 grid.</p> <pre><code>plot_configs = [\n    {\"ycol\": \"normalized_heat_flow_w_g\", \"xlim\": 1, \"ylim\": 0.05},\n    {\"ycol\": \"normalized_heat_flow_w_g\", \"xlim\": 48, \"ylim\": 0.005},\n    {\"ycol\": \"normalized_heat_j_g\", \"xlim\": 1, \"ylim\": 30},\n    {\"ycol\": \"normalized_heat_j_g\", \"xlim\": 48, \"ylim\": 300},\n]\n\nfig, axs = ta.plt.subplots(2, 2, layout=\"constrained\")\nfor ax, config in zip(axs.flatten(), plot_configs):\n    tam.plot(y=config[\"ycol\"], t_unit=\"h\", y_unit_milli=False, ax=ax)\n    ax.set_xlim(0, config[\"xlim\"])\n    ax.set_ylim(0, config[\"ylim\"])\n    ax.get_legend().remove()\nta.plt.show()\n</code></pre> <p></p>"},{"location":"plotting_example.html","title":"Plotting Example","text":"In\u00a0[16]: Copied! <pre>import matplotlib.pyplot as plt\nfrom calocem.tacalorimetry import Measurement\n\nfrom pathlib import Path\n\n# here you need to define the suitable path to the data\n# this reads a little cryptic because it is a relative path from the docs folder of the repo\ndatapath = Path().cwd().parent / \"calocem\" / \"DATA\"\n\ntam = Measurement(datapath, regex=\".*bm2.csv\")\n\nfig, ax = plt.subplots()\ntam.plot(ax=ax)\nax.set_xlim(0, 30)\nax.set_ylim(0, 5)\nax.set_title(\"TACalorimetry Measurement\")\nplt.show()\n</pre> import matplotlib.pyplot as plt from calocem.tacalorimetry import Measurement  from pathlib import Path  # here you need to define the suitable path to the data # this reads a little cryptic because it is a relative path from the docs folder of the repo datapath = Path().cwd().parent / \"calocem\" / \"DATA\"  tam = Measurement(datapath, regex=\".*bm2.csv\")  fig, ax = plt.subplots() tam.plot(ax=ax) ax.set_xlim(0, 30) ax.set_ylim(0, 5) ax.set_title(\"TACalorimetry Measurement\") plt.show() <pre>Reading insitu_bm2.csv.\n================\nAre you missing some samples? Try rerunning with auto_clean=True and cold_start=True.\n=================\n</pre> In\u00a0[\u00a0]: Copied! <pre>tam = Measurement(datapath, regex=\".*bm.*\")\n\nplot_configs = [\n    {\"ycol\": \"normalized_heat_flow_w_g\", \"xlim\": 1, \"ylim\": 0.050},\n    {\"ycol\": \"normalized_heat_flow_w_g\", \"xlim\": 48, \"ylim\": 0.005},\n    {\"ycol\": \"normalized_heat_j_g\", \"xlim\": 1, \"ylim\": 30},\n    {\"ycol\": \"normalized_heat_j_g\", \"xlim\": 48, \"ylim\": 300},\n]\n\nfig, axs = plt.subplots(2, 2, layout=\"constrained\")\nfor ax, config in zip(axs.flatten(), plot_configs):\n    tam.plot(y=config[\"ycol\"], t_unit=\"h\", y_unit_milli=False, ax=ax)\n    ax.set_xlim(0, config[\"xlim\"])\n    ax.set_ylim(0, config[\"ylim\"])\n    ax.get_legend().remove()\nplt.show()\n</pre>  tam = Measurement(datapath, regex=\".*bm.*\")  plot_configs = [     {\"ycol\": \"normalized_heat_flow_w_g\", \"xlim\": 1, \"ylim\": 0.050},     {\"ycol\": \"normalized_heat_flow_w_g\", \"xlim\": 48, \"ylim\": 0.005},     {\"ycol\": \"normalized_heat_j_g\", \"xlim\": 1, \"ylim\": 30},     {\"ycol\": \"normalized_heat_j_g\", \"xlim\": 48, \"ylim\": 300}, ]  fig, axs = plt.subplots(2, 2, layout=\"constrained\") for ax, config in zip(axs.flatten(), plot_configs):     tam.plot(y=config[\"ycol\"], t_unit=\"h\", y_unit_milli=False, ax=ax)     ax.set_xlim(0, config[\"xlim\"])     ax.set_ylim(0, config[\"ylim\"])     ax.get_legend().remove() plt.show() <pre>Reading insitu_bm2.csv.\nReading insitu_bm.csv.\n================\nAre you missing some samples? Try rerunning with auto_clean=True and cold_start=True.\n=================\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"plotting_example.html#plotting-example","title":"Plotting Example\u00b6","text":"<p>This is written as a Jupyter Notebook inside the  <code>docs</code> folder of the repository.</p> <p>The <code>Measurement</code> class loads the calorimetry data. You need to pass a folder name. Ideally a pathlib <code>Path</code> object containing the correct path is passed to <code>Measurement</code>. By default, the <code>Measurement</code> will load all data files contained in the folder that is passed. If you want to only load a specific file, you can pass a regex filter (as shown in the example below).</p> <p>The <code>Measurement</code> object has several methods. Here we use the <code>plot</code> method to create the heat flow plot. If you create an axis object before, the method will draw the plot onto the axis object if it is passed (see the example below). The axis object can then be modified with the methods of Matplotlib.</p>"},{"location":"plotting_example.html#folder-definition","title":"Folder Definition\u00b6","text":"<p>If your folder looks like this, and your script is <code>myscript.py</code></p> <pre>.\n\u251c\u2500\u2500 myscript.py\n\u2514\u2500\u2500 calo_data\n    \u251c\u2500\u2500 calofile1.csv\n    \u2514\u2500\u2500 calofile2.csv\n</pre> <p>then the pathlib definition in <code>myscript.py</code> would look like</p> <pre>from pathlib import Path\ndatapath = Path(__file__).parent / \"calo_data\"\n</pre> <p>Here in the example above, however, the Jupyter Notebook is located in a special location, therefore the path looks a bit cryptic.</p>"},{"location":"plotting_example.html#customizing-the-plot","title":"Customizing the plot\u00b6","text":"<p>Often we load more than one calorimetry experiment. In the following example, two files are loaded. Additionally, we want to plot the data such that we both see the very early heat development and the later heat development. We combine Matplotlib with CaloCem.</p>"},{"location":"quantification.html","title":"Quantification of Calorimetry Data","text":""},{"location":"quantification.html#peak-detection","title":"Peak Detection","text":"<p>First we load the data</p> <p><pre><code># %%\nfrom pathlib import Path\nimport calocem.tacalorimetry as ta\nfrom calocem.processparams import ProcessingParameters\n\n# define the Path of the folder with the calorimetry data\ndatapath = Path()\n\n# experiments via class\ntam = ta.Measurement(\n    folder=datapath,\n    regex=r\".*file.*\",\n    show_info=True,\n    auto_clean=False,\n    cold_start=True,\n)\n</code></pre> Then, before we apply the <code>get_peaks()</code> method, we can define the ProcessingParameters if the default options are not suitable. Here we define the peak prominence and give it the value 1e-4. Note that only the largest peak will be detected if the prominence is set to 1e-3 in this example.</p> <p><pre><code># define the processing parameters\nprocessparams = ProcessingParameters()\nprocessparams.peakdetection.prominence = 1e-4\n\n# plot the peak position\nfig, ax = ta.plt.subplots()\n\n# get peaks (returns both a dataframe and extends the axes object)\npeaks_found = tam.get_peaks(processparams, plt_right_s=3e5, ax=ax, show_plot=True)\nax.set_xlim(0, 100000)\nta.plt.show()\n</code></pre> </p> <p><code>peaks_found</code> is a tuple.  The first element contains the Dataframe with the parameters of the detected peaks. It could be exported to a csv file via</p> <pre><code>df = peaks_found[0].iloc[:,[0,5,6,9]]\ndf.to_csv(plotpath / \"example_get_peaks.csv\", index=False)\n</code></pre> <p>The dataframe looks like this:</p> time_s normalized_heat_flow_w_g normalized_heat_j_g prominence 29565.2 0.00207652 75.7002 0.00109303 82364.8 0.00164736 162.984 0.000207758"},{"location":"quantification.html#maximum-slope-of-c3s-reaction-detection","title":"Maximum Slope (of C3S Reaction) Detection","text":"<p>Programmatic, automatic detection the maximum slope can be a little tricky. The first example is straightforward and looks a normal Portland cement hydration case. Assuming that the data is already loaded, we can inistantiate the ProcessingParameters object. The algorithm detects the maxima of the gradient of the heat flow. It is therefore very useful to apply a little smoothing to the first derivative.</p> <p><pre><code>processparams = ProcessingParameters()\nprocessparams.spline_interpolation.apply = True\nprocessparams.spline_interpolation.smoothing_1st_deriv = 1e-12\n\n# get peak onsets via alternative method\nfig, ax = ta.plt.subplots()\nonsets_spline = tam.get_maximum_slope(\n    processparams=processparams,\n    show_plot=True,\n    ax = ax\n)\n</code></pre> If we set the parameter <code>show_plot</code> to <code>True</code>, we get a nice visual feedback on the gradient and the detected maximum slope (as a green line). The gradient is multiplied by a factor of 10.000 and shifted upwards.</p> <p></p>"},{"location":"reference.html","title":"Reference","text":"<p>This part of the project documentation shows the technical implementation  of the <code>CaloCem</code> project code.</p> <p>Refactored main measurement class for calorimetry data handling.</p>"},{"location":"reference.html#calocem.tacalorimetry.DownSamplingParameters","title":"<code>DownSamplingParameters</code>  <code>dataclass</code>","text":"<p>Parameters for adaptive downsampling of the heat flow data.</p> <p>Parameters:</p> Name Type Description Default <code>apply</code> <code>bool</code> <p>If True, adaptive downsampling is applied to the heat flow data.</p> <code>False</code> <code>num_points</code> <code>int</code> <p>The target number of points after downsampling. Default is 1000.</p> <code>1000</code> <code>smoothing_factor</code> <code>float</code> <p>Smoothing factor used in the downsampling algorithm. Default is 1e-10.</p> <code>1e-10</code> <code>baseline_weight</code> <code>float</code> <p>Weight of the baseline in the downsampling algorithm. Default is 0.1.</p> <code>0.1</code> <code>section_split</code> <code>bool</code> <p>If True, the data is split into sections for downsampling. This can be useful if there is a narrow peak in the data early on. Default is False.</p> <code>False</code> <code>section_split_time_s</code> <code>int</code> <p>Time in seconds for splitting the data into two sections. Default is 1000 seconds.</p> <code>1000</code> Source code in <code>calocem/processparams.py</code> <pre><code>@dataclass\nclass DownSamplingParameters:\n    \"\"\"\n    Parameters for adaptive downsampling of the heat flow data.\n\n    Parameters\n    ----------\n    apply: bool\n        If True, adaptive downsampling is applied to the heat flow data.\n    num_points: int\n        The target number of points after downsampling. Default is 1000.\n    smoothing_factor: float\n        Smoothing factor used in the downsampling algorithm. Default is 1e-10.\n    baseline_weight: float\n        Weight of the baseline in the downsampling algorithm. Default is 0.1.\n    section_split: bool\n        If True, the data is split into sections for downsampling. This can be useful if there is a narrow peak in the data early on. Default is False.\n    section_split_time_s: int\n        Time in seconds for splitting the data into two sections. Default is 1000 seconds.\n    \"\"\"\n    apply: bool = False\n    num_points: int = 1000\n    smoothing_factor: float = 1e-10\n    baseline_weight: float = 0.1\n    section_split: bool = False\n    section_split_time_s: int = 1000\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.GradientPeakDetectionParameters","title":"<code>GradientPeakDetectionParameters</code>  <code>dataclass</code>","text":"<p>Parameters that control the identifcation of Peaks in the first derivative (gradient) of the heat flow data. Under the hood the SciPy <code>find_peaks()</code> is used Link to SciPy method.</p> <p>Parameters:</p> Name Type Description Default <code>prominence</code> <code>float</code> <p>Minimum prominence of the peak</p> <code>1e-09</code> <code>distance</code> <code>int</code> <p>Minimum distance</p> <code>100</code> <code>width</code> <code>int</code> <p>Minimum width</p> <code>20</code> <code>rel_height</code> <code>float</code> <p>Relative height of the peak</p> <code>0.05</code> <code>height</code> <code>float</code> <p>Minimum height of the peak</p> <code>1e-09</code> <code>use_first</code> <code>bool</code> <p>If true, only the first peak will be used</p> <code>False</code> <code>use_largest_width</code> <code>bool</code> <p>If true the peak with the largest peak width will be used.</p> <code>False</code> Source code in <code>calocem/processparams.py</code> <pre><code>@dataclass\nclass GradientPeakDetectionParameters:\n    \"\"\"\n    Parameters that control the identifcation of Peaks in the first derivative (gradient) of the heat flow data. Under the hood the SciPy `find_peaks()` is used [Link to SciPy method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html).\n\n    Parameters\n    ----------\n    prominence: float\n        Minimum prominence of the peak\n    distance: int\n        Minimum distance\n    width: int\n        Minimum width\n    rel_height: float\n        Relative height of the peak\n    height: float\n        Minimum height of the peak\n    use_first: bool\n        If true, only the first peak will be used\n    use_largest_width: bool\n        If true the peak with the largest peak width will be used.\n    \"\"\"\n\n    prominence: float = 1e-9\n    distance: int = 100\n    width: int = 20\n    rel_height: float = 0.05\n    height: float = 1e-9\n    use_first: bool = False\n    use_largest_width: bool = False\n    use_largest_width_height: bool = False\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement","title":"<code>Measurement</code>","text":"<p>A base class for handling and processing isothermal heat flow calorimetry data.</p> <p>Currently supported file formats are .xls and .csv files. Only TA Instruments data files are supported at the moment.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>path to folder containing .xls and/or .csv experimental result files. The default is None.</p> <code>None</code> <code>show_info</code> <code>bool</code> <p>whether or not to print some informative lines during code execution. The default is True.</p> <code>True</code> <code>regex</code> <code>str</code> <p>regex pattern to include only certain experimental result files during initialization. The default is None.</p> <code>None</code> <code>auto_clean</code> <code>bool</code> <p>whether or not to exclude NaN values contained in the original files and combine data from differently names temperature columns. The default is False.</p> <code>False</code> <code>cold_start</code> <code>bool</code> <p>whether or not to use \"pickled\" files for initialization; save time on reading</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import CaloCem as ta\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt;\n&gt;&gt;&gt; calodatapath = Path(__file__).parent\n&gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n</code></pre> <p>We can use a regex pattern to only include certain files in the datafolder. Here we assume that we only want to load .csv files which contain the string \"bm\".</p> <pre><code>&gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, regex=r\".*bm.*.csv\", show_info=True)\n</code></pre> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>class Measurement:\n    \"\"\"\n    A base class for handling and processing isothermal heat flow calorimetry data.\n\n    Currently supported file formats are .xls and .csv files.\n    Only TA Instruments data files are supported at the moment.\n\n    Parameters\n    ----------\n    folder : str, optional\n        path to folder containing .xls and/or .csv experimental result\n        files. The default is None.\n    show_info : bool, optional\n        whether or not to print some informative lines during code\n        execution. The default is True.\n    regex : str, optional\n        regex pattern to include only certain experimental result files\n        during initialization. The default is None.\n    auto_clean : bool, optional\n        whether or not to exclude NaN values contained in the original\n        files and combine data from differently names temperature columns.\n        The default is False.\n    cold_start : bool, optional\n        whether or not to use \"pickled\" files for initialization; save time\n        on reading\n\n    Examples\n    --------\n\n    &gt;&gt;&gt; import CaloCem as ta\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; calodatapath = Path(__file__).parent\n    &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n\n    We can use a regex pattern to only include certain files in the datafolder. Here we assume that we only want to load .csv files which contain the string \"bm\".\n\n    &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, regex=r\".*bm.*.csv\", show_info=True)\n\n    \"\"\"\n\n    # init\n    _info = pd.DataFrame()\n    _data = pd.DataFrame()\n    _data_unprocessed = (\n        pd.DataFrame()\n    )  # helper to store experimental data as loaded from files\n\n    # further metadata\n    _metadata = pd.DataFrame()\n    _metadata_id = \"\"\n\n    # define pickle filenames\n    _file_data_pickle = pathlib.Path().cwd() / \"_data.pickle\"\n    _file_info_pickle = pathlib.Path().cwd() / \"_info.pickle\"\n\n    #\n    # init\n    #\n    def __init__(\n        self,\n        folder=None,\n        show_info=True,\n        regex=None,\n        auto_clean=False,\n        cold_start=True,\n        processparams=None,\n        new_code=False,\n        processed=False,\n    ):\n        \"\"\"\n        intialize measurements from folder\n\n\n        \"\"\"\n        self._new_code = new_code\n        self._processed = processed\n\n        if not isinstance(processparams, ProcessingParameters):\n            self.processparams = ProcessingParameters()\n        else:\n            self.processparams = processparams\n\n        # read\n        if folder:\n            if cold_start:\n                # get data and parameters\n                self._get_data_and_parameters_from_folder(\n                    folder, regex=regex, show_info=show_info\n                )\n            else:\n                # get data and parameters from pickled files\n                self._get_data_and_parameters_from_pickle()\n            try:\n                if auto_clean:\n                    # remove NaN values and merge time columns\n                    self._auto_clean_data()\n            except Exception as e:\n                # info\n                print(e)\n                raise AutoCleanException\n                # return\n                return\n\n        if self.processparams.downsample.apply:\n            self._apply_adaptive_downsampling()\n        # Message\n        print(\n            \"================\\nAre you missing some samples? Try rerunning with auto_clean=True and cold_start=True.\\n=================\"\n        )\n\n    #\n    # get_data_and_parameters_from_folder\n    #\n    def _get_data_and_parameters_from_folder(self, folder, regex=None, show_info=True):\n        \"\"\"\n        get_data_and_parameters_from_folder\n        \"\"\"\n\n        if not isinstance(folder, str):\n            # convert\n            folder = str(folder)\n\n        # loop folder\n        for f in os.listdir(folder):\n            if not f.endswith((\".xls\", \".xlsx\", \".csv\")):\n                # go to next\n                continue\n\n            if regex:\n                # check match\n                if not re.match(regex, f):\n                    # skip this file\n                    continue\n\n            # info\n            if show_info:\n                print(f\"Reading {f}.\")\n\n            # define file\n            file = folder + os.sep + f\n\n            # check xls\n            if f.endswith(\".xls\") or f.endswith(\".xlsx\"):\n                if self._new_code:\n                    self._data = pd.concat(\n                        [\n                            self._data,\n                            self._read_csv_data(file, show_info=show_info),\n                        ]\n                    )\n                if self._new_code is False:\n                    # collect information\n                    try:\n                        self._info = pd.concat(\n                            [\n                                self._info,\n                                self._read_calo_info_xls(file, show_info=show_info),\n                            ]\n                        )\n                    except Exception:\n                        # initialize\n                        if self._info.empty:\n                            self._info = self._read_calo_info_xls(\n                                file, show_info=show_info\n                            )\n\n                    # collect data\n                    try:\n                        self._data = pd.concat(\n                            [\n                                self._data,\n                                self._read_calo_data_xls(file, show_info=show_info),\n                            ]\n                        )\n\n                    except Exception:\n                        # initialize\n                        if self._data.empty:\n                            self._data = self._read_calo_data_xls(\n                                file, show_info=show_info\n                            )\n\n            # append csv\n            if f.endswith(\".csv\"):\n                # collect data\n                if self._new_code:\n                    self._data = pd.concat(\n                        [\n                            self._data,\n                            self._read_csv_data(file, show_info=show_info),\n                        ]\n                    )\n                    # self._read_csv_data(file, show_info=show_info)\n                if self._new_code is False:\n                    try:\n                        self._data = pd.concat(\n                            [\n                                self._data,\n                                self._read_calo_data_csv(file, show_info=show_info),\n                            ]\n                        )\n\n                    except Exception:\n                        # initialize\n                        if self._data.empty:\n                            self._data = self._read_calo_data_csv(\n                                file, show_info=show_info\n                            )\n\n                # collect information\n                try:\n                    self._info = pd.concat(\n                        [\n                            self._info,\n                            self._read_calo_info_csv(file, show_info=show_info),\n                        ]\n                    )\n                except Exception:\n                    # initialize\n                    if self._info.empty:\n                        try:\n                            self._info = self._read_calo_info_csv(\n                                file, show_info=show_info\n                            )\n                        except Exception:\n                            pass\n\n        # get \"heat_j\" columns if the column is not part of the source files\n        if self.processparams.preprocess.infer_heat:\n            try:\n                self._infer_heat_j_column()\n            except Exception:\n                pass\n\n        # if self.processparams.downsample.apply is not False:\n        #     self._apply_adaptive_downsampling()\n        # write _data and _info to pickle\n        with open(self._file_data_pickle, \"wb\") as f:\n            pickle.dump(self._data, f)\n        with open(self._file_info_pickle, \"wb\") as f:\n            pickle.dump(self._info, f)\n\n        # store experimental data for recreating state after reading from files\n        self._data_unprocessed = self._data.copy()\n\n    #\n    # get data and information from pickled files\n    #\n    def _get_data_and_parameters_from_pickle(self):\n        \"\"\"\n        get data and information from pickled files\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # read from pickle\n        try:\n            self._data = pd.read_pickle(self._file_data_pickle)\n            self._info = pd.read_pickle(self._file_info_pickle)\n            # store experimental data for recreating state after reading from files\n            self._data_unprocessed = self._data.copy()\n        except FileNotFoundError:\n            # raise custom Exception\n            raise ColdStartException()\n\n        # log\n        logging.info(\"_data and _info loaded from pickle files.\")\n\n    #\n    # determine csv data range\n    #\n    def _determine_data_range_csv(self, file):\n        \"\"\"\n        determine csv data range of CSV-file.\n\n        Parameters\n        ----------\n        file : str\n            filepath.\n\n        Returns\n        -------\n        empty_lines : TYPE\n            DESCRIPTION.\n\n        \"\"\"\n        # open csv file\n        thefile = open(file)\n        # detect empty lines which are characteristic at the beginning and\n        # end of the data block\n        empty_lines = [\n            index for index, line in enumerate(csv.reader(thefile)) if len(line) == 0\n        ]\n        return empty_lines\n\n    def _read_csv_data(self, file, show_info=True):\n        \"\"\"\n        NEW IMPLEMENTATION\n        \"\"\"\n        filetype = pathlib.Path(file).suffix\n        if not self._processed:\n            if filetype == \".csv\":\n                delimiter = utils.detect_delimiter(file)\n                title_row = utils.find_title_row(file, delimiter)\n            else:\n                delimiter = None\n                title_row = 0\n\n            data = utils.load_data(file, delimiter, title_row)\n\n            start_time = utils.find_reaction_start_time(data)\n\n            if delimiter == \"\\t\":\n                data = utils.prepare_tab_columns(data, file)\n            else:\n                if filetype == \".csv\":\n                    data = utils.tidy_colnames(data)\n\n            data = utils.remove_unnecessary_data(data)\n            data = utils.convert_df_to_float(data)\n            data = utils.correct_start_time(data, start_time)\n            data = utils.add_sample_info(data, file)\n\n        elif self._processed:\n            data = pd.read_csv(file, sep=\",\", header=0)\n\n        return data\n\n    #\n    # read csv data\n    #\n    def _read_calo_data_csv(self, file, show_info=True):\n        \"\"\"\n        try reading calorimetry data from csv file via multiple options\n\n        Parameters\n        ----------\n        file : str | pathlib.Path\n            path to csv fileto be read.\n        show_info : bool, optional\n            flag whether or not to show information. The default is True.\n\n        Returns\n        -------\n        pd.DataFrame\n\n        \"\"\"\n\n        try:\n            data = self._read_calo_data_csv_comma_sep(file, show_info=show_info)\n        except Exception:\n            data = self._read_calo_data_csv_tab_sep(file, show_info=show_info)\n\n        # valid read\n        if data is None:\n            # log\n            logging.info(f\"\\u2716 reading {file} FAILED.\")\n\n        # log\n        logging.info(f\"\\u2714 reading {file} successful.\")\n\n        # return\n        return data\n\n    #\n    # read csv data\n    #\n    def _read_calo_data_csv_comma_sep(self, file, show_info=True):\n        \"\"\"\n        read data from csv file\n\n        Parameters\n        ----------\n        file : str\n            filepath.\n\n        Returns\n        -------\n        data : pd.DataFrame\n            experimental data contained in file.\n\n        \"\"\"\n\n        # define Excel file\n        data = pd.read_csv(\n            file, header=None, sep=\"No meaningful separator\", engine=\"python\"\n        )\n\n        # check for tab-separation\n        if \"\\t\" in data.at[0, 0]:\n            # raise Exception\n            raise ValueError\n\n        # look for potential index indicating in-situ-file\n        if data[0].str.contains(\"Reaction start\").any():\n            # get target row\n            helper = data[0].str.contains(\"Reaction start\")\n            # get row\n            start_row = helper[helper].index.tolist()[0]\n            # get offset for in-situ files\n            t_offset_in_situ_s = float(data.at[start_row, 0].split(\",\")[0])\n\n        data = utils.parse_rowwise_data(data)\n        data = utils.tidy_colnames(data)\n\n        data = utils.remove_unnecessary_data(data)\n\n        # type conversion\n        data = utils.convert_df_to_float(data)\n\n        # check for \"in-situ\" sample --&gt; reset\n        try:\n            # offset\n            data[\"time_s\"] -= t_offset_in_situ_s\n            # write to log\n            logging.info(\n                f\"\\u26a0 Consider {file} as in-situ-file --&gt; time-scale adjusted.\"\n            )\n        except Exception:\n            pass\n\n        # restrict to \"time_s\" &gt; 0\n        data = data.query(\"time_s &gt; 0\").reset_index(drop=True)\n\n        # add sample information\n        data = utils.add_sample_info(data, file)\n\n        # if self.processparams.downsample.apply:\n        #     data = self._apply_adaptive_downsampling(data)\n\n        # return\n        return data\n\n    #\n    # read csv data\n    #\n    def _read_calo_data_csv_tab_sep(self, file: str, show_info=True) -&gt; pd.DataFrame:\n        \"\"\"\n        Parameters\n        ----------\n        file : str | pathlib.Path\n            path to tab separated csv-files from \"older\" versions of the device.\n        show_info : bool, optional\n            flag whether or not to show information. The default is True.\n\n        Returns\n        -------\n        pd.DataFrame\n\n        \"\"\"\n\n        # read\n        raw = pd.read_csv(file, sep=\"\\t\", header=None)\n\n        # process\n        data = raw.copy()\n\n        # get sample mass (if available)\n        try:\n            # get mass, first row in 3rd column is the title\n            # the assumption is that the sample weight is one value on top of the 3rd column\n            mass_index = raw.index[raw.iloc[:, 3].notna()]\n            mass = float(raw.iloc[mass_index[1], 3])\n        except IndexError:\n            # set mass to None\n            mass = None\n            # go on\n            pass\n\n        # get \"reaction start\" time (if available)\n        try:\n            # get \"reaction start\" time in seconds\n            _helper = data[data.iloc[:, 2].str.lower() == \"reaction start\"].head(1)\n            # convert to float\n            t0 = float(_helper[0].values[0])\n        except Exception:\n            # set t0 to None\n            t0 = None\n            # go on\n            pass\n\n        # remove all-Nan columns\n        data = data.dropna(how=\"all\", axis=1)\n\n        # restrict to first two columns\n        data = data.iloc[:, :2]\n\n        # rename\n        try:\n            data.columns = [\"time_s\", \"heat_flow_mw\"]\n        except ValueError:\n            # return empty DataFrame\n            return pd.DataFrame({\"time_s\": 0}, index=[0])\n\n        # get data columns\n        data = data.loc[3:, :].reset_index(drop=True)\n\n        # convert data types\n        data[\"time_s\"] = data[\"time_s\"].astype(float)\n        data[\"heat_flow_mw\"] = data[\"heat_flow_mw\"].apply(\n            lambda x: float(x.replace(\",\", \".\"))\n        )\n\n        # convert to same unit\n        data[\"heat_flow_w\"] = data[\"heat_flow_mw\"] / 1000\n\n        # calculate cumulative heat flow\n        data[\"heat_j\"] = integrate.cumulative_trapezoid(\n            data[\"heat_flow_w\"], x=data[\"time_s\"], initial=0\n        )\n\n        # remove \"heat_flow_w\" column\n        del data[\"heat_flow_mw\"]\n\n        # take into account time offset via \"reactin start\" time\n        if t0:\n            data[\"time_s\"] -= t0\n\n        # calculate normalized heat flow and heat\n        if mass:\n            data[\"normalized_heat_flow_w_g\"] = data[\"heat_flow_w\"] / mass\n            data[\"normalized_heat_j_g\"] = data[\"heat_j\"] / mass\n\n        # restrict to \"time_s\" &gt; 0\n        data = data.query(\"time_s &gt;= 0\").reset_index(drop=True)\n\n        # add sample information\n        data[\"sample\"] = file\n        data[\"sample_short\"] = pathlib.Path(file).stem\n\n        # type conversion\n        data = utils.convert_df_to_float(data)\n\n        # return\n        return data\n\n    #\n    # read csv info\n    #\n    def _read_calo_info_csv(self, file, show_info=True):\n        \"\"\"\n        read info from csv file\n\n        Parameters\n        ----------\n        file : str\n            filepath.\n\n        Returns\n        -------\n        info : pd.DataFrame\n            information (metadata) contained in file\n\n        \"\"\"\n\n        try:\n            # determine number of lines to skip\n            empty_lines = self._determine_data_range_csv(file)\n            # read info block from csv-file\n            info = pd.read_csv(\n                file, nrows=empty_lines[0] - 1, names=[\"parameter\", \"value\"]\n            ).dropna(subset=[\"parameter\"])\n            # the last block is not really meta data but summary data and\n            # somewhat not necessary\n        except IndexError:\n            # return empty DataFrame\n            info = pd.DataFrame()\n\n        # add sample name as column\n        info[\"sample\"] = file\n        info[\"sample_short\"] = pathlib.Path(file).stem\n\n        # return\n        return info\n\n    #\n    # read excel info\n    #\n    def _read_calo_info_xls(self, file, show_info=True):\n        \"\"\"\n        read information from xls-file\n\n        Parameters\n        ----------\n        file : str\n            filepath.\n        show_info : bool, optional\n            flag whether or not to show information. The default is True.\n\n        Returns\n        -------\n        info : pd.DataFrame\n            information (metadata) contained in file\n\n        \"\"\"\n        # specify Excel\n        xl = pd.ExcelFile(file)\n\n        try:\n            # get experiment info (first sheet)\n            df_experiment_info = xl.parse(\n                sheet_name=\"Experiment info\", header=0, names=[\"parameter\", \"value\"]\n            ).dropna(subset=[\"parameter\"])\n            # use first row as header\n            df_experiment_info = df_experiment_info.iloc[1:, :]\n\n            # add sample information\n            df_experiment_info[\"sample\"] = file\n            df_experiment_info[\"sample_short\"] = pathlib.Path(file).stem\n\n            # rename variable\n            info = df_experiment_info\n\n            # return\n            return info\n\n        except Exception as e:\n            if show_info:\n                print(e)\n                print(f\"==&gt; ERROR in file {file}\")\n\n    #\n    # read excel data\n    #\n    def _read_calo_data_xls(self, file, show_info=True):\n        \"\"\"\n        read data from xls-file\n\n        Parameters\n        ----------\n        file : str\n            filepath.\n        show_info : bool, optional\n            flag whether or not to show information. The default is True.\n\n        Returns\n        -------\n        data : pd.DataFrame\n            data contained in file\n\n        \"\"\"\n\n        # define Excel file\n        xl = pd.ExcelFile(file)\n\n        try:\n            # parse \"data\" sheet\n            df_data = xl.parse(\"Raw data\", header=None)\n\n            # replace init timestamp\n            df_data.iloc[0, 0] = \"time\"\n\n            # get new column names\n            new_columnames = []\n            for i, j in zip(df_data.iloc[0, :], df_data.iloc[1, :]):\n                # build\n                new_columnames.append(\n                    re.sub(r\"[\\s\\n\\[\\]\\(\\)\u00b0 _]+\", \"_\", f\"{i}_{j}\".lower())\n                    .replace(\"/\", \"_\")\n                    .replace(\"_signal_\", \"_\")\n                )\n\n            # set\n            df_data.columns = new_columnames\n\n            # cut out data part\n            df_data = df_data.iloc[2:, :].reset_index(drop=True)\n\n            # drop column\n            try:\n                df_data = df_data.drop(columns=[\"time_markers_nan\"])\n            except KeyError:\n                pass\n\n            # remove columns with too many NaNs\n            df_data = df_data.dropna(axis=1, thresh=3)\n            # # remove rows with NaNs\n            # df_data = df_data.dropna(axis=0)\n\n            # float conversion\n            for _c in df_data.columns:\n                # convert\n                df_data[_c] = df_data[_c].astype(float)\n\n            # add sample information\n            df_data[\"sample\"] = file\n            df_data[\"sample_short\"] = pathlib.Path(file).stem\n\n            # rename\n            data = df_data\n\n            # log\n            logging.info(f\"\\u2714 reading {file} successful.\")\n\n            # return\n            return data\n\n        except Exception as e:\n            if show_info:\n                print(\n                    \"\\n\\n===============================================================\"\n                )\n                print(f\"{e} in file '{pathlib.Path(file).name}'\")\n                print(\"Please, rename the data sheet to 'Raw data' (device default).\")\n                print(\n                    \"===============================================================\\n\\n\"\n                )\n\n            # log\n            logging.info(f\"\\u2716 reading {file} FAILED.\")\n\n            # return\n            return None\n\n    #\n    # iterate samples\n    #\n    def _iter_samples(self, regex=None):\n        \"\"\"\n        iterate samples and return corresponding data\n\n        Returns\n        -------\n        sample (str) : name of the current sample\n        data (pd.DataFrame) : data corresponding to the current sample\n        \"\"\"\n\n        for sample, data in self._data.groupby(by=\"sample\"):\n            if regex:\n                if not re.findall(regex, sample):\n                    continue\n\n            yield sample, data\n\n    #\n    # auto clean data\n    #\n    def _auto_clean_data(self):\n        \"\"\"\n        remove NaN values from self._data and merge differently named columns\n        representing the (constant) temperature set for the measurement\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # remove NaN values and reset index\n        self._data = self._data.dropna(\n            subset=[c for c in self._data.columns if re.match(\"normalized_heat\", c)]\n        ).reset_index(drop=True)\n\n        # determine NaN count\n        nan_count = self._data[\"temperature_temperature_c\"].isna().astype(\n            int\n        ) + self._data[\"temperature_c\"].isna().astype(int)\n\n        # consolidate temperature columns\n        if (\n            \"temperature_temperature_c\" in self._data.columns\n            and \"temperature_c\" in self._data.columns\n        ):\n            # use values from column \"temperature_c\" and set the values to column\n            # \"temperature_c\"\n            self._data.loc[\n                (self._data[\"temperature_temperature_c\"].isna()) &amp; (nan_count == 1),\n                \"temperature_temperature_c\",\n            ] = self._data.loc[\n                (~self._data[\"temperature_c\"].isna()) &amp; (nan_count == 1),\n                \"temperature_c\",\n            ]\n\n            # remove values from column \"temperature_c\"\n            self._data = self._data.drop(columns=[\"temperature_c\"])\n\n        # rename column\n        self._data = self._data.rename(\n            columns={\"temperature_temperature_c\": \"temperature_c\"}\n        )\n\n    #\n    # plot\n    #\n    def plot(\n        self,\n        t_unit=\"h\",\n        y=\"normalized_heat_flow_w_g\",\n        y_unit_milli=True,\n        regex=None,\n        show_info=True,\n        ax=None,\n    ):\n        \"\"\"\n\n        Plot the calorimetry data.\n\n        Parameters\n        ----------\n        t_unit : str, optional\n            time unit. The default is \"h\". Options are \"s\", \"min\", \"h\", \"d\".\n        y : str, optional\n            y-axis. The default is \"normalized_heat_flow_w_g\". Options are\n            \"normalized_heat_flow_w_g\", \"heat_flow_w\", \"normalized_heat_j_g\",\n            \"heat_j\".\n        y_unit_milli : bool, optional\n            whether or not to plot y-axis in Milliwatt. The default is True.\n        regex : str, optional\n            regex pattern to include only certain samples during plotting. The\n            default is None.\n        show_info : bool, optional\n            whether or not to show information. The default is True.\n        ax : matplotlib.axes._axes.Axes, optional\n            axis to plot to. The default is None.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import CaloCem as ta\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; calodatapath = Path(__file__).parent\n        &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n        &gt;&gt;&gt; tam.plot(t_unit=\"h\", y=\"normalized_heat_flow_w_g\", y_unit_milli=False)\n\n        \"\"\"\n\n        # y-value\n        if y == \"normalized_heat_flow_w_g\":\n            y_column = \"normalized_heat_flow_w_g\"\n            y_label = \"Normalized Heat Flow / [W/g]\"\n        elif y == \"heat_flow_w\":\n            y_column = \"heat_flow_w\"\n            y_label = \"Heat Flow / [W]\"\n        elif y == \"normalized_heat_j_g\":\n            y_column = \"normalized_heat_j_g\"\n            y_label = \"Normalized Heat / [J/g]\"\n        elif y == \"heat_j\":\n            y_column = \"heat_j\"\n            y_label = \"Heat / [J]\"\n\n        if y_unit_milli:\n            y_label = y_label.replace(\"[\", \"[m\")\n\n        # x-unit\n        if t_unit == \"s\":\n            x_factor = 1.0\n        elif t_unit == \"min\":\n            x_factor = 1 / 60\n        elif t_unit == \"h\":\n            x_factor = 1 / (60 * 60)\n        elif t_unit == \"d\":\n            x_factor = 1 / (60 * 60 * 24)\n\n        # y-unit\n        if y_unit_milli:\n            y_factor = 1000\n        else:\n            y_factor = 1\n\n        for sample, data in self._iter_samples():\n            if regex:\n                if not re.findall(rf\"{regex}\", os.path.basename(sample)):\n                    continue\n            data[\"time_s\"] = data[\"time_s\"] * x_factor\n            # all columns containing heat\n            heatcols = [s for s in data.columns if \"heat\" in s]\n            data[heatcols] = data[heatcols] * y_factor\n            ax, _ = utils.create_base_plot(data, ax, \"time_s\", y_column, sample)\n            ax = utils.style_base_plot(\n                ax,\n                y_label,\n                t_unit,\n                sample,\n            )\n        return ax\n\n    #\n    # plot by category\n    #\n    def plot_by_category(\n        self, categories, t_unit=\"h\", y=\"normalized_heat_flow_w_g\", y_unit_milli=True\n    ):\n        \"\"\"\n        plot by category, wherein the category is based on the information passed\n        via \"self._add_metadata_source\". Options available as \"category\" are\n        accessible via \"self.get_metadata_grouping_options\"\n\n        Parameters\n        ----------\n        categories : str, list[str]\n            category (from \"self.get_metadata_grouping_options\") to group by.\n            specify a string or a list of strings here\n        t_unit : TYPE, optional\n            see \"self.plot\". The default is \"h\".\n        y : TYPE, optional\n            see \"self.plot\". The default is \"normalized_heat_flow_w_g\".\n        y_unit_milli : TYPE, optional\n            see \"self.plot\". The default is True.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import CaloCem as ta\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; calodatapath = Path(__file__).parent\n        &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n        &gt;&gt;&gt; tam.plot_by_category(categories=\"sample\")\n\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        def build_helper_string(values: list) -&gt; str:\n            \"\"\"\n            build a \"nicely\" formatted string from a supplied list\n            \"\"\"\n\n            if len(values) == 2:\n                # connect with \"and\"\n                formatted = \" and \".join([str(i) for i in values])\n            elif len(values) &gt; 2:\n                # connect with comma and \"and\" for last element\n                formatted = (\n                    \", \".join([str(i) for i in values[:-1]]) + \" and \" + str(values[-1])\n                )\n            else:\n                formatted = \"---\"\n\n            # return\n            return formatted\n\n        # loop category values\n        for selections, _ in self._metadata.groupby(by=categories):\n            if isinstance(selections, tuple):\n                # - if multiple categories to group by are specified -\n                # init helper DataFrame\n                target_idx = pd.DataFrame()\n                # identify corresponding samples\n                for selection, category in zip(selections, categories):\n                    target_idx[category] = self._metadata[category] == selection\n                # get relevant indices\n                target_idx = target_idx.sum(axis=1) == len(categories)\n                # define title\n                title = f\"Grouped by {build_helper_string(categories)} ({build_helper_string(selections)})\"\n            else:\n                # - if only one(!) category to group by is specified -\n                # identify corresponding samples\n                target_idx = self._metadata[categories] == selections\n                # define title\n                title = f\"Grouped by {categories} ({selections})\"\n\n            # pick relevant samples\n            target_samples = self._metadata.loc[target_idx, self._metadata_id]\n\n            # build corresponding regex\n            regex = \"(\" + \")|(\".join(target_samples) + \")\"\n\n            # plot\n            ax = self.plot(regex=regex, t_unit=t_unit, y=y, y_unit_milli=y_unit_milli)\n\n            # set title\n            ax.set_title(title)\n\n            # yield latest plot\n            yield selections, ax\n\n    @staticmethod\n    def _plot_peak_positions(\n        data,\n        ax,\n        _age_col,\n        _target_col,\n        peaks,\n        sample,\n        plt_top,\n        plt_right_s,\n        plot_labels,\n        xmarker,\n    ):\n        \"\"\"\n        Plot detected peaks.\n        \"\"\"\n\n        ax, new_ax = utils.create_base_plot(data, ax, _age_col, _target_col, sample)\n\n        if xmarker:\n            ax.plot(\n                data[_age_col][peaks],\n                data[_target_col][peaks],\n                \"x\",\n                color=\"red\",\n            )\n\n        ax.vlines(\n            x=data[_age_col][peaks],\n            ymin=0,\n            ymax=data[_target_col][peaks],\n            color=\"red\",\n        )\n\n        if plot_labels:\n            for x, y in zip(data[_age_col][peaks], data[_target_col][peaks]):\n                y = y + 0.0002\n                ax.text(x, y, f\"{round(x, 2)}\", color=\"red\")\n\n            # ax.text(\n            #     x=data[_age_col][peaks],\n            #     y=data[_target_col][peaks],\n            #     #s=[f\"{round(i,2)}\" for i in data[_age_col][peaks]],\n            #     s=\"hallo\",\n            #     color=\"red\",\n            # )\n\n        limits = {\n            \"left\": ax.get_xlim()[0],\n            \"right\": plt_right_s,\n            \"bottom\": 0,\n            \"top\": plt_top,\n        }\n\n        ax = utils.style_base_plot(ax, _target_col, _age_col, sample, limits)\n\n        if new_ax:\n            plt.show()\n\n    @staticmethod\n    def _plot_maximum_slope(\n        data,\n        ax,\n        age_col,\n        target_col,\n        sample,\n        characteristics,\n        time_discarded_s,\n        save_path=None,\n        xscale=\"log\",\n        xunit=\"s\",\n    ):\n        x_increment = 600\n        if xunit == \"h\":\n            data[age_col] = data[age_col] / 3600\n            characteristics[age_col] = characteristics[age_col] / 3600\n            time_discarded_s = time_discarded_s / 3600\n            x_increment = 0.2\n\n        ax, new_ax = utils.create_base_plot(data, ax, age_col, target_col, sample)\n\n        ax2 = ax.twinx()\n        # plot gradient\n        ax2.plot(data[age_col], data[\"gradient\"], label=\"Gradient\", color=\"orange\")\n        ax2.set_yscale(\"linear\")\n        xmask = data[age_col] &gt; time_discarded_s\n        y_vals = data[target_col][xmask]\n        ymin = y_vals.min() + y_vals.min() * 0.1\n        ymax = y_vals.max() + y_vals.max() * 0.1\n        # ax.set_ylim(ymin, ymax)\n\n        y2_vals = data[\"gradient\"][xmask]\n        y2min = y2_vals.min() + y2_vals.min() * 0.1\n        y2max = y2_vals.max() + y2_vals.max() * 0.1\n        ax2.set_ylim(y2min, y2max)\n\n        ax2.set_ylabel(r\"Gradient [Wg$^{-1}$s$^{-1}$]\")\n\n        # add vertical lines\n        for _idx, _row in characteristics.iterrows():\n            # vline\n            t_maxslope = _row.at[age_col]\n            ax.axvline(t_maxslope, color=\"green\", alpha=0.3)\n\n        if xunit == \"h\":\n            limits = {\"left\": 0.1, \"right\": ax.get_xlim()[1], \"bottom\": 0, \"top\": ymax}\n\n        else:\n            limits = {\"left\": 100, \"right\": ax.get_xlim()[1], \"bottom\": 0, \"top\": ymax}\n\n        ax = utils.style_base_plot(\n            ax,\n            target_col,\n            age_col,\n            sample,\n            limits,\n            time_discarded_s=time_discarded_s,\n            xunit=xunit,\n        )\n\n        ax.set_xscale(xscale)\n        ax.text(\n            t_maxslope + x_increment,\n            0.00025,\n            f\"{round(t_maxslope, 2)} {xunit} \",\n            color=\"green\",\n        )\n\n        if new_ax:\n            if save_path:\n                sample_name = pathlib.Path(sample).stem\n                plt.savefig(save_path / f\"maximum_slope_detect_{sample_name}.pdf\")\n            else:\n                plt.show()\n\n    @staticmethod\n    def _plot_intersection(\n        data,\n        ax,\n        age_col,\n        target_col,\n        sample,\n        # characteristics,\n        time_discarded_s,\n        characteristics,\n        save_path=None,\n        xscale=\"log\",\n        # xunit=\"s\",\n        hmax=None,\n        tmax=None,\n    ):\n        if characteristics.xunit == \"h\":\n            data.loc[:, age_col] = data.loc[:, age_col] / 3600\n            characteristics.time_s = characteristics.time_s / 3600\n            characteristics.dorm_time_s = characteristics.dorm_time_s / 3600\n            characteristics.gradient = characteristics.gradient * 3600\n            tmax = tmax / 3600\n            characteristics.x_intersect = characteristics.x_intersect / 3600\n            # characteristics[age_col] = characteristics[age_col] / 3600\n\n        ax, new_ax = utils.create_base_plot(data, ax, age_col, target_col, sample)\n        # print(new_ax)\n        ax = utils.style_base_plot(\n            ax,\n            target_col,\n            age_col,\n            sample,\n            time_discarded_s=time_discarded_s,\n            xunit=characteristics.xunit,\n        )\n\n        ax.axline(\n            (characteristics.time_s, characteristics.normalized_heat_flow_w_g),\n            slope=characteristics.gradient,\n            color=\"red\",\n            linestyle=\"--\",\n        )\n        if characteristics.intersection == \"dormant_hf\":\n            ax.axhline(\n                y=characteristics.dorm_normalized_heat_flow_w_g,\n                color=\"red\",\n                linestyle=\"--\",\n            )\n            ax.text(\n                x=characteristics.x_intersect,\n                y=characteristics.dorm_normalized_heat_flow_w_g,\n                s=rf\"   $t_i=$ {characteristics.x_intersect:.1f} {characteristics.xunit}\"\n                + \"\\n\",\n                color=\"green\",\n            )\n        elif characteristics.intersection == \"abscissa\":\n            ax.text(\n                x=characteristics.x_intersect,\n                y=0,\n                s=rf\"   $t_i=$ {characteristics.x_intersect:.1f} {characteristics.xunit}\"\n                + \"\\n\",\n                color=\"green\",\n            )\n\n        ax.axvline(\n            x=characteristics.x_intersect,\n            color=\"green\",\n            linestyle=\":\",\n        )\n\n        ax.set_xscale(xscale)\n        ax.set_xlim(0, tmax)\n        ax.set_ylim(0, hmax)\n\n        if new_ax:\n            if save_path:\n                sample_name = pathlib.Path(sample).stem\n                plt.savefig(save_path / f\"intersection_detect_{sample_name}.pdf\")\n            else:\n                plt.show()\n\n    #\n    # get the cumulated heat flow a at a certain age\n    #\n\n    def get_cumulated_heat_at_hours(self, processparams=None, target_h=4, **kwargs):\n        \"\"\"\n        get the cumulated heat flow a at a certain age\n\n        Parameters\n        ----------\n        processparams : ProcessingParameters, optional\n            Processing parameters. The default is None. If None, the default\n            parameters are used. The most important parameter is the cutoff time\n            in minutes which describes the initial time period of the measurement\n            which is not considered for the cumulated heat flow. It is defined in\n            the ProcessingParameters class. The default value is 30 minutes.\n\n        target_h : int | float\n            end time in hourscv\n\n        Returns\n        -------\n        A Pandas dataframe\n\n        \"\"\"\n        if \"cutoff_min\" in kwargs:\n            cutoff_min = kwargs[\"cutoff_min\"]\n            warnings.warn(\n                \"The cutoff_min parameter is deprecated. Please use the ProcessingParameters class instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        else:\n            if not processparams:\n                processparams = ProcessingParameters()\n            cutoff_min = processparams.cutoff.cutoff_min\n\n        def applicable(df, target_h=4, cutoff_min=None):\n            # convert target time to seconds\n            target_s = 3600 * target_h\n            # helper\n            _helper = df.query(\"time_s &lt;= @target_s\").tail(1)\n            # get heat at target time\n            hf_at_target = float(_helper[\"normalized_heat_j_g\"].values[0])\n\n            # if cutoff time specified\n            if cutoff_min:\n                # convert target time to seconds\n                target_s = 60 * cutoff_min\n                try:\n                    # helper\n                    _helper = df.query(\"time_s &lt;= @target_s\").tail(1)\n                    # type conversion\n                    hf_at_cutoff = float(_helper[\"normalized_heat_j_g\"].values[0])\n                    # correct heatflow for heatflow at cutoff\n                    hf_at_target = hf_at_target - hf_at_cutoff\n                except TypeError:\n                    name_wt_nan = df[\"sample_short\"].tolist()[0]\n                    print(\n                        f\"Found NaN in Normalized heat of sample {name_wt_nan} searching for cumulated heat at {target_h}h and a cutoff of {cutoff_min}min.\"\n                    )\n                    return np.nan\n\n            # return\n            return hf_at_target\n\n        # in case of one specified time\n        if isinstance(target_h, int) or isinstance(target_h, float):\n            # groupby\n            results = (\n                self._data.groupby(by=\"sample\")[[\"time_s\", \"normalized_heat_j_g\"]]\n                .apply(\n                    lambda x: applicable(x, target_h=target_h, cutoff_min=cutoff_min),\n                )\n                .reset_index(level=0)\n            )\n            # rename\n            results.columns = [\"sample\", \"cumulated_heat_at_hours\"]\n            results[\"target_h\"] = target_h\n            results[\"cutoff_min\"] = cutoff_min\n\n        # in case of specified list of times\n        elif isinstance(target_h, list):\n            # init list\n            list_of_results = []\n            # loop\n            for this_target_h in target_h:\n                # groupby\n                _results = (\n                    self._data.groupby(by=\"sample\")[[\"time_s\", \"normalized_heat_j_g\"]]\n                    .apply(\n                        lambda x: applicable(\n                            x, target_h=this_target_h, cutoff_min=cutoff_min\n                        ),\n                    )\n                    .reset_index(level=0)\n                )\n                # rename\n                _results.columns = [\"sample\", \"cumulated_heat_at_hours\"]\n                _results[\"target_h\"] = this_target_h\n                _results[\"cutoff_min\"] = cutoff_min\n                # append to list\n                list_of_results.append(_results)\n            # build overall results DataFrame\n            results = pd.concat(list_of_results)\n\n        # return\n        return results\n\n    #\n    # find peaks\n    #\n    def get_peaks(\n        self,\n        processparams,\n        target_col=\"normalized_heat_flow_w_g\",\n        regex=None,\n        cutoff_min=None,\n        show_plot=True,\n        plt_right_s=2e5,\n        plt_top=1e-2,\n        ax=None,\n        xunit=\"s\",\n        plot_labels=None,\n        xmarker=False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        get DataFrame of peak characteristics.\n\n        Parameters\n        ----------\n        target_col : str, optional\n            measured quantity within which peaks are searched for. The default is \"normalized_heat_flow_w_g\"\n        regex : str, optional\n            regex pattern to include only certain experimental result files\n            during initialization. The default is None.\n        cutoff_min : int | float, optional\n            Time in minutes below which collected data points are discarded for peak picking\n        show_plot : bool, optional\n            Flag whether or not to plot peak picking for each sample. The default is True.\n        plt_right_s : int | float, optional\n            Upper limit of x-axis of in seconds. The default is 2e5.\n        plt_top : int | float, optional\n            Upper limit of y-axis of. The default is 1e-2.\n        ax : matplotlib.axes._axes.Axes | None, optional\n            The default is None.\n\n        Returns\n        -------\n        pd.DataFrame holding peak characterisitcs for each sample.\n\n        \"\"\"\n\n        # list of peaks\n        list_of_peaks_dfs = []\n\n        # loop samples\n        for sample, data in self._iter_samples(regex=regex):\n            # cutoff\n            if processparams.cutoff.cutoff_min:\n                # discard points at early age\n                data = data.query(\"time_s &gt;= @processparams.cutoff.cutoff_min * 60\")\n\n            # reset index\n            data = data.reset_index(drop=True)\n\n            # target_columns\n            _age_col = \"time_s\"\n            _target_col = target_col\n\n            # find peaks\n            peaks, properties = signal.find_peaks(\n                data[_target_col],\n                prominence=processparams.peakdetection.prominence,\n                distance=processparams.peakdetection.distance,\n            )\n\n            # plot?\n            if show_plot:\n                if xunit == \"h\":\n                    df_copy = data.copy()\n                    df_copy[_age_col] = df_copy[_age_col] / 3600\n                    plt_right_s = plt_right_s / 3600\n                    self._plot_peak_positions(\n                        df_copy,\n                        ax,\n                        _age_col,\n                        _target_col,\n                        peaks,\n                        sample,\n                        plt_top,\n                        plt_right_s,\n                        plot_labels,\n                        xmarker,\n                    )\n                else:\n                    self._plot_peak_positions(\n                        data,\n                        ax,\n                        _age_col,\n                        _target_col,\n                        peaks,\n                        sample,\n                        plt_top,\n                        plt_right_s,\n                        plot_labels,\n                        xmarker,\n                    )\n\n            # compile peak characteristics\n            peak_characteristics = pd.concat(\n                [\n                    data.iloc[peaks, :],\n                    pd.DataFrame(\n                        properties[\"prominences\"], index=peaks, columns=[\"prominence\"]\n                    ),\n                    pd.DataFrame({\"peak_nr\": np.arange((len(peaks)))}, index=peaks),\n                ],\n                axis=1,\n            )\n\n            # append\n            list_of_peaks_dfs.append(peak_characteristics)\n\n        # compile peak information\n        peaks = pd.concat(list_of_peaks_dfs)\n\n        if isinstance(ax, matplotlib.axes._axes.Axes):\n            # return peak list and ax\n            return peaks, ax\n        else:  # return peak list only\n            return peaks\n\n    #\n    # get peak onsets\n    #\n    def get_peak_onsets(\n        self,\n        target_col=\"normalized_heat_flow_w_g\",\n        age_col=\"time_s\",\n        time_discarded_s=900,\n        rolling=1,\n        gradient_threshold=0.0005,\n        show_plot=False,\n        exclude_discarded_time=False,\n        regex=None,\n        ax: plt.Axes = None,\n    ):\n        \"\"\"\n        get peak onsets based on a criterion of minimum gradient\n\n        Parameters\n        ----------\n        target_col : str, optional\n            measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"\n        age_col : str, optional\n            Time unit within which peak onsets are searched for. The default is \"time_s\"\n        time_discarded_s : int | float, optional\n            Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.\n        rolling : int, optional\n            Width of \"rolling\" window within which the values of \"target_col\" are averaged. A higher value will introduce a stronger smoothing effect. The default is 1, i.e. no smoothing.\n        gradient_threshold : float, optional\n            Threshold of slope for identification of a peak onset. For a lower value, earlier peak onsets will be identified. The default is 0.0005.\n        show_plot : bool, optional\n            Flag whether or not to plot peak picking for each sample. The default is False.\n        exclude_discarded_time : bool, optional\n            Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.\n        regex : str, optional\n            regex pattern to include only certain experimental result files during initialization. The default is None.\n        ax : matplotlib.axes._axes.Axes | None, optional\n            The default is None.\n        Returns\n        -------\n        pd.DataFrame holding peak onset characterisitcs for each sample.\n\n        \"\"\"\n\n        # init list of characteristics\n        list_of_characteristics = []\n\n        # loop samples\n        for sample, data in self._iter_samples(regex=regex):\n            if exclude_discarded_time:\n                # exclude\n                data = data.query(f\"{age_col} &gt;= {time_discarded_s}\")\n\n            # reset index\n            data = data.reset_index(drop=True)\n\n            # calculate get gradient\n            data[\"gradient\"] = pd.Series(\n                np.gradient(data[target_col].rolling(rolling).mean(), data[age_col])\n            )\n\n            # get relevant points\n            characteristics = data.copy()\n            # discard initial time\n            characteristics = characteristics.query(f\"{age_col} &gt;= {time_discarded_s}\")\n            # look at values with certain gradient only\n            characteristics = characteristics.query(\"gradient &gt; @gradient_threshold\")\n            # consider first entry exclusively\n            characteristics = characteristics.head(1)\n\n            # optional plotting\n            if show_plot:\n                # if specific axis to plot to is specified\n                if isinstance(ax, matplotlib.axes._axes.Axes):\n                    # plot heat flow curve\n                    p = ax.plot(data[age_col], data[target_col])\n\n                    # add vertical lines\n                    for _idx, _row in characteristics.iterrows():\n                        # vline\n                        ax.axvline(_row.at[age_col], color=p[0].get_color(), alpha=0.3)\n                        # add \"slope line\"\n                        ax.axline(\n                            (_row.at[age_col], _row.at[target_col]),\n                            slope=_row.at[\"gradient\"],\n                            color=p[0].get_color(),\n                            # color=\"k\",\n                            # linewidth=0.2\n                            alpha=0.25,\n                            linestyle=\"--\",\n                        )\n\n                    # cosmetics\n                    # ax.set_xscale(\"log\")\n                    ax.set_title(\"Onset for \" + pathlib.Path(sample).stem)\n                    ax.set_xlabel(age_col)\n                    ax.set_ylabel(target_col)\n\n                    ax.fill_between(\n                        [ax.get_ylim()[0], time_discarded_s],\n                        [ax.get_ylim()[0]] * 2,\n                        [ax.get_ylim()[1]] * 2,\n                        color=\"black\",\n                        alpha=0.35,\n                    )\n\n                    # set axis limit\n                    ax.set_xlim(left=100)\n\n                else:\n                    # plot heat flow curve\n                    plt.plot(data[age_col], data[target_col])\n\n                    # add vertical lines\n                    for _idx, _row in characteristics.iterrows():\n                        # vline\n                        plt.axvline(_row.at[age_col], color=\"red\", alpha=0.3)\n\n                    # cosmetics\n                    # plt.xscale(\"log\")\n                    plt.title(\"Onset for \" + pathlib.Path(sample).stem)\n                    plt.xlabel(age_col)\n                    plt.ylabel(target_col)\n\n                    # get axis\n                    ax = plt.gca()\n\n                    plt.fill_between(\n                        [ax.get_ylim()[0], time_discarded_s],\n                        [ax.get_ylim()[0]] * 2,\n                        [ax.get_ylim()[1]] * 2,\n                        color=\"black\",\n                        alpha=0.35,\n                    )\n\n                    # set axis limit\n                    plt.xlim(left=100)\n\n            # append to list\n            list_of_characteristics.append(characteristics)\n\n        # build overall list\n        onset_characteristics = pd.concat(list_of_characteristics)\n\n        # return\n        if isinstance(ax, matplotlib.axes._axes.Axes):\n            # return onset characteristics and ax\n            return onset_characteristics, ax\n        else:\n            # return onset characteristics exclusively\n            return onset_characteristics\n\n    #\n    # get maximum slope\n    #\n\n    def get_maximum_slope(\n        self,\n        processparams,\n        target_col=\"normalized_heat_flow_w_g\",\n        age_col=\"time_s\",\n        time_discarded_s=900,\n        show_plot=False,\n        show_info=True,\n        exclude_discarded_time=False,\n        regex=None,\n        read_start_c3s=False,\n        ax=None,\n        save_path=None,\n        xscale=\"log\",\n        xunit=\"s\",\n    ):\n        \"\"\"\n        The method finds the point in time of the maximum slope. It also calculates the gradient at this point. The method can be controlled by passing a customized ProcessingParameters object for the `processparams` parameter. If no object is passed, the default parameters will be used.\n\n        Parameters\n        ----------\n        target_col : str, optional\n            measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"\n        age_col : str, optional\n            Time unit within which peak onsets are searched for. The default is \"time_s\"\n        time_discarded_s : int | float, optional\n            Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.\n        show_plot : bool, optional\n            Flag whether or not to plot peak picking for each sample. The default is False.\n        exclude_discarded_time : bool, optional\n            Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.\n        regex : str, optional\n            regex pattern to include only certain experimental result files during initialization. The default is None.\n        Returns\n        -------\n        Pandas Dataframe\n            A dataframe that contains the time and the gradient of the maximum slope.\n        Examples\n        --------\n        &gt;&gt;&gt; from CaloCem import tacalorimetry as ta\n        &gt;&gt;&gt; from pathlib import Path\n\n        &gt;&gt;&gt; thepath = Path(__file__).parent / \"data\"\n        &gt;&gt;&gt; tam = ta.Measurement(thepath)\n        &gt;&gt;&gt; processparams = ta.ProcessingParameters()\n        &gt;&gt;&gt; processparams..apply = True\n        &gt;&gt;&gt; max_slopes = tam.get_maximum_slope(processparams)\n        \"\"\"\n\n        # init list of characteristics\n        list_of_characteristics = []\n\n        # loop samples\n        for sample, data in self._iter_samples(regex=regex):\n            sample_name = pathlib.Path(sample).stem\n            if exclude_discarded_time:\n                # exclude\n                data = data.query(f\"{age_col} &gt;= {time_discarded_s}\")\n\n            # manual definition of start time to look for c3s - in case auto peak detection becomes difficult\n            if read_start_c3s:\n                c3s_start_time_s = self._metadata.query(\n                    f\"sample_number == '{sample_name}'\"\n                )[\"t_c3s_min_s\"].values[0]\n                c3s_end_time_s = self._metadata.query(\n                    f\"sample_number == '{sample_name}'\"\n                )[\"t_c3s_max_s\"].values[0]\n                data = data.query(\n                    f\"{age_col} &gt;= {c3s_start_time_s} &amp; {age_col} &lt;= {c3s_end_time_s}\"\n                )\n\n            if show_info:\n                print(f\"Determineing maximum slope of {pathlib.Path(sample).stem}\")\n\n            processor = HeatFlowProcessor(processparams)\n\n            data = utils.make_equidistant(data)\n\n            if processparams.rolling_mean.apply:\n                data = processor.apply_rolling_mean(data)\n\n            data[\"gradient\"], data[\"curvature\"] = (\n                processor.calculate_heatflow_derivatives(data)\n            )\n\n            characteristics = processor.get_largest_slope(data, processparams)\n            if characteristics.empty:\n                continue\n\n            # optional plotting\n            if show_plot:\n                self._plot_maximum_slope(\n                    data,\n                    ax,\n                    age_col,\n                    target_col,\n                    sample,\n                    characteristics,\n                    time_discarded_s,\n                    save_path=save_path,\n                    xscale=xscale,\n                    xunit=xunit,\n                )\n                # plot heat flow curve\n                # plt.plot(data[age_col], data[target_col], label=target_col)\n                # plt.plot(\n                #     data[age_col],\n                #     data[\"gradient\"] * 1e4 + 0.001,\n                #     label=\"gradient * 1e4 + 1mW\",\n                # )\n\n                # # add vertical lines\n                # for _idx, _row in characteristics.iterrows():\n                #     # vline\n                #     plt.axvline(_row.at[age_col], color=\"green\", alpha=0.3)\n\n                # # cosmetics\n                # plt.xscale(\"log\")\n                # plt.title(f\"Maximum slope plot for {pathlib.Path(sample).stem}\")\n                # plt.xlabel(age_col)\n                # plt.ylabel(target_col)\n                # plt.legend()\n\n                # # get axis\n                # ax = plt.gca()\n\n                # plt.fill_between(\n                #     [ax.get_ylim()[0], time_discarded_s],\n                #     [ax.get_ylim()[0]] * 2,\n                #     [ax.get_ylim()[1]] * 2,\n                #     color=\"black\",\n                #     alpha=0.35,\n                # )\n\n                # # set axis limit\n                # plt.xlim(left=100)\n                # plt.ylim(bottom=0, top=0.01)\n\n                # # show\n                # plt.show()\n\n            # append to list\n            list_of_characteristics.append(characteristics)\n\n        if not list_of_characteristics:\n            print(\"No maximum slope found, check you processing parameters\")\n        # build overall list\n        else:\n            max_slope_characteristics = pd.concat(list_of_characteristics)\n            # return\n            return max_slope_characteristics\n\n\n    def get_average_slope(\n    self,\n    processparams,\n    target_col=\"normalized_heat_flow_w_g\",\n    age_col=\"time_s\",\n    regex=None,\n    show_plot=False,\n    ax=None,\n    save_path=None,\n    xscale=\"linear\",\n    xunit=\"s\",\n    ):\n        \"\"\"\n        Calculate average slope by determining 4 additional slope values between \n        onset time and heat flow maximum, in addition to the maximum slope.\n\n        Parameters\n        ----------\n        processparams : ProcessingParameters\n            Processing parameters for analysis\n        target_col : str, optional\n            Target measurement column, by default \"normalized_heat_flow_w_g\"\n        age_col : str, optional\n            Time column name, by default \"time_s\"\n        regex : str, optional\n            Regex pattern to filter samples, by default None\n        show_plot : bool, optional\n            Whether to show plots, by default False\n        ax : matplotlib.axes.Axes, optional\n            Existing axis to plot on, by default None\n        save_path : Path, optional\n            Path to save plots, by default None\n        xscale : str, optional\n            X-axis scale, by default \"log\"\n        xunit : str, optional\n            Time unit for display, by default \"s\"\n\n        Returns\n        -------\n        pd.DataFrame\n            DataFrame containing average slope characteristics for each sample\n        \"\"\"\n\n        # Get maximum slopes using existing method\n        max_slopes = self.get_maximum_slope(\n            processparams,\n            target_col=target_col,\n            age_col=age_col,\n            regex=regex,\n            show_plot=False,\n            ax=ax,\n            save_path=save_path,\n            xscale=xscale,\n            xunit=xunit,\n        )\n\n        if max_slopes is None or max_slopes.empty:\n            print(\"No maximum slopes found. Cannot calculate average slopes.\")\n            return pd.DataFrame()\n\n        # Get onset times using the peak onset method\n        onsets = self.get_peak_onset_via_max_slope(\n            processparams,\n            show_plot=False,\n            regex=regex,\n            age_col=age_col,\n            target_col=target_col,\n            xunit=xunit,\n        )\n\n        if onsets.empty:\n            print(\"No onset times found. Cannot calculate average slopes.\")\n            return pd.DataFrame()\n\n        list_of_characteristics = []\n\n        # Loop through samples\n        for sample, data in self._iter_samples(regex=regex):\n            sample_short = pathlib.Path(sample).stem\n\n            # Get max slope data for this sample\n            max_slope_row = max_slopes[max_slopes[\"sample_short\"] == sample_short]\n            if max_slope_row.empty:\n                continue\n\n            # Get onset data for this sample\n            onset_row = onsets[onsets[\"sample\"] == sample_short]\n            if onset_row.empty:\n                continue\n\n            # Get time points\n            onset_time = onset_row[\"onset_time_s\"].iloc[0]\n            max_slope_time = max_slope_row[age_col].iloc[0]\n\n            # Find heat flow maximum after onset\n            data_after_onset = data[data[age_col] &gt;= onset_time]\n            if data_after_onset.empty:\n                continue\n\n            max_hf_time = data_after_onset.loc[data_after_onset[target_col].idxmax(), age_col]\n\n            # Create 4 intermediate time points between onset and heat flow maximum\n            if max_hf_time &lt;= onset_time:\n                print(f\"Warning: Heat flow maximum occurs before onset for {sample_short}\")\n                continue\n\n            # Create 6 time points total (onset, 4 intermediate, max_hf)\n            time_points = np.linspace(onset_time + 3600, max_hf_time - 3600, 6)\n\n            # Calculate slopes at each interval\n            slopes = []\n            slope_times = []\n\n            for i in range(len(time_points) - 1):\n                t1, t2 = time_points[i], time_points[i + 1]\n\n                # Get data points in this interval\n                interval_data = data[(data[age_col] &gt;= t1) &amp; (data[age_col] &lt;= t2)]\n\n                if len(interval_data) &lt; 2:\n                    continue\n\n                # Calculate slope using linear regression\n                x_vals = interval_data[age_col].values\n                y_vals = interval_data[target_col].values\n\n                # Simple slope calculation: (y2 - y1) / (x2 - x1)\n                slope = (y_vals[-1] - y_vals[0]) / (x_vals[-1] - x_vals[0])\n                slopes.append(slope)\n                slope_times.append((t1 + t2) / 2)  # Midpoint time\n\n            # Include the maximum slope\n            max_slope_value = max_slope_row[\"gradient\"].iloc[0]\n            slopes.append(max_slope_value)\n            slope_times.append(max_slope_time)\n\n            # Calculate average slope\n            if slopes:\n                avg_slope = np.mean(slopes)\n                std_slope = np.std(slopes)\n\n                # Create characteristics dictionary\n                characteristics = {\n                    \"sample\": sample,\n                    \"sample_short\": sample_short,\n                    \"onset_time_s\": onset_time,\n                    \"max_hf_time_s\": max_hf_time,\n                    \"max_slope_time_s\": max_slope_time,\n                    \"max_slope_value\": max_slope_value,\n                    \"average_slope\": avg_slope,\n                    \"slope_std\": std_slope,\n                    \"n_slopes\": len(slopes),\n                    \"individual_slopes\": slopes,\n                    \"slope_times\": slope_times,\n                }\n\n                # Optional plotting\n                if show_plot:\n                    self._plot_average_slope_analysis(\n                        data, characteristics, ax, age_col, target_col, \n                        sample_short, save_path, xscale, xunit\n                    )\n\n                list_of_characteristics.append(characteristics)\n\n        if not list_of_characteristics:\n            print(\"No average slope characteristics calculated.\")\n            return pd.DataFrame()\n\n        # Convert to DataFrame\n        avg_slope_df = pd.DataFrame(list_of_characteristics)\n\n        return avg_slope_df\n\n\n    @staticmethod\n    def _plot_average_slope_analysis(\n        data, characteristics, ax, age_col, target_col, \n        sample_short, save_path=None, xscale=\"linear\", xunit=\"s\"\n    ):\n        \"\"\"Plot average slope analysis for visualization\"\"\"\n\n        ax, new_ax = utils.create_base_plot(data, ax, age_col, target_col, sample_short, color=\"gray\")\n\n        # Plot the heat flow curve\n        #ax.plot(data[age_col], data[target_col], 'b-', alpha=0.7, label='Heat Flow')\n\n        # Mark onset time\n        ax.axvline(characteristics[\"onset_time_s\"], color='green', \n                linestyle='--', alpha=0.7, label='Onset')\n\n        # Mark max heat flow time\n        ax.axvline(characteristics[\"max_hf_time_s\"], color='orange', \n                linestyle='--', alpha=0.7, label='Max Heat Flow')\n\n        # Mark max slope time\n        ax.axvline(characteristics[\"max_slope_time_s\"], color='red', \n                linestyle='--', alpha=0.7, label='Max Slope')\n\n        # Plot individual slope lines\n        colors = plt.cm.viridis(np.linspace(0, 1, len(characteristics[\"individual_slopes\"])))\n\n        for i, (slope, time, color) in enumerate(zip(\n            characteristics[\"individual_slopes\"], \n            characteristics[\"slope_times\"], \n            colors\n        )):\n            # Find corresponding y-value\n            y_val = np.interp(time, data[age_col], data[target_col])\n\n            # Plot slope line (extend \u00b110% of time range)\n            time_range = characteristics[\"max_hf_time_s\"] - characteristics[\"onset_time_s\"]\n            dt = 0.1 * time_range\n\n            x_line = [time - dt, time + dt]\n            y_line = [y_val - slope * dt, y_val + slope * dt]\n\n            ax.plot(x_line, y_line, color=color, alpha=0.6, linewidth=2,\n                    label=f'Slope {i+1}: {slope:.2e}')\n\n        # Add text annotation for average slope\n        ax.text(0.05, 0.95, \n                f'Avg Slope: {characteristics[\"average_slope\"]:.2e}\\n'\n                f'Std: {characteristics[\"slope_std\"]:.2e}',\n                transform=ax.transAxes, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n\n        ax.set_xscale(xscale)\n        ax.set_xlabel(f'Time [{xunit}]')\n        ax.set_ylabel(target_col.replace('_', ' ').title())\n        ax.set_title(f'Average Slope Analysis - {sample_short}')\n        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        ax.grid(True, alpha=0.3)\n        #ax.set_ylim(0,0.003)\n\n        if new_ax:\n            if save_path:\n                plt.tight_layout()\n                plt.show()\n                #plt.savefig(save_path / f\"average_slope_analysis_{sample_short}.pdf\")\n                plt.close()\n            else:\n                plt.tight_layout()\n                plt.show() \n\n\n    # get reaction onset via maximum slope\n    #\n    def get_peak_onset_via_max_slope(\n        self,\n        processparams,\n        show_plot=False,\n        ax=None,\n        regex=None,\n        age_col=\"time_s\",\n        target_col=\"normalized_heat_flow_w_g\",\n        time_discarded_s=900,\n        save_path=None,\n        xscale=\"linear\",\n        xunit=\"s\",\n        intersection=\"dormant_hf\",\n    ):\n        \"\"\"\n        get reaction onset based on tangent of maximum heat flow and heat flow\n        during the dormant period. The characteristic time is inferred from\n        the intersection of both characteristic lines\n\n        Parameters\n        ----------\n        show_plot : TYPE, optional\n            DESCRIPTION. The default is False.\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n        # get onsets\n        max_slopes = self.get_maximum_slope(\n            processparams,\n            regex=regex,\n            show_plot=False,\n            ax=ax,\n        )\n        # % get dormant period HFs\n        dorm_hfs = self.get_dormant_period_heatflow(\n            processparams,  # cutoff_min=cutoff_min, prominence=prominence\n            regex=regex,\n            show_plot=False,\n            # ax=ax,\n        )\n\n        # init list\n        list_characteristics = []\n\n        # loop samples\n        for i, row in max_slopes.iterrows():\n            # calculate y-offset\n            t = row[\"normalized_heat_flow_w_g\"] - row[\"time_s\"] * row[\"gradient\"]\n            # calculate point of intersection\n            # calculate x-intersect of tangent with dormant heat flow\n            x_intersect_dormant = (\n                    float(\n                        dorm_hfs[dorm_hfs[\"sample_short\"] == row[\"sample_short\"]][\n                            \"normalized_heat_flow_w_g\"\n                        ]\n                    )\n                    - t\n                ) / row[\"gradient\"]\n            # elif intersection == \"abscissa\":\n                # calculate x-intersect of tangent with abscissa (y=0)\n            x_intersect = row[\"time_s\"] - (row[\"normalized_heat_flow_w_g\"] / row[\"gradient\"])\n\n            data = self._data.query(\"sample_short == @row['sample_short']\")\n            sample = row[\"sample_short\"]\n\n            heat_at_intersect = np.interp(x_intersect, data[\"time_s\"], data[\"normalized_heat_j_g\"])\n\n            # append to list\n            list_characteristics.append(\n                {\n                    \"sample\": row[\"sample_short\"],\n                    \"onset_time_s_abscissa\": x_intersect,\n                    \"onset_time_min_abscissa\": x_intersect / 60,\n                    \"heat_at_onset_j_g\": heat_at_intersect,\n                    \"onset_time_s\": x_intersect_dormant,\n                    \"onset_time_min\": x_intersect_dormant / 60,\n                }\n            )\n\n\n            dorm_hfs_sample = dorm_hfs.query(\"sample_short == @sample\")\n            # add prefix dorm to all columns\n            dorm_hfs_sample.columns = [\"dorm_\" + s for s in dorm_hfs_sample.columns]\n\n            characteristics = pd.concat([row, dorm_hfs_sample.squeeze()])\n            characteristics.loc[\"xunit\"] = xunit\n            characteristics.loc[\"x_intersect\"] = x_intersect\n            characteristics.loc[\"intersection\"] = intersection\n            # print(characteristics.x_intersect)\n\n            # get maximum time value\n            tmax = self._data.query(\"sample_short == @row['sample_short']\")[\n                \"time_s\"\n            ].max()\n            # get maximum heat flow value\n            hmax = self._data.query(\n                \"time_s &gt; 3000 &amp; sample_short == @row['sample_short']\"\n            )[\"normalized_heat_flow_w_g\"].max()\n\n            if show_plot:\n                self._plot_intersection(\n                    data,\n                    ax,\n                    age_col,\n                    target_col,\n                    sample,\n                    # max_slopes,\n                    time_discarded_s,\n                    characteristics=characteristics,\n                    save_path=save_path,\n                    xscale=xscale,\n                    # xunit=xunit,\n                    hmax=hmax,\n                    tmax=tmax,\n                )\n\n        # build overall dataframe to be returned\n        onsets = pd.DataFrame(list_characteristics)\n\n        # merge with dorm_hfs\n        onsets = onsets.merge(\n            dorm_hfs[\n                [\"sample_short\", \"normalized_heat_flow_w_g\", \"normalized_heat_j_g\"]\n            ],\n            left_on=\"sample\",\n            right_on=\"sample_short\",\n            how=\"left\",\n        )\n\n        # rename\n        onsets = onsets.rename(\n            columns={\n                \"normalized_heat_flow_w_g\": \"normalized_heat_flow_w_g_at_dorm_min\",\n                \"normalized_heat_j_g\": \"normalized_heat_j_g_at_dorm_min\",\n            }\n        )\n\n        # return\n        return onsets\n        # if isinstance(ax, matplotlib.axes._axes.Axes):\n        #     # return onset characteristics and ax\n        #     return onsets, ax\n        # else:\n        #     # return onset characteristics exclusively\n        #     return onsets\n\n    #\n    # get dormant period heatflow\n    #\n\n    def get_dormant_period_heatflow(\n        self,\n        processparams,\n        regex: str = None,\n        cutoff_min: int = 5,\n        upper_dormant_thresh_w_g: float = 0.002,\n        plot_right_boundary=2e5,\n        prominence: float = 1e-3,\n        show_plot=False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        get dormant period heatflow\n\n        Parameters\n        ----------\n        regex : str, optional\n            Regex which can be used to filter the data, i.e., only the patterns which fit the regex will be evaluated. The default is None.\n        cutoff_min : int | float, optional\n            Time at the start of the experiment which will be cutoff from analysis. This can be useful for ex-situ mixed samples. The default is 5.\n        upper_dormant_thresh_w_g : float, optional\n            Parameter which controls the upper limit for the plotting option. The default is 0.001.\n        show_plot : bool, optional\n            If set to true, the data is plotted. The default is False.\n\n        Returns\n        -------\n        Pandas Dataframe\n\n        \"\"\"\n\n        # init results list\n        list_dfs = []\n\n        # loop samples\n        for sample, data in self._iter_samples(regex=regex):\n            # get peak as \"right border\"\n            _peaks = self.get_peaks(\n                processparams,\n                # cutoff_min=cutoff_min,\n                regex=pathlib.Path(sample).name,\n                # prominence=processparams.gradient_peak_prominence, # prominence,\n                show_plot=show_plot,\n            )\n\n            # identify \"dormant period\" as range between initial spike\n            # and first reaction peak\n\n            if show_plot:\n                # plot\n                plt.plot(\n                    data[\"time_s\"],\n                    data[\"normalized_heat_flow_w_g\"],\n                    # linestyle=\"\",\n                    # marker=\"o\",\n                )\n\n            # discard points at early age\n            data = data.query(\"time_s &gt;= @processparams.cutoff.cutoff_min * 60\")\n            if not _peaks.empty:\n                # discard points after the first peak\n                data = data.query('time_s &lt;= @_peaks[\"time_s\"].min()')\n\n            # reset index\n            data = data.reset_index(drop=True)\n\n            # pick relevant points at minimum heat flow\n            data = data.iloc[data[\"normalized_heat_flow_w_g\"].idxmin(), :].to_frame().T\n\n            if show_plot:\n                # guide to the eye lines\n                plt.axhline(float(data[\"normalized_heat_flow_w_g\"]), color=\"red\")\n                plt.axvline(float(data[\"time_s\"]), color=\"red\")\n                # indicate cutoff time\n                plt.axvspan(0, cutoff_min * 60, color=\"black\", alpha=0.5)\n                # limits\n                # plt.xlim(0, _peaks[\"time_s\"].min())\n                plt.xlim(0, plot_right_boundary)\n                plt.ylim(0, upper_dormant_thresh_w_g)\n                # title\n                plt.title(pathlib.Path(sample).stem)\n                # show\n                plt.show()\n\n            # add to list\n            list_dfs.append(data)\n\n        # convert to overall datafram\n        result = pd.concat(list_dfs).reset_index(drop=True)\n\n        # return\n        return result\n\n    #\n    # get ASTM C1679 characteristics\n    #\n\n    def get_astm_c1679_characteristics(\n        self,\n        processparams,\n        individual: bool = False,\n        show_plot=False,\n        ax=None,\n        regex=None,\n        xscale=\"log\",\n        xunit=\"s\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        get characteristics according to ASTM C1679. Compiles a list of data\n        points at half-maximum \"normalized heat flow\", wherein the half maximum\n        is either determined for each individual heat flow curve individually\n        or as the mean value if the heat flow curves considered.\n\n        Parameters\n        ----------\n        individual : bool, optional\n            DESCRIPTION. The default is False.\n        processparams: ProcessingParameters\n            Dataclass containing parameters which control the processing of the calorimetry data.\n\n        Returns\n        -------\n        Pandas Dataframe\n\n        Examples\n        --------\n        Assuming that the calorimetry data is contained in a subfolder `data`, the time according to ASTM c1679 can be obtained by\n\n        &gt;&gt;&gt; from CaloCem import tacalorimetry as ta\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; thepath = Path(__file__).parent / \"data\"\n        &gt;&gt;&gt; tam = ta.Measurement(thepath)\n        &gt;&gt;&gt; astm = tam.get_astm_c1679_characteristics()\n        \"\"\"\n\n        # get peaks\n        peaks = self.get_peaks(processparams, plt_right_s=4e5, show_plot=False)\n        # sort peaks by ascending normalized heat flow\n        peaks = peaks.sort_values(by=\"normalized_heat_flow_w_g\", ascending=True)\n        # select highest peak --&gt; ASTM C1679\n        peaks = peaks.groupby(by=\"sample\").last()\n\n        # get data\n        data = self.get_data()\n\n        # init empty list for collecting characteristics\n        astm_times = []\n\n        # loop samples\n        for sample, sample_data in self._iter_samples(regex=regex):\n            # pick sample data\n            helper = data[data[\"sample\"] == sample]\n            helper_df = helper.copy()\n\n            # check if peak was found\n            if peaks[peaks[\"sample_short\"] == sample_data.sample_short.iloc[0]].empty:\n                helper = helper.iloc[0:1]\n                # manually set time to NaN to indicate that no peak was found\n                helper[\"time_s\"] = np.nan\n\n            else:\n                # restrict to times before the peak\n                helper = helper[helper[\"time_s\"] &lt;= peaks.at[sample, \"time_s\"]]\n\n                # restrict to relevant heatflows the peak\n                if individual == True:\n                    helper = helper[\n                        helper[\"normalized_heat_flow_w_g\"]\n                        &lt;= peaks.at[sample, \"normalized_heat_flow_w_g\"] * 0.50\n                    ]\n                else:\n                    # use half-maximum average\n                    helper = helper[\n                        helper[\"normalized_heat_flow_w_g\"]\n                        &lt;= peaks[\"normalized_heat_flow_w_g\"].mean() * 0.50\n                    ]\n\n                # add to list of of selected points\n            astm_times.append(helper.tail(1))\n\n            if helper.tail(1)[\"time_s\"].isna().all():\n                continue\n\n            if show_plot:\n                # plot\n                if xunit == \"h\":\n                    helper_df[\"time_s\"] = helper_df[\"time_s\"] / 3600\n                    helper[\"time_s\"] = helper[\"time_s\"] / 3600\n                if isinstance(ax, matplotlib.axes._axes.Axes):\n                    ax.plot(\n                        helper_df[\"time_s\"],\n                        helper_df[\"normalized_heat_flow_w_g\"],\n                        label=sample,\n                    )\n                    ax.plot(\n                        helper.tail(1)[\"time_s\"],\n                        helper.tail(1)[\"normalized_heat_flow_w_g\"],\n                        marker=\"o\",\n                        color=\"red\",\n                    )\n                    ax.vlines(\n                        x=helper.tail(1)[\"time_s\"],\n                        ymin=0,\n                        ymax=helper.tail(1)[\"normalized_heat_flow_w_g\"],\n                        color=\"red\",\n                        linestyle=\"--\",\n                    )\n                    ax.text(\n                        x=helper.tail(1)[\"time_s\"],\n                        y=helper.tail(1)[\"normalized_heat_flow_w_g\"] / 2,\n                        s=r\" $t_{ASTM}$ =\"\n                        + f\"{helper.tail(1)['time_s'].values[0]:.1f}\",\n                        color=\"red\",\n                    )\n                else:\n                    plt.plot(\n                        data[\"time_s\"],\n                        data[\"normalized_heat_flow_w_g\"],\n                        label=sample,\n                    )\n\n        # build overall DataFrame\n        astm_times = pd.concat(astm_times)\n\n        # return\n        return astm_times\n\n    #\n    # get data\n    #\n\n    def get_data(self):\n        \"\"\"\n        A convenience function which returns the Pandas Dataframe containing the read and processed calorimetry data.\n        Returns\n        -------\n        Pandas DataFrame\n\n        Examples\n        --------\n        Assuming that the calorimetry data is contained in a subfolder `data`, a conventional Pandas dataframe `df` containing the data from all calorimetry files in `data` can be obtained with the following code.\n\n        &gt;&gt;&gt; from CaloCem import tacalorimetry as ta\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; thepath = Path(__file__).parent / \"data\"\n        &gt;&gt;&gt; tam = ta.Measurement(thepath)\n        &gt;&gt;&gt; df = tam.get_data()\n\n        \"\"\"\n\n        return self._data\n\n    #\n    # get information\n    #\n\n    def get_information(self):\n        \"\"\"\n        get information\n\n        Returns\n        -------\n        pd.DataFrame\n            information, i.e. date of measurement, operator, comment ...\n\n        \"\"\"\n\n        return self._info\n\n    #\n    # get added metadata\n    #\n    def get_metadata(self) -&gt; tuple:\n        \"\"\"\n\n\n         Returns\n         -------\n        tuple\n             pd.DataFrame of metadata and string of the column used as ID (has to\n             be unique).\n        \"\"\"\n\n        # return\n        return self._metadata, self._metadata_id\n\n    #\n    # get sample names\n    #\n\n    def get_sample_names(self):\n        \"\"\"\n        get list of sample names\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # get list\n        samples = [pathlib.Path(s).stem for s, _ in self._iter_samples()]\n\n        # return\n        return samples\n\n    #\n    # set\n    #\n\n    def normalize_sample_to_mass(\n        self, sample_short: str, mass_g: float, show_info=True\n    ):\n        \"\"\"\n        normalize \"heat_flow\" to a certain mass\n\n        Parameters\n        ----------\n        sample_short : str\n            \"sample_short\" name of sample to be normalized.\n        mass_g : float\n            mass in gram to which \"heat_flow_w\" are normalized.\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # normalize \"heat_flow_w\" to sample mass\n        self._data.loc[\n            self._data[\"sample_short\"] == sample_short, \"normalized_heat_flow_w_g\"\n        ] = (\n            self._data.loc[self._data[\"sample_short\"] == sample_short, \"heat_flow_w\"]\n            / mass_g\n        )\n\n        # normalize \"heat_j\" to sample mass\n        try:\n            self._data.loc[\n                self._data[\"sample_short\"] == sample_short, \"normalized_heat_j_g\"\n            ] = (\n                self._data.loc[self._data[\"sample_short\"] == sample_short, \"heat_j\"]\n                / mass_g\n            )\n        except Exception:\n            pass\n\n        # info\n        if show_info:\n            print(f\"Sample {sample_short} normalized to {mass_g}g sample.\")\n\n    #\n    # infer \"heat_j\" values\n    #\n\n    def _infer_heat_j_column(self):\n        \"\"\"\n        helper function to calculate the \"heat_j\" columns from \"heat_flow_w\" and\n        \"time_s\" columns\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # list of dfs\n        list_of_dfs = []\n\n        # loop samples\n        for sample, roi in self._iter_samples():\n            # check whether a \"native\" \"heat_j\"-column is available\n            try:\n                if not roi[\"heat_j\"].isna().all():\n                    # use as is\n                    list_of_dfs.append(roi)\n                    # go to next\n                    continue\n            except KeyError as e:\n                # info\n                print(e)\n\n            # info\n            print(f'==&gt; Inferring \"heat_j\" column for {sample}')\n\n            # get target rows\n            roi = roi.dropna(subset=[\"heat_flow_w\"]).sort_values(by=\"time_s\")\n\n            # inferring cumulated heat using the \"trapezoidal integration method\"\n\n            # introduce helpers\n            roi[\"_h1_y\"] = 0.5 * (\n                roi[\"heat_flow_w\"] + roi[\"heat_flow_w\"].shift(1)\n            ).shift(-1)\n            roi[\"_h2_x\"] = (roi[\"time_s\"] - roi[\"time_s\"].shift(1)).shift(-1)\n\n            # integrate\n            roi[\"heat_j\"] = (roi[\"_h1_y\"] * roi[\"_h2_x\"]).cumsum()\n\n            # clean\n            del roi[\"_h1_y\"], roi[\"_h2_x\"]\n\n            # append to list\n            list_of_dfs.append(roi)\n\n        # set data including \"heat_j\"\n        self._data = pd.concat(list_of_dfs)\n\n    #\n    # remove pickle files\n    #\n    def remove_pickle_files(self):\n        \"\"\"\n        remove pickle files if re-reading of source files needed\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # remove files\n        for file in [self._file_data_pickle, self._file_info_pickle]:\n            # remove file\n            pathlib.Path(file).unlink()\n\n    #\n    # add metadata\n    #\n    def add_metadata_source(self, file: str, sample_id_column: str):\n        \"\"\"\n        add an additional source of metadata the object. The source file is of\n        type \"csv\" or \"xlsx\" and holds information on one sample per row. Columns\n        can be named without restrictions.\n\n        To allow for a mapping, the values occurring in self._data[\"sample_short\"]\n        should appear in the source file. The column is declared via the keyword\n        \"sample_id_colum\"\n\n        Parameters\n        ----------\n        file : str\n            path to additonal metadata source file.\n        sample_id_column : str\n            column name in the additional source file matching self._data[\"sample_short\"].\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        if not pathlib.Path(file).suffix.lower() in [\".csv\", \".xlsx\"]:\n            # info\n            print(\"Please use metadata files of type csv and xlsx only.\")\n            # return\n            return\n\n        # read file\n        try:\n            # read as Excel\n            self._metadata = pd.read_excel(file)\n        except ValueError:\n            # read as csv\n            self._metadata = pd.read_csv(file)\n\n        # save mapper column\n        if sample_id_column in self._metadata.columns:\n            # save mapper column\n            self._metadata_id = sample_id_column\n        else:\n            # raise custom Exception\n            raise AddMetaDataSourceException(self._metadata.columns.tolist())\n\n    #\n    # get metadata group-by options\n    #\n    def get_metadata_grouping_options(self) -&gt; list:\n        \"\"\"\n        get a list of categories to group by in in \"self.plot_by_category\"\n\n        Returns\n        -------\n        list\n            list of categories avaialble for grouping by.\n        \"\"\"\n\n        # get list based on column names of \"_metadata\"\n        return self._metadata.columns.tolist()\n\n    #\n    # average by metadata\n    #\n    def average_by_metadata(\n        self,\n        group_by: str,\n        meta_id=\"experiment_nr\",\n        data_id=\"sample_short\",\n        time_average_window_s: int = None,\n        time_average_log_bin_count: int = None,\n        time_s_max: int = 2 * 24 * 60 * 60,\n        get_time_from=\"left\",\n        resampling_s: str = \"5s\",\n    ):\n        \"\"\"\n\n\n        Parameters\n        ----------\n        group_by : str | list[str]\n            DESCRIPTION.\n        meta_id : TYPE, optional\n            DESCRIPTION. The default is \"experiment_nr\".\n        data_id : TYPE, optional\n            DESCRIPTION. The default is \"sample_short\".\n        time_average_window_s : TYPE, optional\n            DESCRIPTION. The default is 60. The value is not(!) consindered if\n            the keyword time_average_log_bin_count is specified\n        get_time_from : TYPE, optional\n            DESCRIPTION. The default is \"left\". further options: # \"mid\" \"right\"\n\n        time_average_log_bin_count: number of bins if even spacing in logarithmic scale is applied\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # get metadata\n        meta, meta_id = self.get_metadata()\n\n        # get data\n        df = self._data\n\n        # make data equidistant grouped by sample_short\n        df = (\n            df.groupby(data_id)\n            .apply(lambda x: utils.apply_resampling(x, resampling_s))\n            .reset_index(drop=True)\n        )\n\n        # rename sample in \"data\" by metadata grouping options\n        for value, group in meta.groupby(group_by):\n            # if one grouping level is used\n            if isinstance(value, str) or isinstance(value, int):\n                # modify data --&gt; replace \"sample_short\" with metadata group name\n                _idx_to_replace = df[data_id].isin(group[meta_id])\n                df.loc[_idx_to_replace, data_id] = str(value)\n            # if multiple grouping levels are used\n            elif isinstance(value, tuple):\n                # modify data --&gt; replace \"sample_short\" with metadata group name\n                _idx_to_replace = df[data_id].isin(group[meta_id])\n                df.loc[_idx_to_replace, data_id] = \" | \".join([str(x) for x in value])\n            else:\n                pass\n\n        # sort experimentally detected times to \"bins\"\n        if time_average_log_bin_count:\n            # evenly spaced bins on log scale (geometric spacing)\n            df[\"BIN\"] = pd.cut(\n                df[\"time_s\"],\n                np.geomspace(1, time_s_max, num=time_average_log_bin_count),\n            )\n        elif time_average_window_s:\n            # evenly spaced bins on linear scale with fixed width\n            df[\"BIN\"] = pd.cut(\n                df[\"time_s\"], np.arange(0, time_s_max, time_average_window_s)\n            )\n\n        if \"BIN\" in df.columns:\n            # calculate average and std\n            df = (\n                df.groupby([data_id, \"BIN\"])\n                .agg(\n                    {\n                        \"normalized_heat_flow_w_g\": [\"mean\", \"std\"],\n                        \"normalized_heat_j_g\": [\"mean\", \"std\"],\n                    }\n                )\n                .dropna(thresh=2)\n                .reset_index()\n            )\n        else:\n            # calculate average and std\n            df = (\n                df.groupby([data_id, \"time_s\"])\n                .agg(\n                    {\n                        \"normalized_heat_flow_w_g\": [\"mean\", \"std\"],\n                        \"normalized_heat_j_g\": [\"mean\", \"std\"],\n                    }\n                )\n                .dropna(thresh=2)\n                .reset_index()\n            )\n\n        # \"flatten\" column names\n        df.columns = [\"_\".join(i).replace(\"mean\", \"_\").strip(\"_\") for i in df.columns]\n\n        if \"BIN\" in df.columns:\n            # regain \"time_s\" columns\n            if get_time_from == \"left\":\n                df[\"time_s\"] = [i.left for i in df[\"BIN\"]]\n            elif get_time_from == \"mid\":\n                df[\"time_s\"] = [i.mid for i in df[\"BIN\"]]\n            elif get_time_from == \"right\":\n                df[\"time_s\"] = [i.right for i in df[\"BIN\"]]\n\n            # remove \"BIN\" auxiliary column\n            del df[\"BIN\"]\n\n        # copy information to \"sample\" column --&gt; needed for plotting\n        df[\"sample\"] = df[data_id]\n\n        # overwrite data with averaged data\n        self._data = df\n\n    #\n    # undo action of \"average_by_metadata\"\n    #\n    def undo_average_by_metadata(self):\n        \"\"\"\n        undo action of \"average_by_metadata\"\n        \"\"\"\n\n        # set \"unprocessed\" data as exeperimental data / \"de-average\"\n        if not self._data_unprocessed.empty:\n            # reset\n            self._data = self._data_unprocessed.copy()\n\n    #\n    # apply_tian_correction\n    #\n    def apply_tian_correction(\n        self,\n        processparams,  # tau=300, window=11, polynom=3, spline_smoothing_1st: float = 1e-9, spline_smoothing_2nd: float = 1e-9\n    ) -&gt; None:\n        \"\"\"\n        apply_tian_correction\n\n        Parameters\n        ----------\n\n        processparams :\n            ProcessingParameters object containing all processing parameters for calorimetry data.\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # apply the correction for each sample\n        for s, d in self._iter_samples():\n            # get y-data\n            y = d[\"normalized_heat_flow_w_g\"]\n            # NaN-handling in y-data\n            y = y.fillna(0)\n            # get x-data\n            x = d[\"time_s\"]\n\n            processor = HeatFlowProcessor(processparams)\n\n            dydx, dy2dx2 = processor.calculate_heatflow_derivatives(d)\n\n            if processparams.time_constants.tau2 == None:\n                # calculate corrected heatflow\n                norm_hf = (\n                    dydx * processparams.time_constants.tau1\n                    + self._data.loc[\n                        self._data[\"sample\"] == s, \"normalized_heat_flow_w_g\"\n                    ]\n                )\n            else:\n                # calculate corrected heatflow\n                norm_hf = (\n                    dydx\n                    * (\n                        processparams.time_constants.tau1\n                        + processparams.time_constants.tau2\n                    )\n                    + dy2dx2\n                    * processparams.time_constants.tau1\n                    * processparams.time_constants.tau2\n                    + d[\"normalized_heat_flow_w_g\"]\n                )\n\n            self._data.loc[\n                self._data[\"sample\"] == s, \"normalized_heat_flow_w_g_tian\"\n            ] = norm_hf\n\n            self._data.loc[\n                self._data[\"sample\"] == s, \"gradient_normalized_heat_flow_w_g\"\n            ] = dydx\n\n            # calculate corresponding cumulative heat\n            self._data.loc[self._data[\"sample\"] == s, \"normalized_heat_j_g_tian\"] = (\n                integrate.cumulative_trapezoid(norm_hf.fillna(0), x=x, initial=0)\n            )\n\n    #\n    # undo Tian-correction\n    #\n    def undo_tian_correction(self):\n        \"\"\"\n        undo_tian_correction; i.e. restore original data\n\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n\n        # call original restore function\n        self.undo_average_by_metadata()\n\n    def _apply_adaptive_downsampling(self):\n        \"\"\"\n        apply adaptive downsampling to data\n        \"\"\"\n\n        # define temporary empty DataFrame\n        df = pd.DataFrame()\n\n        # apply the correction for each sample\n        for s, d in self._iter_samples():\n            # print(d.sample_short[0])\n            # print(len(d))\n            d = d.dropna(subset=[\"normalized_heat_flow_w_g\"])\n\n            processor = HeatFlowProcessor(self.processparams)\n            d = processor.restrict_data_range(d)\n            # apply adaptive downsampling\n            if not self.processparams.downsample.section_split:\n                d = utils.adaptive_downsample(\n                    d,\n                    x_col=\"time_s\",\n                    y_col=\"normalized_heat_flow_w_g\",\n                    processparams=self.processparams,\n                )\n            else:\n                d = utils.downsample_sections(\n                    d,\n                    x_col=\"time_s\",\n                    y_col=\"normalized_heat_flow_w_g\",\n                    processparams=self.processparams,\n                )\n            df = pd.concat([df, d])\n\n        # set data to downsampled data\n        self._data = df\n\n    def get_ascending_flank_tangent(\n        self,\n        processparams,\n        target_col=\"normalized_heat_flow_w_g\",\n        age_col=\"time_s\",\n        flank_fraction_start=0.2,  # Start at 20% of peak height\n        flank_fraction_end=0.8,  # End at 80% of peak height\n        window_size=0.1,  # Window size as fraction of flank range\n        cutoff_min=None,  # Initial cutoff time in minutes to ignore\n        show_plot=False,\n        regex=None,\n        plotpath=None,\n    ):\n        \"\"\"\n        Determine tangent to ascending flank of peak by averaging over sections.\n\n        Parameters\n        ----------\n        target_col : str\n            Column containing heat flow data\n        age_col : str\n            Column containing time data\n        flank_fraction_start : float\n            Start of flank section as fraction of peak height (0-1)\n        flank_fraction_end : float\n            End of flank section as fraction of peak height (0-1)\n        window_size : float\n            Size of averaging window as fraction of flank time range\n        cutoff_min : float, optional\n            Initial cutoff time in minutes to ignore from analysis. If None,\n            uses processparams.cutoff.cutoff_min. The default is None.\n        show_plot : bool\n            Whether to plot the results\n        regex : str\n            Regex to filter samples\n\n        Returns\n        -------\n        pd.DataFrame\n            DataFrame with tangent characteristics for each sample\n        \"\"\"\n\n        results = []\n\n        for sample, data in self._iter_samples(regex=regex):\n            # Apply cutoff if specified - use parameter cutoff_min first, then fallback to processparams\n            cutoff_time_min = (\n                cutoff_min\n                if cutoff_min is not None\n                else processparams.cutoff.cutoff_min\n            )\n            if cutoff_time_min:\n                data = data.query(f\"{age_col} &gt;= @cutoff_time_min * 60\")\n\n            data = data.reset_index(drop=True)\n\n            # Find the main peak\n            peaks, _ = signal.find_peaks(\n                data[target_col],\n                prominence=processparams.peakdetection.prominence,\n                distance=processparams.peakdetection.distance,\n            )\n\n            if len(peaks) == 0:\n                print(f\"No peak found in {sample}\")\n                continue\n\n            # Use the highest peak\n            peak_idx = peaks[np.argmax(data.iloc[peaks][target_col])]\n            peak_time = data.iloc[peak_idx][age_col]\n            peak_value = data.iloc[peak_idx][target_col]\n\n            # Find baseline (minimum before peak)\n            baseline_data = data[data[age_col] &lt; peak_time]\n            if len(baseline_data) == 0:\n                baseline_value = 0\n            else:\n                baseline_value = baseline_data[target_col].min()\n\n            # Define flank region\n            flank_height_range = peak_value - baseline_value\n            flank_start_value = (\n                baseline_value + flank_fraction_start * flank_height_range\n            )\n            flank_end_value = baseline_value + flank_fraction_end * flank_height_range\n\n            # Calculate gradient to ensure we only consider regions with positive slope\n            data['gradient'] = np.gradient(data[target_col], data[age_col])\n\n            # Extract ascending flank data - only include points with positive gradient\n            flank_data = data[\n                (data[target_col] &gt;= flank_start_value)\n                &amp; (data[target_col] &lt;= flank_end_value)\n                &amp; (data[age_col] &lt;= peak_time)\n                &amp; (data['gradient'] &gt; 0)  # Only positive gradients\n            ].copy()\n\n            # If no positive gradient data in initial range, try to find the lowest point with positive gradient\n            if len(flank_data) &lt; 3:\n                # Find data points with positive gradient before peak\n                positive_gradient_data = data[\n                    (data[age_col] &lt;= peak_time) &amp; (data['gradient'] &gt; 0)\n                ]\n\n                if len(positive_gradient_data) &gt;= 3:\n                    # Adjust flank start to the minimum value with positive gradient\n                    min_positive_value = positive_gradient_data[target_col].min()\n                    adjusted_flank_start = max(flank_start_value, min_positive_value)\n\n                    flank_data = data[\n                        (data[target_col] &gt;= adjusted_flank_start)\n                        &amp; (data[target_col] &lt;= flank_end_value)\n                        &amp; (data[age_col] &lt;= peak_time)\n                        &amp; (data['gradient'] &gt; 0)\n                    ].copy()\n\n                    # Update the flank_start_value for recording\n                    flank_start_value = adjusted_flank_start\n\n            if len(flank_data) &lt; 3:\n                print(f\"Insufficient flank data in {sample}\")\n                continue\n\n            # Calculate moving tangents over windows\n            flank_time_range = flank_data[age_col].max() - flank_data[age_col].min()\n            window_time = window_size * flank_time_range\n\n            tangent_slopes = []\n            tangent_times = []\n            tangent_values = []\n\n            # Slide window across flank\n            start_time = flank_data[age_col].min()\n            end_time = flank_data[age_col].max() - window_time\n\n            step_size = window_time * 0.1  # 10% overlap\n            current_time = start_time\n\n            while current_time &lt;= end_time:\n                window_end = current_time + window_time\n                window_data = flank_data[\n                    (flank_data[age_col] &gt;= current_time)\n                    &amp; (flank_data[age_col] &lt;= window_end)\n                ]\n\n                if len(window_data) &gt;= 3:\n                    # Linear regression for this window\n                    x = window_data[age_col].values\n                    y = window_data[target_col].values\n\n                    # Use numpy polyfit for linear regression\n                    slope, intercept = np.polyfit(x, y, 1)\n\n                    # Only consider positive gradients (ascending flank)\n                    if slope &gt; 0:\n                        # Store tangent info at window center\n                        center_time = (current_time + window_end) / 2\n                        center_value = slope * center_time + intercept\n\n                        tangent_slopes.append(slope)\n                        tangent_times.append(center_time)\n                        tangent_values.append(center_value)\n\n                current_time += step_size\n\n            if not tangent_slopes:\n                print(\n                    f\"No valid tangent windows with positive gradients found in {sample}\"\n                )\n                continue\n\n            # Calculate representative tangent (median to avoid outliers)\n            representative_slope = np.median(tangent_slopes)\n            representative_time = np.median(tangent_times)\n            representative_value = np.median(tangent_values)\n\n            # Calculate tangent line parameters\n            # y = mx + b, so b = y - mx\n            tangent_intercept = (\n                representative_value - representative_slope * representative_time\n            )\n            # calculate x intection\n            # y=0, so x = -b/m\n            x_intersection = (\n                -tangent_intercept / representative_slope if representative_slope != 0 else np.nan\n            )\n\n            # Calculate intersection with horizontal line at minimum before tangent_time_s\n            data_before_tangent = data[data[age_col] &lt;= representative_time]\n            if len(data_before_tangent) &gt; 0:\n                min_value_before_tangent = data_before_tangent[target_col].min()\n                # Intersection: y = min_value = slope * x + intercept\n                # x = (y - intercept) / slope\n                x_intersection_min = (\n                    (min_value_before_tangent - tangent_intercept) / representative_slope \n                    if representative_slope != 0 else np.nan\n                )\n            else:\n                min_value_before_tangent = np.nan\n                x_intersection_min = np.nan\n\n            result = {\n                \"sample\": sample,\n                \"sample_short\": pathlib.Path(sample).stem,\n                \"peak_time_s\": peak_time,\n                \"peak_value\": peak_value,\n                \"tangent_slope\": representative_slope,\n                \"tangent_time_s\": representative_time,\n                \"tangent_value\": representative_value,\n                \"tangent_intercept\": tangent_intercept,\n                \"flank_start_value\": flank_start_value,\n                \"flank_end_value\": flank_end_value,\n                \"n_windows\": len(tangent_slopes),\n                \"slope_std\": np.std(tangent_slopes),\n                \"x_intersection\": x_intersection,\n                \"min_value_before_tangent\": min_value_before_tangent,\n                \"x_intersection_min\": x_intersection_min,\n            }\n\n            results.append(result)\n\n            # Optional plotting\n            if show_plot:\n                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n\n                # Plot 1: Full curve with peak and flank region\n                ax1.plot(data[age_col], data[target_col], \"b-\", alpha=0.7, label=\"Data\")\n                ax1.axvline(\n                    peak_time, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Peak\"\n                )\n                ax1.axhline(flank_start_value, color=\"green\", linestyle=\":\", alpha=0.7)\n                ax1.axhline(flank_end_value, color=\"green\", linestyle=\":\", alpha=0.7)\n                ax1.fill_between(\n                    flank_data[age_col],\n                    flank_start_value,\n                    flank_end_value,\n                    alpha=0.2,\n                    color=\"green\",\n                    label=\"Flank region\",\n                )\n\n                # Plot tangent line\n                x_tangent = np.linspace(x_intersection, peak_time, 10)\n                y_tangent = representative_slope * x_tangent + tangent_intercept\n                ax1.plot(\n                    x_tangent, y_tangent, \"r-\", linewidth=2, label=\"Average tangent\"\n                )\n\n                # Add text label for tangent slope\n                mid_x = (x_intersection + peak_time) / 2\n                mid_y = representative_slope * mid_x + tangent_intercept\n                ax1.annotate(\n                    f\"Slope: {representative_slope:.2e}\",\n                    xy=(mid_x, mid_y),\n                    xytext=(10, 10),\n                    textcoords='offset points',\n                    fontsize=10,\n                    color='red',\n                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8)\n                )\n\n                # Add vertical line and label for x_intersection\n                if not np.isnan(x_intersection) and x_intersection &gt; data[age_col].min():\n                    ax1.axvline(x_intersection, color='orange', linestyle=':', alpha=0.8,) \n                            #    label=f'X-intersection: {x_intersection:.0f}s')\n                    ax1.annotate(\n                        f\"{x_intersection:.0f}s\",\n                        xy=(x_intersection, baseline_value),\n                        xytext=(-50, 20),\n                        textcoords='offset points',\n                        fontsize=10,\n                        color='orange',\n                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8),\n                        arrowprops=dict(arrowstyle='-&gt;', color='orange', alpha=0.6)\n                    )\n\n                # Add horizontal line at minimum and its intersection with tangent\n                if not np.isnan(min_value_before_tangent) and not np.isnan(x_intersection_min):\n                    # Draw horizontal line at minimum value\n                    ax1.axhline(min_value_before_tangent, color='purple', linestyle='--', \n                               alpha=0.7, label=f'Min before tangent: {min_value_before_tangent:.4f}')\n\n                    # Add vertical line at intersection with minimum\n                    if x_intersection_min &gt; data[age_col].min() and x_intersection_min &lt; peak_time:\n                        ax1.axvline(x_intersection_min, color='purple', linestyle=':', alpha=0.8)\n                        ax1.annotate(\n                            f\"Min-int: {x_intersection_min:.0f}s\",\n                            xy=(x_intersection_min, min_value_before_tangent),\n                            xytext=(10, -30),\n                            textcoords='offset points',\n                            fontsize=10,\n                            color='purple',\n                            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8),\n                            arrowprops=dict(arrowstyle='-&gt;', color='purple', alpha=0.6)\n                        )\n\n                ax1.set_xlabel(age_col)\n                ax1.set_ylabel(target_col)\n                ax1.set_title(f\"Peak Analysis: {pathlib.Path(sample).stem}\")\n                ax1.legend()\n                # ax1.grid(True, alpha=0.3)\n                ax1.set_ylim(0,)\n\n                # Plot 2: Slope variation across windows\n                ax2.plot(tangent_times, tangent_slopes, \"bo-\", alpha=0.7)\n                ax2.axhline(\n                    representative_slope,\n                    color=\"red\",\n                    linestyle=\"--\",\n                    label=f\"Median slope: {representative_slope:.2e}\",\n                )\n                ax2.set_xlabel(\"Window center time (s)\")\n                ax2.set_ylabel(\"Local slope\")\n                ax2.set_title(\"Slope variation across flank windows\")\n                ax2.legend()\n                # ax2.grid(True, alpha=0.3)\n                # ax2.set_ylim(0,)\n\n                plt.tight_layout()\n                if plotpath is not None:\n                    filename = pathlib.Path(sample).stem\n                    plt.savefig(plotpath / f\"{filename}.png\")\n                else:\n                    plt.show()\n\n        return pd.DataFrame(results)\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.__init__","title":"<code>__init__(folder=None, show_info=True, regex=None, auto_clean=False, cold_start=True, processparams=None, new_code=False, processed=False)</code>","text":"<p>intialize measurements from folder</p> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def __init__(\n    self,\n    folder=None,\n    show_info=True,\n    regex=None,\n    auto_clean=False,\n    cold_start=True,\n    processparams=None,\n    new_code=False,\n    processed=False,\n):\n    \"\"\"\n    intialize measurements from folder\n\n\n    \"\"\"\n    self._new_code = new_code\n    self._processed = processed\n\n    if not isinstance(processparams, ProcessingParameters):\n        self.processparams = ProcessingParameters()\n    else:\n        self.processparams = processparams\n\n    # read\n    if folder:\n        if cold_start:\n            # get data and parameters\n            self._get_data_and_parameters_from_folder(\n                folder, regex=regex, show_info=show_info\n            )\n        else:\n            # get data and parameters from pickled files\n            self._get_data_and_parameters_from_pickle()\n        try:\n            if auto_clean:\n                # remove NaN values and merge time columns\n                self._auto_clean_data()\n        except Exception as e:\n            # info\n            print(e)\n            raise AutoCleanException\n            # return\n            return\n\n    if self.processparams.downsample.apply:\n        self._apply_adaptive_downsampling()\n    # Message\n    print(\n        \"================\\nAre you missing some samples? Try rerunning with auto_clean=True and cold_start=True.\\n=================\"\n    )\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.add_metadata_source","title":"<code>add_metadata_source(file, sample_id_column)</code>","text":"<p>add an additional source of metadata the object. The source file is of type \"csv\" or \"xlsx\" and holds information on one sample per row. Columns can be named without restrictions.</p> <p>To allow for a mapping, the values occurring in self._data[\"sample_short\"] should appear in the source file. The column is declared via the keyword \"sample_id_colum\"</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>path to additonal metadata source file.</p> required <code>sample_id_column</code> <code>str</code> <p>column name in the additional source file matching self._data[\"sample_short\"].</p> required <p>Returns:</p> Type Description <code>None.</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def add_metadata_source(self, file: str, sample_id_column: str):\n    \"\"\"\n    add an additional source of metadata the object. The source file is of\n    type \"csv\" or \"xlsx\" and holds information on one sample per row. Columns\n    can be named without restrictions.\n\n    To allow for a mapping, the values occurring in self._data[\"sample_short\"]\n    should appear in the source file. The column is declared via the keyword\n    \"sample_id_colum\"\n\n    Parameters\n    ----------\n    file : str\n        path to additonal metadata source file.\n    sample_id_column : str\n        column name in the additional source file matching self._data[\"sample_short\"].\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    if not pathlib.Path(file).suffix.lower() in [\".csv\", \".xlsx\"]:\n        # info\n        print(\"Please use metadata files of type csv and xlsx only.\")\n        # return\n        return\n\n    # read file\n    try:\n        # read as Excel\n        self._metadata = pd.read_excel(file)\n    except ValueError:\n        # read as csv\n        self._metadata = pd.read_csv(file)\n\n    # save mapper column\n    if sample_id_column in self._metadata.columns:\n        # save mapper column\n        self._metadata_id = sample_id_column\n    else:\n        # raise custom Exception\n        raise AddMetaDataSourceException(self._metadata.columns.tolist())\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.apply_tian_correction","title":"<code>apply_tian_correction(processparams)</code>","text":"<p>apply_tian_correction</p> <p>Parameters:</p> Name Type Description Default <code>processparams</code> <p>ProcessingParameters object containing all processing parameters for calorimetry data.</p> required <p>Returns:</p> Type Description <code>None.</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def apply_tian_correction(\n    self,\n    processparams,  # tau=300, window=11, polynom=3, spline_smoothing_1st: float = 1e-9, spline_smoothing_2nd: float = 1e-9\n) -&gt; None:\n    \"\"\"\n    apply_tian_correction\n\n    Parameters\n    ----------\n\n    processparams :\n        ProcessingParameters object containing all processing parameters for calorimetry data.\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # apply the correction for each sample\n    for s, d in self._iter_samples():\n        # get y-data\n        y = d[\"normalized_heat_flow_w_g\"]\n        # NaN-handling in y-data\n        y = y.fillna(0)\n        # get x-data\n        x = d[\"time_s\"]\n\n        processor = HeatFlowProcessor(processparams)\n\n        dydx, dy2dx2 = processor.calculate_heatflow_derivatives(d)\n\n        if processparams.time_constants.tau2 == None:\n            # calculate corrected heatflow\n            norm_hf = (\n                dydx * processparams.time_constants.tau1\n                + self._data.loc[\n                    self._data[\"sample\"] == s, \"normalized_heat_flow_w_g\"\n                ]\n            )\n        else:\n            # calculate corrected heatflow\n            norm_hf = (\n                dydx\n                * (\n                    processparams.time_constants.tau1\n                    + processparams.time_constants.tau2\n                )\n                + dy2dx2\n                * processparams.time_constants.tau1\n                * processparams.time_constants.tau2\n                + d[\"normalized_heat_flow_w_g\"]\n            )\n\n        self._data.loc[\n            self._data[\"sample\"] == s, \"normalized_heat_flow_w_g_tian\"\n        ] = norm_hf\n\n        self._data.loc[\n            self._data[\"sample\"] == s, \"gradient_normalized_heat_flow_w_g\"\n        ] = dydx\n\n        # calculate corresponding cumulative heat\n        self._data.loc[self._data[\"sample\"] == s, \"normalized_heat_j_g_tian\"] = (\n            integrate.cumulative_trapezoid(norm_hf.fillna(0), x=x, initial=0)\n        )\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.average_by_metadata","title":"<code>average_by_metadata(group_by, meta_id='experiment_nr', data_id='sample_short', time_average_window_s=None, time_average_log_bin_count=None, time_s_max=2 * 24 * 60 * 60, get_time_from='left', resampling_s='5s')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>group_by</code> <code>str | list[str]</code> <p>DESCRIPTION.</p> required <code>meta_id</code> <code>TYPE</code> <p>DESCRIPTION. The default is \"experiment_nr\".</p> <code>'experiment_nr'</code> <code>data_id</code> <code>TYPE</code> <p>DESCRIPTION. The default is \"sample_short\".</p> <code>'sample_short'</code> <code>time_average_window_s</code> <code>TYPE</code> <p>DESCRIPTION. The default is 60. The value is not(!) consindered if the keyword time_average_log_bin_count is specified</p> <code>None</code> <code>get_time_from</code> <code>TYPE</code> <p>DESCRIPTION. The default is \"left\". further options: # \"mid\" \"right\"</p> <code>'left'</code> <code>time_average_log_bin_count</code> <code>int</code> <code>None</code> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def average_by_metadata(\n    self,\n    group_by: str,\n    meta_id=\"experiment_nr\",\n    data_id=\"sample_short\",\n    time_average_window_s: int = None,\n    time_average_log_bin_count: int = None,\n    time_s_max: int = 2 * 24 * 60 * 60,\n    get_time_from=\"left\",\n    resampling_s: str = \"5s\",\n):\n    \"\"\"\n\n\n    Parameters\n    ----------\n    group_by : str | list[str]\n        DESCRIPTION.\n    meta_id : TYPE, optional\n        DESCRIPTION. The default is \"experiment_nr\".\n    data_id : TYPE, optional\n        DESCRIPTION. The default is \"sample_short\".\n    time_average_window_s : TYPE, optional\n        DESCRIPTION. The default is 60. The value is not(!) consindered if\n        the keyword time_average_log_bin_count is specified\n    get_time_from : TYPE, optional\n        DESCRIPTION. The default is \"left\". further options: # \"mid\" \"right\"\n\n    time_average_log_bin_count: number of bins if even spacing in logarithmic scale is applied\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # get metadata\n    meta, meta_id = self.get_metadata()\n\n    # get data\n    df = self._data\n\n    # make data equidistant grouped by sample_short\n    df = (\n        df.groupby(data_id)\n        .apply(lambda x: utils.apply_resampling(x, resampling_s))\n        .reset_index(drop=True)\n    )\n\n    # rename sample in \"data\" by metadata grouping options\n    for value, group in meta.groupby(group_by):\n        # if one grouping level is used\n        if isinstance(value, str) or isinstance(value, int):\n            # modify data --&gt; replace \"sample_short\" with metadata group name\n            _idx_to_replace = df[data_id].isin(group[meta_id])\n            df.loc[_idx_to_replace, data_id] = str(value)\n        # if multiple grouping levels are used\n        elif isinstance(value, tuple):\n            # modify data --&gt; replace \"sample_short\" with metadata group name\n            _idx_to_replace = df[data_id].isin(group[meta_id])\n            df.loc[_idx_to_replace, data_id] = \" | \".join([str(x) for x in value])\n        else:\n            pass\n\n    # sort experimentally detected times to \"bins\"\n    if time_average_log_bin_count:\n        # evenly spaced bins on log scale (geometric spacing)\n        df[\"BIN\"] = pd.cut(\n            df[\"time_s\"],\n            np.geomspace(1, time_s_max, num=time_average_log_bin_count),\n        )\n    elif time_average_window_s:\n        # evenly spaced bins on linear scale with fixed width\n        df[\"BIN\"] = pd.cut(\n            df[\"time_s\"], np.arange(0, time_s_max, time_average_window_s)\n        )\n\n    if \"BIN\" in df.columns:\n        # calculate average and std\n        df = (\n            df.groupby([data_id, \"BIN\"])\n            .agg(\n                {\n                    \"normalized_heat_flow_w_g\": [\"mean\", \"std\"],\n                    \"normalized_heat_j_g\": [\"mean\", \"std\"],\n                }\n            )\n            .dropna(thresh=2)\n            .reset_index()\n        )\n    else:\n        # calculate average and std\n        df = (\n            df.groupby([data_id, \"time_s\"])\n            .agg(\n                {\n                    \"normalized_heat_flow_w_g\": [\"mean\", \"std\"],\n                    \"normalized_heat_j_g\": [\"mean\", \"std\"],\n                }\n            )\n            .dropna(thresh=2)\n            .reset_index()\n        )\n\n    # \"flatten\" column names\n    df.columns = [\"_\".join(i).replace(\"mean\", \"_\").strip(\"_\") for i in df.columns]\n\n    if \"BIN\" in df.columns:\n        # regain \"time_s\" columns\n        if get_time_from == \"left\":\n            df[\"time_s\"] = [i.left for i in df[\"BIN\"]]\n        elif get_time_from == \"mid\":\n            df[\"time_s\"] = [i.mid for i in df[\"BIN\"]]\n        elif get_time_from == \"right\":\n            df[\"time_s\"] = [i.right for i in df[\"BIN\"]]\n\n        # remove \"BIN\" auxiliary column\n        del df[\"BIN\"]\n\n    # copy information to \"sample\" column --&gt; needed for plotting\n    df[\"sample\"] = df[data_id]\n\n    # overwrite data with averaged data\n    self._data = df\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_ascending_flank_tangent","title":"<code>get_ascending_flank_tangent(processparams, target_col='normalized_heat_flow_w_g', age_col='time_s', flank_fraction_start=0.2, flank_fraction_end=0.8, window_size=0.1, cutoff_min=None, show_plot=False, regex=None, plotpath=None)</code>","text":"<p>Determine tangent to ascending flank of peak by averaging over sections.</p> <p>Parameters:</p> Name Type Description Default <code>target_col</code> <code>str</code> <p>Column containing heat flow data</p> <code>'normalized_heat_flow_w_g'</code> <code>age_col</code> <code>str</code> <p>Column containing time data</p> <code>'time_s'</code> <code>flank_fraction_start</code> <code>float</code> <p>Start of flank section as fraction of peak height (0-1)</p> <code>0.2</code> <code>flank_fraction_end</code> <code>float</code> <p>End of flank section as fraction of peak height (0-1)</p> <code>0.8</code> <code>window_size</code> <code>float</code> <p>Size of averaging window as fraction of flank time range</p> <code>0.1</code> <code>cutoff_min</code> <code>float</code> <p>Initial cutoff time in minutes to ignore from analysis. If None, uses processparams.cutoff.cutoff_min. The default is None.</p> <code>None</code> <code>show_plot</code> <code>bool</code> <p>Whether to plot the results</p> <code>False</code> <code>regex</code> <code>str</code> <p>Regex to filter samples</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with tangent characteristics for each sample</p> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_ascending_flank_tangent(\n    self,\n    processparams,\n    target_col=\"normalized_heat_flow_w_g\",\n    age_col=\"time_s\",\n    flank_fraction_start=0.2,  # Start at 20% of peak height\n    flank_fraction_end=0.8,  # End at 80% of peak height\n    window_size=0.1,  # Window size as fraction of flank range\n    cutoff_min=None,  # Initial cutoff time in minutes to ignore\n    show_plot=False,\n    regex=None,\n    plotpath=None,\n):\n    \"\"\"\n    Determine tangent to ascending flank of peak by averaging over sections.\n\n    Parameters\n    ----------\n    target_col : str\n        Column containing heat flow data\n    age_col : str\n        Column containing time data\n    flank_fraction_start : float\n        Start of flank section as fraction of peak height (0-1)\n    flank_fraction_end : float\n        End of flank section as fraction of peak height (0-1)\n    window_size : float\n        Size of averaging window as fraction of flank time range\n    cutoff_min : float, optional\n        Initial cutoff time in minutes to ignore from analysis. If None,\n        uses processparams.cutoff.cutoff_min. The default is None.\n    show_plot : bool\n        Whether to plot the results\n    regex : str\n        Regex to filter samples\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with tangent characteristics for each sample\n    \"\"\"\n\n    results = []\n\n    for sample, data in self._iter_samples(regex=regex):\n        # Apply cutoff if specified - use parameter cutoff_min first, then fallback to processparams\n        cutoff_time_min = (\n            cutoff_min\n            if cutoff_min is not None\n            else processparams.cutoff.cutoff_min\n        )\n        if cutoff_time_min:\n            data = data.query(f\"{age_col} &gt;= @cutoff_time_min * 60\")\n\n        data = data.reset_index(drop=True)\n\n        # Find the main peak\n        peaks, _ = signal.find_peaks(\n            data[target_col],\n            prominence=processparams.peakdetection.prominence,\n            distance=processparams.peakdetection.distance,\n        )\n\n        if len(peaks) == 0:\n            print(f\"No peak found in {sample}\")\n            continue\n\n        # Use the highest peak\n        peak_idx = peaks[np.argmax(data.iloc[peaks][target_col])]\n        peak_time = data.iloc[peak_idx][age_col]\n        peak_value = data.iloc[peak_idx][target_col]\n\n        # Find baseline (minimum before peak)\n        baseline_data = data[data[age_col] &lt; peak_time]\n        if len(baseline_data) == 0:\n            baseline_value = 0\n        else:\n            baseline_value = baseline_data[target_col].min()\n\n        # Define flank region\n        flank_height_range = peak_value - baseline_value\n        flank_start_value = (\n            baseline_value + flank_fraction_start * flank_height_range\n        )\n        flank_end_value = baseline_value + flank_fraction_end * flank_height_range\n\n        # Calculate gradient to ensure we only consider regions with positive slope\n        data['gradient'] = np.gradient(data[target_col], data[age_col])\n\n        # Extract ascending flank data - only include points with positive gradient\n        flank_data = data[\n            (data[target_col] &gt;= flank_start_value)\n            &amp; (data[target_col] &lt;= flank_end_value)\n            &amp; (data[age_col] &lt;= peak_time)\n            &amp; (data['gradient'] &gt; 0)  # Only positive gradients\n        ].copy()\n\n        # If no positive gradient data in initial range, try to find the lowest point with positive gradient\n        if len(flank_data) &lt; 3:\n            # Find data points with positive gradient before peak\n            positive_gradient_data = data[\n                (data[age_col] &lt;= peak_time) &amp; (data['gradient'] &gt; 0)\n            ]\n\n            if len(positive_gradient_data) &gt;= 3:\n                # Adjust flank start to the minimum value with positive gradient\n                min_positive_value = positive_gradient_data[target_col].min()\n                adjusted_flank_start = max(flank_start_value, min_positive_value)\n\n                flank_data = data[\n                    (data[target_col] &gt;= adjusted_flank_start)\n                    &amp; (data[target_col] &lt;= flank_end_value)\n                    &amp; (data[age_col] &lt;= peak_time)\n                    &amp; (data['gradient'] &gt; 0)\n                ].copy()\n\n                # Update the flank_start_value for recording\n                flank_start_value = adjusted_flank_start\n\n        if len(flank_data) &lt; 3:\n            print(f\"Insufficient flank data in {sample}\")\n            continue\n\n        # Calculate moving tangents over windows\n        flank_time_range = flank_data[age_col].max() - flank_data[age_col].min()\n        window_time = window_size * flank_time_range\n\n        tangent_slopes = []\n        tangent_times = []\n        tangent_values = []\n\n        # Slide window across flank\n        start_time = flank_data[age_col].min()\n        end_time = flank_data[age_col].max() - window_time\n\n        step_size = window_time * 0.1  # 10% overlap\n        current_time = start_time\n\n        while current_time &lt;= end_time:\n            window_end = current_time + window_time\n            window_data = flank_data[\n                (flank_data[age_col] &gt;= current_time)\n                &amp; (flank_data[age_col] &lt;= window_end)\n            ]\n\n            if len(window_data) &gt;= 3:\n                # Linear regression for this window\n                x = window_data[age_col].values\n                y = window_data[target_col].values\n\n                # Use numpy polyfit for linear regression\n                slope, intercept = np.polyfit(x, y, 1)\n\n                # Only consider positive gradients (ascending flank)\n                if slope &gt; 0:\n                    # Store tangent info at window center\n                    center_time = (current_time + window_end) / 2\n                    center_value = slope * center_time + intercept\n\n                    tangent_slopes.append(slope)\n                    tangent_times.append(center_time)\n                    tangent_values.append(center_value)\n\n            current_time += step_size\n\n        if not tangent_slopes:\n            print(\n                f\"No valid tangent windows with positive gradients found in {sample}\"\n            )\n            continue\n\n        # Calculate representative tangent (median to avoid outliers)\n        representative_slope = np.median(tangent_slopes)\n        representative_time = np.median(tangent_times)\n        representative_value = np.median(tangent_values)\n\n        # Calculate tangent line parameters\n        # y = mx + b, so b = y - mx\n        tangent_intercept = (\n            representative_value - representative_slope * representative_time\n        )\n        # calculate x intection\n        # y=0, so x = -b/m\n        x_intersection = (\n            -tangent_intercept / representative_slope if representative_slope != 0 else np.nan\n        )\n\n        # Calculate intersection with horizontal line at minimum before tangent_time_s\n        data_before_tangent = data[data[age_col] &lt;= representative_time]\n        if len(data_before_tangent) &gt; 0:\n            min_value_before_tangent = data_before_tangent[target_col].min()\n            # Intersection: y = min_value = slope * x + intercept\n            # x = (y - intercept) / slope\n            x_intersection_min = (\n                (min_value_before_tangent - tangent_intercept) / representative_slope \n                if representative_slope != 0 else np.nan\n            )\n        else:\n            min_value_before_tangent = np.nan\n            x_intersection_min = np.nan\n\n        result = {\n            \"sample\": sample,\n            \"sample_short\": pathlib.Path(sample).stem,\n            \"peak_time_s\": peak_time,\n            \"peak_value\": peak_value,\n            \"tangent_slope\": representative_slope,\n            \"tangent_time_s\": representative_time,\n            \"tangent_value\": representative_value,\n            \"tangent_intercept\": tangent_intercept,\n            \"flank_start_value\": flank_start_value,\n            \"flank_end_value\": flank_end_value,\n            \"n_windows\": len(tangent_slopes),\n            \"slope_std\": np.std(tangent_slopes),\n            \"x_intersection\": x_intersection,\n            \"min_value_before_tangent\": min_value_before_tangent,\n            \"x_intersection_min\": x_intersection_min,\n        }\n\n        results.append(result)\n\n        # Optional plotting\n        if show_plot:\n            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n\n            # Plot 1: Full curve with peak and flank region\n            ax1.plot(data[age_col], data[target_col], \"b-\", alpha=0.7, label=\"Data\")\n            ax1.axvline(\n                peak_time, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Peak\"\n            )\n            ax1.axhline(flank_start_value, color=\"green\", linestyle=\":\", alpha=0.7)\n            ax1.axhline(flank_end_value, color=\"green\", linestyle=\":\", alpha=0.7)\n            ax1.fill_between(\n                flank_data[age_col],\n                flank_start_value,\n                flank_end_value,\n                alpha=0.2,\n                color=\"green\",\n                label=\"Flank region\",\n            )\n\n            # Plot tangent line\n            x_tangent = np.linspace(x_intersection, peak_time, 10)\n            y_tangent = representative_slope * x_tangent + tangent_intercept\n            ax1.plot(\n                x_tangent, y_tangent, \"r-\", linewidth=2, label=\"Average tangent\"\n            )\n\n            # Add text label for tangent slope\n            mid_x = (x_intersection + peak_time) / 2\n            mid_y = representative_slope * mid_x + tangent_intercept\n            ax1.annotate(\n                f\"Slope: {representative_slope:.2e}\",\n                xy=(mid_x, mid_y),\n                xytext=(10, 10),\n                textcoords='offset points',\n                fontsize=10,\n                color='red',\n                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8)\n            )\n\n            # Add vertical line and label for x_intersection\n            if not np.isnan(x_intersection) and x_intersection &gt; data[age_col].min():\n                ax1.axvline(x_intersection, color='orange', linestyle=':', alpha=0.8,) \n                        #    label=f'X-intersection: {x_intersection:.0f}s')\n                ax1.annotate(\n                    f\"{x_intersection:.0f}s\",\n                    xy=(x_intersection, baseline_value),\n                    xytext=(-50, 20),\n                    textcoords='offset points',\n                    fontsize=10,\n                    color='orange',\n                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8),\n                    arrowprops=dict(arrowstyle='-&gt;', color='orange', alpha=0.6)\n                )\n\n            # Add horizontal line at minimum and its intersection with tangent\n            if not np.isnan(min_value_before_tangent) and not np.isnan(x_intersection_min):\n                # Draw horizontal line at minimum value\n                ax1.axhline(min_value_before_tangent, color='purple', linestyle='--', \n                           alpha=0.7, label=f'Min before tangent: {min_value_before_tangent:.4f}')\n\n                # Add vertical line at intersection with minimum\n                if x_intersection_min &gt; data[age_col].min() and x_intersection_min &lt; peak_time:\n                    ax1.axvline(x_intersection_min, color='purple', linestyle=':', alpha=0.8)\n                    ax1.annotate(\n                        f\"Min-int: {x_intersection_min:.0f}s\",\n                        xy=(x_intersection_min, min_value_before_tangent),\n                        xytext=(10, -30),\n                        textcoords='offset points',\n                        fontsize=10,\n                        color='purple',\n                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8),\n                        arrowprops=dict(arrowstyle='-&gt;', color='purple', alpha=0.6)\n                    )\n\n            ax1.set_xlabel(age_col)\n            ax1.set_ylabel(target_col)\n            ax1.set_title(f\"Peak Analysis: {pathlib.Path(sample).stem}\")\n            ax1.legend()\n            # ax1.grid(True, alpha=0.3)\n            ax1.set_ylim(0,)\n\n            # Plot 2: Slope variation across windows\n            ax2.plot(tangent_times, tangent_slopes, \"bo-\", alpha=0.7)\n            ax2.axhline(\n                representative_slope,\n                color=\"red\",\n                linestyle=\"--\",\n                label=f\"Median slope: {representative_slope:.2e}\",\n            )\n            ax2.set_xlabel(\"Window center time (s)\")\n            ax2.set_ylabel(\"Local slope\")\n            ax2.set_title(\"Slope variation across flank windows\")\n            ax2.legend()\n            # ax2.grid(True, alpha=0.3)\n            # ax2.set_ylim(0,)\n\n            plt.tight_layout()\n            if plotpath is not None:\n                filename = pathlib.Path(sample).stem\n                plt.savefig(plotpath / f\"{filename}.png\")\n            else:\n                plt.show()\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_astm_c1679_characteristics","title":"<code>get_astm_c1679_characteristics(processparams, individual=False, show_plot=False, ax=None, regex=None, xscale='log', xunit='s')</code>","text":"<p>get characteristics according to ASTM C1679. Compiles a list of data points at half-maximum \"normalized heat flow\", wherein the half maximum is either determined for each individual heat flow curve individually or as the mean value if the heat flow curves considered.</p> <p>Parameters:</p> Name Type Description Default <code>individual</code> <code>bool</code> <p>DESCRIPTION. The default is False.</p> <code>False</code> <code>processparams</code> <p>Dataclass containing parameters which control the processing of the calorimetry data.</p> required <p>Returns:</p> Type Description <code>Pandas Dataframe</code> <p>Examples:</p> <p>Assuming that the calorimetry data is contained in a subfolder <code>data</code>, the time according to ASTM c1679 can be obtained by</p> <pre><code>&gt;&gt;&gt; from CaloCem import tacalorimetry as ta\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt;\n&gt;&gt;&gt; thepath = Path(__file__).parent / \"data\"\n&gt;&gt;&gt; tam = ta.Measurement(thepath)\n&gt;&gt;&gt; astm = tam.get_astm_c1679_characteristics()\n</code></pre> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_astm_c1679_characteristics(\n    self,\n    processparams,\n    individual: bool = False,\n    show_plot=False,\n    ax=None,\n    regex=None,\n    xscale=\"log\",\n    xunit=\"s\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    get characteristics according to ASTM C1679. Compiles a list of data\n    points at half-maximum \"normalized heat flow\", wherein the half maximum\n    is either determined for each individual heat flow curve individually\n    or as the mean value if the heat flow curves considered.\n\n    Parameters\n    ----------\n    individual : bool, optional\n        DESCRIPTION. The default is False.\n    processparams: ProcessingParameters\n        Dataclass containing parameters which control the processing of the calorimetry data.\n\n    Returns\n    -------\n    Pandas Dataframe\n\n    Examples\n    --------\n    Assuming that the calorimetry data is contained in a subfolder `data`, the time according to ASTM c1679 can be obtained by\n\n    &gt;&gt;&gt; from CaloCem import tacalorimetry as ta\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; thepath = Path(__file__).parent / \"data\"\n    &gt;&gt;&gt; tam = ta.Measurement(thepath)\n    &gt;&gt;&gt; astm = tam.get_astm_c1679_characteristics()\n    \"\"\"\n\n    # get peaks\n    peaks = self.get_peaks(processparams, plt_right_s=4e5, show_plot=False)\n    # sort peaks by ascending normalized heat flow\n    peaks = peaks.sort_values(by=\"normalized_heat_flow_w_g\", ascending=True)\n    # select highest peak --&gt; ASTM C1679\n    peaks = peaks.groupby(by=\"sample\").last()\n\n    # get data\n    data = self.get_data()\n\n    # init empty list for collecting characteristics\n    astm_times = []\n\n    # loop samples\n    for sample, sample_data in self._iter_samples(regex=regex):\n        # pick sample data\n        helper = data[data[\"sample\"] == sample]\n        helper_df = helper.copy()\n\n        # check if peak was found\n        if peaks[peaks[\"sample_short\"] == sample_data.sample_short.iloc[0]].empty:\n            helper = helper.iloc[0:1]\n            # manually set time to NaN to indicate that no peak was found\n            helper[\"time_s\"] = np.nan\n\n        else:\n            # restrict to times before the peak\n            helper = helper[helper[\"time_s\"] &lt;= peaks.at[sample, \"time_s\"]]\n\n            # restrict to relevant heatflows the peak\n            if individual == True:\n                helper = helper[\n                    helper[\"normalized_heat_flow_w_g\"]\n                    &lt;= peaks.at[sample, \"normalized_heat_flow_w_g\"] * 0.50\n                ]\n            else:\n                # use half-maximum average\n                helper = helper[\n                    helper[\"normalized_heat_flow_w_g\"]\n                    &lt;= peaks[\"normalized_heat_flow_w_g\"].mean() * 0.50\n                ]\n\n            # add to list of of selected points\n        astm_times.append(helper.tail(1))\n\n        if helper.tail(1)[\"time_s\"].isna().all():\n            continue\n\n        if show_plot:\n            # plot\n            if xunit == \"h\":\n                helper_df[\"time_s\"] = helper_df[\"time_s\"] / 3600\n                helper[\"time_s\"] = helper[\"time_s\"] / 3600\n            if isinstance(ax, matplotlib.axes._axes.Axes):\n                ax.plot(\n                    helper_df[\"time_s\"],\n                    helper_df[\"normalized_heat_flow_w_g\"],\n                    label=sample,\n                )\n                ax.plot(\n                    helper.tail(1)[\"time_s\"],\n                    helper.tail(1)[\"normalized_heat_flow_w_g\"],\n                    marker=\"o\",\n                    color=\"red\",\n                )\n                ax.vlines(\n                    x=helper.tail(1)[\"time_s\"],\n                    ymin=0,\n                    ymax=helper.tail(1)[\"normalized_heat_flow_w_g\"],\n                    color=\"red\",\n                    linestyle=\"--\",\n                )\n                ax.text(\n                    x=helper.tail(1)[\"time_s\"],\n                    y=helper.tail(1)[\"normalized_heat_flow_w_g\"] / 2,\n                    s=r\" $t_{ASTM}$ =\"\n                    + f\"{helper.tail(1)['time_s'].values[0]:.1f}\",\n                    color=\"red\",\n                )\n            else:\n                plt.plot(\n                    data[\"time_s\"],\n                    data[\"normalized_heat_flow_w_g\"],\n                    label=sample,\n                )\n\n    # build overall DataFrame\n    astm_times = pd.concat(astm_times)\n\n    # return\n    return astm_times\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_average_slope","title":"<code>get_average_slope(processparams, target_col='normalized_heat_flow_w_g', age_col='time_s', regex=None, show_plot=False, ax=None, save_path=None, xscale='linear', xunit='s')</code>","text":"<p>Calculate average slope by determining 4 additional slope values between  onset time and heat flow maximum, in addition to the maximum slope.</p> <p>Parameters:</p> Name Type Description Default <code>processparams</code> <code>ProcessingParameters</code> <p>Processing parameters for analysis</p> required <code>target_col</code> <code>str</code> <p>Target measurement column, by default \"normalized_heat_flow_w_g\"</p> <code>'normalized_heat_flow_w_g'</code> <code>age_col</code> <code>str</code> <p>Time column name, by default \"time_s\"</p> <code>'time_s'</code> <code>regex</code> <code>str</code> <p>Regex pattern to filter samples, by default None</p> <code>None</code> <code>show_plot</code> <code>bool</code> <p>Whether to show plots, by default False</p> <code>False</code> <code>ax</code> <code>Axes</code> <p>Existing axis to plot on, by default None</p> <code>None</code> <code>save_path</code> <code>Path</code> <p>Path to save plots, by default None</p> <code>None</code> <code>xscale</code> <code>str</code> <p>X-axis scale, by default \"log\"</p> <code>'linear'</code> <code>xunit</code> <code>str</code> <p>Time unit for display, by default \"s\"</p> <code>'s'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing average slope characteristics for each sample</p> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_average_slope(\nself,\nprocessparams,\ntarget_col=\"normalized_heat_flow_w_g\",\nage_col=\"time_s\",\nregex=None,\nshow_plot=False,\nax=None,\nsave_path=None,\nxscale=\"linear\",\nxunit=\"s\",\n):\n    \"\"\"\n    Calculate average slope by determining 4 additional slope values between \n    onset time and heat flow maximum, in addition to the maximum slope.\n\n    Parameters\n    ----------\n    processparams : ProcessingParameters\n        Processing parameters for analysis\n    target_col : str, optional\n        Target measurement column, by default \"normalized_heat_flow_w_g\"\n    age_col : str, optional\n        Time column name, by default \"time_s\"\n    regex : str, optional\n        Regex pattern to filter samples, by default None\n    show_plot : bool, optional\n        Whether to show plots, by default False\n    ax : matplotlib.axes.Axes, optional\n        Existing axis to plot on, by default None\n    save_path : Path, optional\n        Path to save plots, by default None\n    xscale : str, optional\n        X-axis scale, by default \"log\"\n    xunit : str, optional\n        Time unit for display, by default \"s\"\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing average slope characteristics for each sample\n    \"\"\"\n\n    # Get maximum slopes using existing method\n    max_slopes = self.get_maximum_slope(\n        processparams,\n        target_col=target_col,\n        age_col=age_col,\n        regex=regex,\n        show_plot=False,\n        ax=ax,\n        save_path=save_path,\n        xscale=xscale,\n        xunit=xunit,\n    )\n\n    if max_slopes is None or max_slopes.empty:\n        print(\"No maximum slopes found. Cannot calculate average slopes.\")\n        return pd.DataFrame()\n\n    # Get onset times using the peak onset method\n    onsets = self.get_peak_onset_via_max_slope(\n        processparams,\n        show_plot=False,\n        regex=regex,\n        age_col=age_col,\n        target_col=target_col,\n        xunit=xunit,\n    )\n\n    if onsets.empty:\n        print(\"No onset times found. Cannot calculate average slopes.\")\n        return pd.DataFrame()\n\n    list_of_characteristics = []\n\n    # Loop through samples\n    for sample, data in self._iter_samples(regex=regex):\n        sample_short = pathlib.Path(sample).stem\n\n        # Get max slope data for this sample\n        max_slope_row = max_slopes[max_slopes[\"sample_short\"] == sample_short]\n        if max_slope_row.empty:\n            continue\n\n        # Get onset data for this sample\n        onset_row = onsets[onsets[\"sample\"] == sample_short]\n        if onset_row.empty:\n            continue\n\n        # Get time points\n        onset_time = onset_row[\"onset_time_s\"].iloc[0]\n        max_slope_time = max_slope_row[age_col].iloc[0]\n\n        # Find heat flow maximum after onset\n        data_after_onset = data[data[age_col] &gt;= onset_time]\n        if data_after_onset.empty:\n            continue\n\n        max_hf_time = data_after_onset.loc[data_after_onset[target_col].idxmax(), age_col]\n\n        # Create 4 intermediate time points between onset and heat flow maximum\n        if max_hf_time &lt;= onset_time:\n            print(f\"Warning: Heat flow maximum occurs before onset for {sample_short}\")\n            continue\n\n        # Create 6 time points total (onset, 4 intermediate, max_hf)\n        time_points = np.linspace(onset_time + 3600, max_hf_time - 3600, 6)\n\n        # Calculate slopes at each interval\n        slopes = []\n        slope_times = []\n\n        for i in range(len(time_points) - 1):\n            t1, t2 = time_points[i], time_points[i + 1]\n\n            # Get data points in this interval\n            interval_data = data[(data[age_col] &gt;= t1) &amp; (data[age_col] &lt;= t2)]\n\n            if len(interval_data) &lt; 2:\n                continue\n\n            # Calculate slope using linear regression\n            x_vals = interval_data[age_col].values\n            y_vals = interval_data[target_col].values\n\n            # Simple slope calculation: (y2 - y1) / (x2 - x1)\n            slope = (y_vals[-1] - y_vals[0]) / (x_vals[-1] - x_vals[0])\n            slopes.append(slope)\n            slope_times.append((t1 + t2) / 2)  # Midpoint time\n\n        # Include the maximum slope\n        max_slope_value = max_slope_row[\"gradient\"].iloc[0]\n        slopes.append(max_slope_value)\n        slope_times.append(max_slope_time)\n\n        # Calculate average slope\n        if slopes:\n            avg_slope = np.mean(slopes)\n            std_slope = np.std(slopes)\n\n            # Create characteristics dictionary\n            characteristics = {\n                \"sample\": sample,\n                \"sample_short\": sample_short,\n                \"onset_time_s\": onset_time,\n                \"max_hf_time_s\": max_hf_time,\n                \"max_slope_time_s\": max_slope_time,\n                \"max_slope_value\": max_slope_value,\n                \"average_slope\": avg_slope,\n                \"slope_std\": std_slope,\n                \"n_slopes\": len(slopes),\n                \"individual_slopes\": slopes,\n                \"slope_times\": slope_times,\n            }\n\n            # Optional plotting\n            if show_plot:\n                self._plot_average_slope_analysis(\n                    data, characteristics, ax, age_col, target_col, \n                    sample_short, save_path, xscale, xunit\n                )\n\n            list_of_characteristics.append(characteristics)\n\n    if not list_of_characteristics:\n        print(\"No average slope characteristics calculated.\")\n        return pd.DataFrame()\n\n    # Convert to DataFrame\n    avg_slope_df = pd.DataFrame(list_of_characteristics)\n\n    return avg_slope_df\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_cumulated_heat_at_hours","title":"<code>get_cumulated_heat_at_hours(processparams=None, target_h=4, **kwargs)</code>","text":"<p>get the cumulated heat flow a at a certain age</p> <p>Parameters:</p> Name Type Description Default <code>processparams</code> <code>ProcessingParameters</code> <p>Processing parameters. The default is None. If None, the default parameters are used. The most important parameter is the cutoff time in minutes which describes the initial time period of the measurement which is not considered for the cumulated heat flow. It is defined in the ProcessingParameters class. The default value is 30 minutes.</p> <code>None</code> <code>target_h</code> <code>int | float</code> <p>end time in hourscv</p> <code>4</code> <p>Returns:</p> Type Description <code>A Pandas dataframe</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_cumulated_heat_at_hours(self, processparams=None, target_h=4, **kwargs):\n    \"\"\"\n    get the cumulated heat flow a at a certain age\n\n    Parameters\n    ----------\n    processparams : ProcessingParameters, optional\n        Processing parameters. The default is None. If None, the default\n        parameters are used. The most important parameter is the cutoff time\n        in minutes which describes the initial time period of the measurement\n        which is not considered for the cumulated heat flow. It is defined in\n        the ProcessingParameters class. The default value is 30 minutes.\n\n    target_h : int | float\n        end time in hourscv\n\n    Returns\n    -------\n    A Pandas dataframe\n\n    \"\"\"\n    if \"cutoff_min\" in kwargs:\n        cutoff_min = kwargs[\"cutoff_min\"]\n        warnings.warn(\n            \"The cutoff_min parameter is deprecated. Please use the ProcessingParameters class instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    else:\n        if not processparams:\n            processparams = ProcessingParameters()\n        cutoff_min = processparams.cutoff.cutoff_min\n\n    def applicable(df, target_h=4, cutoff_min=None):\n        # convert target time to seconds\n        target_s = 3600 * target_h\n        # helper\n        _helper = df.query(\"time_s &lt;= @target_s\").tail(1)\n        # get heat at target time\n        hf_at_target = float(_helper[\"normalized_heat_j_g\"].values[0])\n\n        # if cutoff time specified\n        if cutoff_min:\n            # convert target time to seconds\n            target_s = 60 * cutoff_min\n            try:\n                # helper\n                _helper = df.query(\"time_s &lt;= @target_s\").tail(1)\n                # type conversion\n                hf_at_cutoff = float(_helper[\"normalized_heat_j_g\"].values[0])\n                # correct heatflow for heatflow at cutoff\n                hf_at_target = hf_at_target - hf_at_cutoff\n            except TypeError:\n                name_wt_nan = df[\"sample_short\"].tolist()[0]\n                print(\n                    f\"Found NaN in Normalized heat of sample {name_wt_nan} searching for cumulated heat at {target_h}h and a cutoff of {cutoff_min}min.\"\n                )\n                return np.nan\n\n        # return\n        return hf_at_target\n\n    # in case of one specified time\n    if isinstance(target_h, int) or isinstance(target_h, float):\n        # groupby\n        results = (\n            self._data.groupby(by=\"sample\")[[\"time_s\", \"normalized_heat_j_g\"]]\n            .apply(\n                lambda x: applicable(x, target_h=target_h, cutoff_min=cutoff_min),\n            )\n            .reset_index(level=0)\n        )\n        # rename\n        results.columns = [\"sample\", \"cumulated_heat_at_hours\"]\n        results[\"target_h\"] = target_h\n        results[\"cutoff_min\"] = cutoff_min\n\n    # in case of specified list of times\n    elif isinstance(target_h, list):\n        # init list\n        list_of_results = []\n        # loop\n        for this_target_h in target_h:\n            # groupby\n            _results = (\n                self._data.groupby(by=\"sample\")[[\"time_s\", \"normalized_heat_j_g\"]]\n                .apply(\n                    lambda x: applicable(\n                        x, target_h=this_target_h, cutoff_min=cutoff_min\n                    ),\n                )\n                .reset_index(level=0)\n            )\n            # rename\n            _results.columns = [\"sample\", \"cumulated_heat_at_hours\"]\n            _results[\"target_h\"] = this_target_h\n            _results[\"cutoff_min\"] = cutoff_min\n            # append to list\n            list_of_results.append(_results)\n        # build overall results DataFrame\n        results = pd.concat(list_of_results)\n\n    # return\n    return results\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_data","title":"<code>get_data()</code>","text":"<p>A convenience function which returns the Pandas Dataframe containing the read and processed calorimetry data.</p> <p>Returns:</p> Type Description <code>Pandas DataFrame</code> <p>Examples:</p> <p>Assuming that the calorimetry data is contained in a subfolder <code>data</code>, a conventional Pandas dataframe <code>df</code> containing the data from all calorimetry files in <code>data</code> can be obtained with the following code.</p> <pre><code>&gt;&gt;&gt; from CaloCem import tacalorimetry as ta\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt;\n&gt;&gt;&gt; thepath = Path(__file__).parent / \"data\"\n&gt;&gt;&gt; tam = ta.Measurement(thepath)\n&gt;&gt;&gt; df = tam.get_data()\n</code></pre> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_data(self):\n    \"\"\"\n    A convenience function which returns the Pandas Dataframe containing the read and processed calorimetry data.\n    Returns\n    -------\n    Pandas DataFrame\n\n    Examples\n    --------\n    Assuming that the calorimetry data is contained in a subfolder `data`, a conventional Pandas dataframe `df` containing the data from all calorimetry files in `data` can be obtained with the following code.\n\n    &gt;&gt;&gt; from CaloCem import tacalorimetry as ta\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; thepath = Path(__file__).parent / \"data\"\n    &gt;&gt;&gt; tam = ta.Measurement(thepath)\n    &gt;&gt;&gt; df = tam.get_data()\n\n    \"\"\"\n\n    return self._data\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_dormant_period_heatflow","title":"<code>get_dormant_period_heatflow(processparams, regex=None, cutoff_min=5, upper_dormant_thresh_w_g=0.002, plot_right_boundary=200000.0, prominence=0.001, show_plot=False)</code>","text":"<p>get dormant period heatflow</p> <p>Parameters:</p> Name Type Description Default <code>regex</code> <code>str</code> <p>Regex which can be used to filter the data, i.e., only the patterns which fit the regex will be evaluated. The default is None.</p> <code>None</code> <code>cutoff_min</code> <code>int | float</code> <p>Time at the start of the experiment which will be cutoff from analysis. This can be useful for ex-situ mixed samples. The default is 5.</p> <code>5</code> <code>upper_dormant_thresh_w_g</code> <code>float</code> <p>Parameter which controls the upper limit for the plotting option. The default is 0.001.</p> <code>0.002</code> <code>show_plot</code> <code>bool</code> <p>If set to true, the data is plotted. The default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Pandas Dataframe</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_dormant_period_heatflow(\n    self,\n    processparams,\n    regex: str = None,\n    cutoff_min: int = 5,\n    upper_dormant_thresh_w_g: float = 0.002,\n    plot_right_boundary=2e5,\n    prominence: float = 1e-3,\n    show_plot=False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    get dormant period heatflow\n\n    Parameters\n    ----------\n    regex : str, optional\n        Regex which can be used to filter the data, i.e., only the patterns which fit the regex will be evaluated. The default is None.\n    cutoff_min : int | float, optional\n        Time at the start of the experiment which will be cutoff from analysis. This can be useful for ex-situ mixed samples. The default is 5.\n    upper_dormant_thresh_w_g : float, optional\n        Parameter which controls the upper limit for the plotting option. The default is 0.001.\n    show_plot : bool, optional\n        If set to true, the data is plotted. The default is False.\n\n    Returns\n    -------\n    Pandas Dataframe\n\n    \"\"\"\n\n    # init results list\n    list_dfs = []\n\n    # loop samples\n    for sample, data in self._iter_samples(regex=regex):\n        # get peak as \"right border\"\n        _peaks = self.get_peaks(\n            processparams,\n            # cutoff_min=cutoff_min,\n            regex=pathlib.Path(sample).name,\n            # prominence=processparams.gradient_peak_prominence, # prominence,\n            show_plot=show_plot,\n        )\n\n        # identify \"dormant period\" as range between initial spike\n        # and first reaction peak\n\n        if show_plot:\n            # plot\n            plt.plot(\n                data[\"time_s\"],\n                data[\"normalized_heat_flow_w_g\"],\n                # linestyle=\"\",\n                # marker=\"o\",\n            )\n\n        # discard points at early age\n        data = data.query(\"time_s &gt;= @processparams.cutoff.cutoff_min * 60\")\n        if not _peaks.empty:\n            # discard points after the first peak\n            data = data.query('time_s &lt;= @_peaks[\"time_s\"].min()')\n\n        # reset index\n        data = data.reset_index(drop=True)\n\n        # pick relevant points at minimum heat flow\n        data = data.iloc[data[\"normalized_heat_flow_w_g\"].idxmin(), :].to_frame().T\n\n        if show_plot:\n            # guide to the eye lines\n            plt.axhline(float(data[\"normalized_heat_flow_w_g\"]), color=\"red\")\n            plt.axvline(float(data[\"time_s\"]), color=\"red\")\n            # indicate cutoff time\n            plt.axvspan(0, cutoff_min * 60, color=\"black\", alpha=0.5)\n            # limits\n            # plt.xlim(0, _peaks[\"time_s\"].min())\n            plt.xlim(0, plot_right_boundary)\n            plt.ylim(0, upper_dormant_thresh_w_g)\n            # title\n            plt.title(pathlib.Path(sample).stem)\n            # show\n            plt.show()\n\n        # add to list\n        list_dfs.append(data)\n\n    # convert to overall datafram\n    result = pd.concat(list_dfs).reset_index(drop=True)\n\n    # return\n    return result\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_information","title":"<code>get_information()</code>","text":"<p>get information</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>information, i.e. date of measurement, operator, comment ...</p> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_information(self):\n    \"\"\"\n    get information\n\n    Returns\n    -------\n    pd.DataFrame\n        information, i.e. date of measurement, operator, comment ...\n\n    \"\"\"\n\n    return self._info\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_maximum_slope","title":"<code>get_maximum_slope(processparams, target_col='normalized_heat_flow_w_g', age_col='time_s', time_discarded_s=900, show_plot=False, show_info=True, exclude_discarded_time=False, regex=None, read_start_c3s=False, ax=None, save_path=None, xscale='log', xunit='s')</code>","text":"<p>The method finds the point in time of the maximum slope. It also calculates the gradient at this point. The method can be controlled by passing a customized ProcessingParameters object for the <code>processparams</code> parameter. If no object is passed, the default parameters will be used.</p> <p>Parameters:</p> Name Type Description Default <code>target_col</code> <code>str</code> <p>measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"</p> <code>'normalized_heat_flow_w_g'</code> <code>age_col</code> <code>str</code> <p>Time unit within which peak onsets are searched for. The default is \"time_s\"</p> <code>'time_s'</code> <code>time_discarded_s</code> <code>int | float</code> <p>Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.</p> <code>900</code> <code>show_plot</code> <code>bool</code> <p>Flag whether or not to plot peak picking for each sample. The default is False.</p> <code>False</code> <code>exclude_discarded_time</code> <code>bool</code> <p>Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.</p> <code>False</code> <code>regex</code> <code>str</code> <p>regex pattern to include only certain experimental result files during initialization. The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pandas Dataframe</code> <p>A dataframe that contains the time and the gradient of the maximum slope.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from CaloCem import tacalorimetry as ta\n&gt;&gt;&gt; from pathlib import Path\n</code></pre> <pre><code>&gt;&gt;&gt; thepath = Path(__file__).parent / \"data\"\n&gt;&gt;&gt; tam = ta.Measurement(thepath)\n&gt;&gt;&gt; processparams = ta.ProcessingParameters()\n&gt;&gt;&gt; processparams..apply = True\n&gt;&gt;&gt; max_slopes = tam.get_maximum_slope(processparams)\n</code></pre> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_maximum_slope(\n    self,\n    processparams,\n    target_col=\"normalized_heat_flow_w_g\",\n    age_col=\"time_s\",\n    time_discarded_s=900,\n    show_plot=False,\n    show_info=True,\n    exclude_discarded_time=False,\n    regex=None,\n    read_start_c3s=False,\n    ax=None,\n    save_path=None,\n    xscale=\"log\",\n    xunit=\"s\",\n):\n    \"\"\"\n    The method finds the point in time of the maximum slope. It also calculates the gradient at this point. The method can be controlled by passing a customized ProcessingParameters object for the `processparams` parameter. If no object is passed, the default parameters will be used.\n\n    Parameters\n    ----------\n    target_col : str, optional\n        measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"\n    age_col : str, optional\n        Time unit within which peak onsets are searched for. The default is \"time_s\"\n    time_discarded_s : int | float, optional\n        Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.\n    show_plot : bool, optional\n        Flag whether or not to plot peak picking for each sample. The default is False.\n    exclude_discarded_time : bool, optional\n        Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.\n    regex : str, optional\n        regex pattern to include only certain experimental result files during initialization. The default is None.\n    Returns\n    -------\n    Pandas Dataframe\n        A dataframe that contains the time and the gradient of the maximum slope.\n    Examples\n    --------\n    &gt;&gt;&gt; from CaloCem import tacalorimetry as ta\n    &gt;&gt;&gt; from pathlib import Path\n\n    &gt;&gt;&gt; thepath = Path(__file__).parent / \"data\"\n    &gt;&gt;&gt; tam = ta.Measurement(thepath)\n    &gt;&gt;&gt; processparams = ta.ProcessingParameters()\n    &gt;&gt;&gt; processparams..apply = True\n    &gt;&gt;&gt; max_slopes = tam.get_maximum_slope(processparams)\n    \"\"\"\n\n    # init list of characteristics\n    list_of_characteristics = []\n\n    # loop samples\n    for sample, data in self._iter_samples(regex=regex):\n        sample_name = pathlib.Path(sample).stem\n        if exclude_discarded_time:\n            # exclude\n            data = data.query(f\"{age_col} &gt;= {time_discarded_s}\")\n\n        # manual definition of start time to look for c3s - in case auto peak detection becomes difficult\n        if read_start_c3s:\n            c3s_start_time_s = self._metadata.query(\n                f\"sample_number == '{sample_name}'\"\n            )[\"t_c3s_min_s\"].values[0]\n            c3s_end_time_s = self._metadata.query(\n                f\"sample_number == '{sample_name}'\"\n            )[\"t_c3s_max_s\"].values[0]\n            data = data.query(\n                f\"{age_col} &gt;= {c3s_start_time_s} &amp; {age_col} &lt;= {c3s_end_time_s}\"\n            )\n\n        if show_info:\n            print(f\"Determineing maximum slope of {pathlib.Path(sample).stem}\")\n\n        processor = HeatFlowProcessor(processparams)\n\n        data = utils.make_equidistant(data)\n\n        if processparams.rolling_mean.apply:\n            data = processor.apply_rolling_mean(data)\n\n        data[\"gradient\"], data[\"curvature\"] = (\n            processor.calculate_heatflow_derivatives(data)\n        )\n\n        characteristics = processor.get_largest_slope(data, processparams)\n        if characteristics.empty:\n            continue\n\n        # optional plotting\n        if show_plot:\n            self._plot_maximum_slope(\n                data,\n                ax,\n                age_col,\n                target_col,\n                sample,\n                characteristics,\n                time_discarded_s,\n                save_path=save_path,\n                xscale=xscale,\n                xunit=xunit,\n            )\n            # plot heat flow curve\n            # plt.plot(data[age_col], data[target_col], label=target_col)\n            # plt.plot(\n            #     data[age_col],\n            #     data[\"gradient\"] * 1e4 + 0.001,\n            #     label=\"gradient * 1e4 + 1mW\",\n            # )\n\n            # # add vertical lines\n            # for _idx, _row in characteristics.iterrows():\n            #     # vline\n            #     plt.axvline(_row.at[age_col], color=\"green\", alpha=0.3)\n\n            # # cosmetics\n            # plt.xscale(\"log\")\n            # plt.title(f\"Maximum slope plot for {pathlib.Path(sample).stem}\")\n            # plt.xlabel(age_col)\n            # plt.ylabel(target_col)\n            # plt.legend()\n\n            # # get axis\n            # ax = plt.gca()\n\n            # plt.fill_between(\n            #     [ax.get_ylim()[0], time_discarded_s],\n            #     [ax.get_ylim()[0]] * 2,\n            #     [ax.get_ylim()[1]] * 2,\n            #     color=\"black\",\n            #     alpha=0.35,\n            # )\n\n            # # set axis limit\n            # plt.xlim(left=100)\n            # plt.ylim(bottom=0, top=0.01)\n\n            # # show\n            # plt.show()\n\n        # append to list\n        list_of_characteristics.append(characteristics)\n\n    if not list_of_characteristics:\n        print(\"No maximum slope found, check you processing parameters\")\n    # build overall list\n    else:\n        max_slope_characteristics = pd.concat(list_of_characteristics)\n        # return\n        return max_slope_characteristics\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_metadata","title":"<code>get_metadata()</code>","text":"Returns <p>tuple      pd.DataFrame of metadata and string of the column used as ID (has to      be unique).</p> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_metadata(self) -&gt; tuple:\n    \"\"\"\n\n\n     Returns\n     -------\n    tuple\n         pd.DataFrame of metadata and string of the column used as ID (has to\n         be unique).\n    \"\"\"\n\n    # return\n    return self._metadata, self._metadata_id\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_metadata_grouping_options","title":"<code>get_metadata_grouping_options()</code>","text":"<p>get a list of categories to group by in in \"self.plot_by_category\"</p> <p>Returns:</p> Type Description <code>list</code> <p>list of categories avaialble for grouping by.</p> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_metadata_grouping_options(self) -&gt; list:\n    \"\"\"\n    get a list of categories to group by in in \"self.plot_by_category\"\n\n    Returns\n    -------\n    list\n        list of categories avaialble for grouping by.\n    \"\"\"\n\n    # get list based on column names of \"_metadata\"\n    return self._metadata.columns.tolist()\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_peak_onset_via_max_slope","title":"<code>get_peak_onset_via_max_slope(processparams, show_plot=False, ax=None, regex=None, age_col='time_s', target_col='normalized_heat_flow_w_g', time_discarded_s=900, save_path=None, xscale='linear', xunit='s', intersection='dormant_hf')</code>","text":"<p>get reaction onset based on tangent of maximum heat flow and heat flow during the dormant period. The characteristic time is inferred from the intersection of both characteristic lines</p> <p>Parameters:</p> Name Type Description Default <code>show_plot</code> <code>TYPE</code> <p>DESCRIPTION. The default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_peak_onset_via_max_slope(\n    self,\n    processparams,\n    show_plot=False,\n    ax=None,\n    regex=None,\n    age_col=\"time_s\",\n    target_col=\"normalized_heat_flow_w_g\",\n    time_discarded_s=900,\n    save_path=None,\n    xscale=\"linear\",\n    xunit=\"s\",\n    intersection=\"dormant_hf\",\n):\n    \"\"\"\n    get reaction onset based on tangent of maximum heat flow and heat flow\n    during the dormant period. The characteristic time is inferred from\n    the intersection of both characteristic lines\n\n    Parameters\n    ----------\n    show_plot : TYPE, optional\n        DESCRIPTION. The default is False.\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n    # get onsets\n    max_slopes = self.get_maximum_slope(\n        processparams,\n        regex=regex,\n        show_plot=False,\n        ax=ax,\n    )\n    # % get dormant period HFs\n    dorm_hfs = self.get_dormant_period_heatflow(\n        processparams,  # cutoff_min=cutoff_min, prominence=prominence\n        regex=regex,\n        show_plot=False,\n        # ax=ax,\n    )\n\n    # init list\n    list_characteristics = []\n\n    # loop samples\n    for i, row in max_slopes.iterrows():\n        # calculate y-offset\n        t = row[\"normalized_heat_flow_w_g\"] - row[\"time_s\"] * row[\"gradient\"]\n        # calculate point of intersection\n        # calculate x-intersect of tangent with dormant heat flow\n        x_intersect_dormant = (\n                float(\n                    dorm_hfs[dorm_hfs[\"sample_short\"] == row[\"sample_short\"]][\n                        \"normalized_heat_flow_w_g\"\n                    ]\n                )\n                - t\n            ) / row[\"gradient\"]\n        # elif intersection == \"abscissa\":\n            # calculate x-intersect of tangent with abscissa (y=0)\n        x_intersect = row[\"time_s\"] - (row[\"normalized_heat_flow_w_g\"] / row[\"gradient\"])\n\n        data = self._data.query(\"sample_short == @row['sample_short']\")\n        sample = row[\"sample_short\"]\n\n        heat_at_intersect = np.interp(x_intersect, data[\"time_s\"], data[\"normalized_heat_j_g\"])\n\n        # append to list\n        list_characteristics.append(\n            {\n                \"sample\": row[\"sample_short\"],\n                \"onset_time_s_abscissa\": x_intersect,\n                \"onset_time_min_abscissa\": x_intersect / 60,\n                \"heat_at_onset_j_g\": heat_at_intersect,\n                \"onset_time_s\": x_intersect_dormant,\n                \"onset_time_min\": x_intersect_dormant / 60,\n            }\n        )\n\n\n        dorm_hfs_sample = dorm_hfs.query(\"sample_short == @sample\")\n        # add prefix dorm to all columns\n        dorm_hfs_sample.columns = [\"dorm_\" + s for s in dorm_hfs_sample.columns]\n\n        characteristics = pd.concat([row, dorm_hfs_sample.squeeze()])\n        characteristics.loc[\"xunit\"] = xunit\n        characteristics.loc[\"x_intersect\"] = x_intersect\n        characteristics.loc[\"intersection\"] = intersection\n        # print(characteristics.x_intersect)\n\n        # get maximum time value\n        tmax = self._data.query(\"sample_short == @row['sample_short']\")[\n            \"time_s\"\n        ].max()\n        # get maximum heat flow value\n        hmax = self._data.query(\n            \"time_s &gt; 3000 &amp; sample_short == @row['sample_short']\"\n        )[\"normalized_heat_flow_w_g\"].max()\n\n        if show_plot:\n            self._plot_intersection(\n                data,\n                ax,\n                age_col,\n                target_col,\n                sample,\n                # max_slopes,\n                time_discarded_s,\n                characteristics=characteristics,\n                save_path=save_path,\n                xscale=xscale,\n                # xunit=xunit,\n                hmax=hmax,\n                tmax=tmax,\n            )\n\n    # build overall dataframe to be returned\n    onsets = pd.DataFrame(list_characteristics)\n\n    # merge with dorm_hfs\n    onsets = onsets.merge(\n        dorm_hfs[\n            [\"sample_short\", \"normalized_heat_flow_w_g\", \"normalized_heat_j_g\"]\n        ],\n        left_on=\"sample\",\n        right_on=\"sample_short\",\n        how=\"left\",\n    )\n\n    # rename\n    onsets = onsets.rename(\n        columns={\n            \"normalized_heat_flow_w_g\": \"normalized_heat_flow_w_g_at_dorm_min\",\n            \"normalized_heat_j_g\": \"normalized_heat_j_g_at_dorm_min\",\n        }\n    )\n\n    # return\n    return onsets\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_peak_onsets","title":"<code>get_peak_onsets(target_col='normalized_heat_flow_w_g', age_col='time_s', time_discarded_s=900, rolling=1, gradient_threshold=0.0005, show_plot=False, exclude_discarded_time=False, regex=None, ax=None)</code>","text":"<p>get peak onsets based on a criterion of minimum gradient</p> <p>Parameters:</p> Name Type Description Default <code>target_col</code> <code>str</code> <p>measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"</p> <code>'normalized_heat_flow_w_g'</code> <code>age_col</code> <code>str</code> <p>Time unit within which peak onsets are searched for. The default is \"time_s\"</p> <code>'time_s'</code> <code>time_discarded_s</code> <code>int | float</code> <p>Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.</p> <code>900</code> <code>rolling</code> <code>int</code> <p>Width of \"rolling\" window within which the values of \"target_col\" are averaged. A higher value will introduce a stronger smoothing effect. The default is 1, i.e. no smoothing.</p> <code>1</code> <code>gradient_threshold</code> <code>float</code> <p>Threshold of slope for identification of a peak onset. For a lower value, earlier peak onsets will be identified. The default is 0.0005.</p> <code>0.0005</code> <code>show_plot</code> <code>bool</code> <p>Flag whether or not to plot peak picking for each sample. The default is False.</p> <code>False</code> <code>exclude_discarded_time</code> <code>bool</code> <p>Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.</p> <code>False</code> <code>regex</code> <code>str</code> <p>regex pattern to include only certain experimental result files during initialization. The default is None.</p> <code>None</code> <code>ax</code> <code>Axes | None</code> <p>The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame holding peak onset characterisitcs for each sample.</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_peak_onsets(\n    self,\n    target_col=\"normalized_heat_flow_w_g\",\n    age_col=\"time_s\",\n    time_discarded_s=900,\n    rolling=1,\n    gradient_threshold=0.0005,\n    show_plot=False,\n    exclude_discarded_time=False,\n    regex=None,\n    ax: plt.Axes = None,\n):\n    \"\"\"\n    get peak onsets based on a criterion of minimum gradient\n\n    Parameters\n    ----------\n    target_col : str, optional\n        measured quantity within which peak onsets are searched for. The default is \"normalized_heat_flow_w_g\"\n    age_col : str, optional\n        Time unit within which peak onsets are searched for. The default is \"time_s\"\n    time_discarded_s : int | float, optional\n        Time in seconds below which collected data points are discarded for peak onset picking. The default is 900.\n    rolling : int, optional\n        Width of \"rolling\" window within which the values of \"target_col\" are averaged. A higher value will introduce a stronger smoothing effect. The default is 1, i.e. no smoothing.\n    gradient_threshold : float, optional\n        Threshold of slope for identification of a peak onset. For a lower value, earlier peak onsets will be identified. The default is 0.0005.\n    show_plot : bool, optional\n        Flag whether or not to plot peak picking for each sample. The default is False.\n    exclude_discarded_time : bool, optional\n        Whether or not to discard the experimental values obtained before \"time_discarded_s\" also in the visualization. The default is False.\n    regex : str, optional\n        regex pattern to include only certain experimental result files during initialization. The default is None.\n    ax : matplotlib.axes._axes.Axes | None, optional\n        The default is None.\n    Returns\n    -------\n    pd.DataFrame holding peak onset characterisitcs for each sample.\n\n    \"\"\"\n\n    # init list of characteristics\n    list_of_characteristics = []\n\n    # loop samples\n    for sample, data in self._iter_samples(regex=regex):\n        if exclude_discarded_time:\n            # exclude\n            data = data.query(f\"{age_col} &gt;= {time_discarded_s}\")\n\n        # reset index\n        data = data.reset_index(drop=True)\n\n        # calculate get gradient\n        data[\"gradient\"] = pd.Series(\n            np.gradient(data[target_col].rolling(rolling).mean(), data[age_col])\n        )\n\n        # get relevant points\n        characteristics = data.copy()\n        # discard initial time\n        characteristics = characteristics.query(f\"{age_col} &gt;= {time_discarded_s}\")\n        # look at values with certain gradient only\n        characteristics = characteristics.query(\"gradient &gt; @gradient_threshold\")\n        # consider first entry exclusively\n        characteristics = characteristics.head(1)\n\n        # optional plotting\n        if show_plot:\n            # if specific axis to plot to is specified\n            if isinstance(ax, matplotlib.axes._axes.Axes):\n                # plot heat flow curve\n                p = ax.plot(data[age_col], data[target_col])\n\n                # add vertical lines\n                for _idx, _row in characteristics.iterrows():\n                    # vline\n                    ax.axvline(_row.at[age_col], color=p[0].get_color(), alpha=0.3)\n                    # add \"slope line\"\n                    ax.axline(\n                        (_row.at[age_col], _row.at[target_col]),\n                        slope=_row.at[\"gradient\"],\n                        color=p[0].get_color(),\n                        # color=\"k\",\n                        # linewidth=0.2\n                        alpha=0.25,\n                        linestyle=\"--\",\n                    )\n\n                # cosmetics\n                # ax.set_xscale(\"log\")\n                ax.set_title(\"Onset for \" + pathlib.Path(sample).stem)\n                ax.set_xlabel(age_col)\n                ax.set_ylabel(target_col)\n\n                ax.fill_between(\n                    [ax.get_ylim()[0], time_discarded_s],\n                    [ax.get_ylim()[0]] * 2,\n                    [ax.get_ylim()[1]] * 2,\n                    color=\"black\",\n                    alpha=0.35,\n                )\n\n                # set axis limit\n                ax.set_xlim(left=100)\n\n            else:\n                # plot heat flow curve\n                plt.plot(data[age_col], data[target_col])\n\n                # add vertical lines\n                for _idx, _row in characteristics.iterrows():\n                    # vline\n                    plt.axvline(_row.at[age_col], color=\"red\", alpha=0.3)\n\n                # cosmetics\n                # plt.xscale(\"log\")\n                plt.title(\"Onset for \" + pathlib.Path(sample).stem)\n                plt.xlabel(age_col)\n                plt.ylabel(target_col)\n\n                # get axis\n                ax = plt.gca()\n\n                plt.fill_between(\n                    [ax.get_ylim()[0], time_discarded_s],\n                    [ax.get_ylim()[0]] * 2,\n                    [ax.get_ylim()[1]] * 2,\n                    color=\"black\",\n                    alpha=0.35,\n                )\n\n                # set axis limit\n                plt.xlim(left=100)\n\n        # append to list\n        list_of_characteristics.append(characteristics)\n\n    # build overall list\n    onset_characteristics = pd.concat(list_of_characteristics)\n\n    # return\n    if isinstance(ax, matplotlib.axes._axes.Axes):\n        # return onset characteristics and ax\n        return onset_characteristics, ax\n    else:\n        # return onset characteristics exclusively\n        return onset_characteristics\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_peaks","title":"<code>get_peaks(processparams, target_col='normalized_heat_flow_w_g', regex=None, cutoff_min=None, show_plot=True, plt_right_s=200000.0, plt_top=0.01, ax=None, xunit='s', plot_labels=None, xmarker=False)</code>","text":"<p>get DataFrame of peak characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>target_col</code> <code>str</code> <p>measured quantity within which peaks are searched for. The default is \"normalized_heat_flow_w_g\"</p> <code>'normalized_heat_flow_w_g'</code> <code>regex</code> <code>str</code> <p>regex pattern to include only certain experimental result files during initialization. The default is None.</p> <code>None</code> <code>cutoff_min</code> <code>int | float</code> <p>Time in minutes below which collected data points are discarded for peak picking</p> <code>None</code> <code>show_plot</code> <code>bool</code> <p>Flag whether or not to plot peak picking for each sample. The default is True.</p> <code>True</code> <code>plt_right_s</code> <code>int | float</code> <p>Upper limit of x-axis of in seconds. The default is 2e5.</p> <code>200000.0</code> <code>plt_top</code> <code>int | float</code> <p>Upper limit of y-axis of. The default is 1e-2.</p> <code>0.01</code> <code>ax</code> <code>Axes | None</code> <p>The default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame holding peak characterisitcs for each sample.</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_peaks(\n    self,\n    processparams,\n    target_col=\"normalized_heat_flow_w_g\",\n    regex=None,\n    cutoff_min=None,\n    show_plot=True,\n    plt_right_s=2e5,\n    plt_top=1e-2,\n    ax=None,\n    xunit=\"s\",\n    plot_labels=None,\n    xmarker=False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    get DataFrame of peak characteristics.\n\n    Parameters\n    ----------\n    target_col : str, optional\n        measured quantity within which peaks are searched for. The default is \"normalized_heat_flow_w_g\"\n    regex : str, optional\n        regex pattern to include only certain experimental result files\n        during initialization. The default is None.\n    cutoff_min : int | float, optional\n        Time in minutes below which collected data points are discarded for peak picking\n    show_plot : bool, optional\n        Flag whether or not to plot peak picking for each sample. The default is True.\n    plt_right_s : int | float, optional\n        Upper limit of x-axis of in seconds. The default is 2e5.\n    plt_top : int | float, optional\n        Upper limit of y-axis of. The default is 1e-2.\n    ax : matplotlib.axes._axes.Axes | None, optional\n        The default is None.\n\n    Returns\n    -------\n    pd.DataFrame holding peak characterisitcs for each sample.\n\n    \"\"\"\n\n    # list of peaks\n    list_of_peaks_dfs = []\n\n    # loop samples\n    for sample, data in self._iter_samples(regex=regex):\n        # cutoff\n        if processparams.cutoff.cutoff_min:\n            # discard points at early age\n            data = data.query(\"time_s &gt;= @processparams.cutoff.cutoff_min * 60\")\n\n        # reset index\n        data = data.reset_index(drop=True)\n\n        # target_columns\n        _age_col = \"time_s\"\n        _target_col = target_col\n\n        # find peaks\n        peaks, properties = signal.find_peaks(\n            data[_target_col],\n            prominence=processparams.peakdetection.prominence,\n            distance=processparams.peakdetection.distance,\n        )\n\n        # plot?\n        if show_plot:\n            if xunit == \"h\":\n                df_copy = data.copy()\n                df_copy[_age_col] = df_copy[_age_col] / 3600\n                plt_right_s = plt_right_s / 3600\n                self._plot_peak_positions(\n                    df_copy,\n                    ax,\n                    _age_col,\n                    _target_col,\n                    peaks,\n                    sample,\n                    plt_top,\n                    plt_right_s,\n                    plot_labels,\n                    xmarker,\n                )\n            else:\n                self._plot_peak_positions(\n                    data,\n                    ax,\n                    _age_col,\n                    _target_col,\n                    peaks,\n                    sample,\n                    plt_top,\n                    plt_right_s,\n                    plot_labels,\n                    xmarker,\n                )\n\n        # compile peak characteristics\n        peak_characteristics = pd.concat(\n            [\n                data.iloc[peaks, :],\n                pd.DataFrame(\n                    properties[\"prominences\"], index=peaks, columns=[\"prominence\"]\n                ),\n                pd.DataFrame({\"peak_nr\": np.arange((len(peaks)))}, index=peaks),\n            ],\n            axis=1,\n        )\n\n        # append\n        list_of_peaks_dfs.append(peak_characteristics)\n\n    # compile peak information\n    peaks = pd.concat(list_of_peaks_dfs)\n\n    if isinstance(ax, matplotlib.axes._axes.Axes):\n        # return peak list and ax\n        return peaks, ax\n    else:  # return peak list only\n        return peaks\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.get_sample_names","title":"<code>get_sample_names()</code>","text":"<p>get list of sample names</p> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def get_sample_names(self):\n    \"\"\"\n    get list of sample names\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # get list\n    samples = [pathlib.Path(s).stem for s, _ in self._iter_samples()]\n\n    # return\n    return samples\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.normalize_sample_to_mass","title":"<code>normalize_sample_to_mass(sample_short, mass_g, show_info=True)</code>","text":"<p>normalize \"heat_flow\" to a certain mass</p> <p>Parameters:</p> Name Type Description Default <code>sample_short</code> <code>str</code> <p>\"sample_short\" name of sample to be normalized.</p> required <code>mass_g</code> <code>float</code> <p>mass in gram to which \"heat_flow_w\" are normalized.</p> required <p>Returns:</p> Type Description <code>None.</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def normalize_sample_to_mass(\n    self, sample_short: str, mass_g: float, show_info=True\n):\n    \"\"\"\n    normalize \"heat_flow\" to a certain mass\n\n    Parameters\n    ----------\n    sample_short : str\n        \"sample_short\" name of sample to be normalized.\n    mass_g : float\n        mass in gram to which \"heat_flow_w\" are normalized.\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # normalize \"heat_flow_w\" to sample mass\n    self._data.loc[\n        self._data[\"sample_short\"] == sample_short, \"normalized_heat_flow_w_g\"\n    ] = (\n        self._data.loc[self._data[\"sample_short\"] == sample_short, \"heat_flow_w\"]\n        / mass_g\n    )\n\n    # normalize \"heat_j\" to sample mass\n    try:\n        self._data.loc[\n            self._data[\"sample_short\"] == sample_short, \"normalized_heat_j_g\"\n        ] = (\n            self._data.loc[self._data[\"sample_short\"] == sample_short, \"heat_j\"]\n            / mass_g\n        )\n    except Exception:\n        pass\n\n    # info\n    if show_info:\n        print(f\"Sample {sample_short} normalized to {mass_g}g sample.\")\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.plot","title":"<code>plot(t_unit='h', y='normalized_heat_flow_w_g', y_unit_milli=True, regex=None, show_info=True, ax=None)</code>","text":"<p>Plot the calorimetry data.</p> <p>Parameters:</p> Name Type Description Default <code>t_unit</code> <code>str</code> <p>time unit. The default is \"h\". Options are \"s\", \"min\", \"h\", \"d\".</p> <code>'h'</code> <code>y</code> <code>str</code> <p>y-axis. The default is \"normalized_heat_flow_w_g\". Options are \"normalized_heat_flow_w_g\", \"heat_flow_w\", \"normalized_heat_j_g\", \"heat_j\".</p> <code>'normalized_heat_flow_w_g'</code> <code>y_unit_milli</code> <code>bool</code> <p>whether or not to plot y-axis in Milliwatt. The default is True.</p> <code>True</code> <code>regex</code> <code>str</code> <p>regex pattern to include only certain samples during plotting. The default is None.</p> <code>None</code> <code>show_info</code> <code>bool</code> <p>whether or not to show information. The default is True.</p> <code>True</code> <code>ax</code> <code>Axes</code> <p>axis to plot to. The default is None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import CaloCem as ta\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt;\n&gt;&gt;&gt; calodatapath = Path(__file__).parent\n&gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n&gt;&gt;&gt; tam.plot(t_unit=\"h\", y=\"normalized_heat_flow_w_g\", y_unit_milli=False)\n</code></pre> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def plot(\n    self,\n    t_unit=\"h\",\n    y=\"normalized_heat_flow_w_g\",\n    y_unit_milli=True,\n    regex=None,\n    show_info=True,\n    ax=None,\n):\n    \"\"\"\n\n    Plot the calorimetry data.\n\n    Parameters\n    ----------\n    t_unit : str, optional\n        time unit. The default is \"h\". Options are \"s\", \"min\", \"h\", \"d\".\n    y : str, optional\n        y-axis. The default is \"normalized_heat_flow_w_g\". Options are\n        \"normalized_heat_flow_w_g\", \"heat_flow_w\", \"normalized_heat_j_g\",\n        \"heat_j\".\n    y_unit_milli : bool, optional\n        whether or not to plot y-axis in Milliwatt. The default is True.\n    regex : str, optional\n        regex pattern to include only certain samples during plotting. The\n        default is None.\n    show_info : bool, optional\n        whether or not to show information. The default is True.\n    ax : matplotlib.axes._axes.Axes, optional\n        axis to plot to. The default is None.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import CaloCem as ta\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; calodatapath = Path(__file__).parent\n    &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n    &gt;&gt;&gt; tam.plot(t_unit=\"h\", y=\"normalized_heat_flow_w_g\", y_unit_milli=False)\n\n    \"\"\"\n\n    # y-value\n    if y == \"normalized_heat_flow_w_g\":\n        y_column = \"normalized_heat_flow_w_g\"\n        y_label = \"Normalized Heat Flow / [W/g]\"\n    elif y == \"heat_flow_w\":\n        y_column = \"heat_flow_w\"\n        y_label = \"Heat Flow / [W]\"\n    elif y == \"normalized_heat_j_g\":\n        y_column = \"normalized_heat_j_g\"\n        y_label = \"Normalized Heat / [J/g]\"\n    elif y == \"heat_j\":\n        y_column = \"heat_j\"\n        y_label = \"Heat / [J]\"\n\n    if y_unit_milli:\n        y_label = y_label.replace(\"[\", \"[m\")\n\n    # x-unit\n    if t_unit == \"s\":\n        x_factor = 1.0\n    elif t_unit == \"min\":\n        x_factor = 1 / 60\n    elif t_unit == \"h\":\n        x_factor = 1 / (60 * 60)\n    elif t_unit == \"d\":\n        x_factor = 1 / (60 * 60 * 24)\n\n    # y-unit\n    if y_unit_milli:\n        y_factor = 1000\n    else:\n        y_factor = 1\n\n    for sample, data in self._iter_samples():\n        if regex:\n            if not re.findall(rf\"{regex}\", os.path.basename(sample)):\n                continue\n        data[\"time_s\"] = data[\"time_s\"] * x_factor\n        # all columns containing heat\n        heatcols = [s for s in data.columns if \"heat\" in s]\n        data[heatcols] = data[heatcols] * y_factor\n        ax, _ = utils.create_base_plot(data, ax, \"time_s\", y_column, sample)\n        ax = utils.style_base_plot(\n            ax,\n            y_label,\n            t_unit,\n            sample,\n        )\n    return ax\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.plot_by_category","title":"<code>plot_by_category(categories, t_unit='h', y='normalized_heat_flow_w_g', y_unit_milli=True)</code>","text":"<p>plot by category, wherein the category is based on the information passed via \"self._add_metadata_source\". Options available as \"category\" are accessible via \"self.get_metadata_grouping_options\"</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>(str, list[str])</code> <p>category (from \"self.get_metadata_grouping_options\") to group by. specify a string or a list of strings here</p> required <code>t_unit</code> <code>TYPE</code> <p>see \"self.plot\". The default is \"h\".</p> <code>'h'</code> <code>y</code> <code>TYPE</code> <p>see \"self.plot\". The default is \"normalized_heat_flow_w_g\".</p> <code>'normalized_heat_flow_w_g'</code> <code>y_unit_milli</code> <code>TYPE</code> <p>see \"self.plot\". The default is True.</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import CaloCem as ta\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt;\n&gt;&gt;&gt; calodatapath = Path(__file__).parent\n&gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n&gt;&gt;&gt; tam.plot_by_category(categories=\"sample\")\n</code></pre> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def plot_by_category(\n    self, categories, t_unit=\"h\", y=\"normalized_heat_flow_w_g\", y_unit_milli=True\n):\n    \"\"\"\n    plot by category, wherein the category is based on the information passed\n    via \"self._add_metadata_source\". Options available as \"category\" are\n    accessible via \"self.get_metadata_grouping_options\"\n\n    Parameters\n    ----------\n    categories : str, list[str]\n        category (from \"self.get_metadata_grouping_options\") to group by.\n        specify a string or a list of strings here\n    t_unit : TYPE, optional\n        see \"self.plot\". The default is \"h\".\n    y : TYPE, optional\n        see \"self.plot\". The default is \"normalized_heat_flow_w_g\".\n    y_unit_milli : TYPE, optional\n        see \"self.plot\". The default is True.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import CaloCem as ta\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; calodatapath = Path(__file__).parent\n    &gt;&gt;&gt; tam = ta.Measurement(folder=calodatapath, show_info=True)\n    &gt;&gt;&gt; tam.plot_by_category(categories=\"sample\")\n\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    def build_helper_string(values: list) -&gt; str:\n        \"\"\"\n        build a \"nicely\" formatted string from a supplied list\n        \"\"\"\n\n        if len(values) == 2:\n            # connect with \"and\"\n            formatted = \" and \".join([str(i) for i in values])\n        elif len(values) &gt; 2:\n            # connect with comma and \"and\" for last element\n            formatted = (\n                \", \".join([str(i) for i in values[:-1]]) + \" and \" + str(values[-1])\n            )\n        else:\n            formatted = \"---\"\n\n        # return\n        return formatted\n\n    # loop category values\n    for selections, _ in self._metadata.groupby(by=categories):\n        if isinstance(selections, tuple):\n            # - if multiple categories to group by are specified -\n            # init helper DataFrame\n            target_idx = pd.DataFrame()\n            # identify corresponding samples\n            for selection, category in zip(selections, categories):\n                target_idx[category] = self._metadata[category] == selection\n            # get relevant indices\n            target_idx = target_idx.sum(axis=1) == len(categories)\n            # define title\n            title = f\"Grouped by {build_helper_string(categories)} ({build_helper_string(selections)})\"\n        else:\n            # - if only one(!) category to group by is specified -\n            # identify corresponding samples\n            target_idx = self._metadata[categories] == selections\n            # define title\n            title = f\"Grouped by {categories} ({selections})\"\n\n        # pick relevant samples\n        target_samples = self._metadata.loc[target_idx, self._metadata_id]\n\n        # build corresponding regex\n        regex = \"(\" + \")|(\".join(target_samples) + \")\"\n\n        # plot\n        ax = self.plot(regex=regex, t_unit=t_unit, y=y, y_unit_milli=y_unit_milli)\n\n        # set title\n        ax.set_title(title)\n\n        # yield latest plot\n        yield selections, ax\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.remove_pickle_files","title":"<code>remove_pickle_files()</code>","text":"<p>remove pickle files if re-reading of source files needed</p> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def remove_pickle_files(self):\n    \"\"\"\n    remove pickle files if re-reading of source files needed\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # remove files\n    for file in [self._file_data_pickle, self._file_info_pickle]:\n        # remove file\n        pathlib.Path(file).unlink()\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.undo_average_by_metadata","title":"<code>undo_average_by_metadata()</code>","text":"<p>undo action of \"average_by_metadata\"</p> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def undo_average_by_metadata(self):\n    \"\"\"\n    undo action of \"average_by_metadata\"\n    \"\"\"\n\n    # set \"unprocessed\" data as exeperimental data / \"de-average\"\n    if not self._data_unprocessed.empty:\n        # reset\n        self._data = self._data_unprocessed.copy()\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.Measurement.undo_tian_correction","title":"<code>undo_tian_correction()</code>","text":"<p>undo_tian_correction; i.e. restore original data</p> <p>Returns:</p> Type Description <code>None.</code> Source code in <code>calocem/tacalorimetry.py</code> <pre><code>def undo_tian_correction(self):\n    \"\"\"\n    undo_tian_correction; i.e. restore original data\n\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n\n    # call original restore function\n    self.undo_average_by_metadata()\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.MedianFilterParameters","title":"<code>MedianFilterParameters</code>  <code>dataclass</code>","text":"<p>Parameters for the application of a Median filter to the data. The SciPy method <code>median_filter</code> is applied. Link to method</p> <p>Parameters:</p> Name Type Description Default <code>apply</code> <code>bool</code> <p>default is false. If <code>True</code>, a median filter is applied. The Scipy function <code>median_filter</code> is  applied.</p> <code>False</code> <code>size</code> <code>int</code> <p>The size of the median filter (see the SciPy documentation)</p> <code>7</code> Source code in <code>calocem/processparams.py</code> <pre><code>@dataclass\nclass MedianFilterParameters:\n    \"\"\"Parameters for the application of a Median filter to the data. The SciPy method `median_filter` is applied. [Link to method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.median_filter.html)\n\n\n    Parameters\n    ----------\n    apply: bool\n        default is false. If `True`, a median filter is applied. The Scipy function `median_filter` is  applied.\n    size: int\n        The size of the median filter (see the SciPy documentation)\n\n    \"\"\"\n\n    apply: bool = False\n    size: int = 7\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.PeakDetectionParameters","title":"<code>PeakDetectionParameters</code>  <code>dataclass</code>","text":"<p>Parameters that control the identication of peaks during peak detection.</p> <p>Parameters:</p> Name Type Description Default <code>prominence</code> <code>float</code> <p>The minimum prominence of the peak</p> <code>1e-05</code> <code>distance</code> <code>int</code> <p>The minimum distance of the peak.</p> <code>100</code> Source code in <code>calocem/processparams.py</code> <pre><code>@dataclass\nclass PeakDetectionParameters:\n    \"\"\"\n    Parameters that control the identication of peaks during peak detection.\n\n    Parameters\n    ----------\n\n    prominence: float\n        The minimum prominence of the peak\n    distance: int\n        The minimum distance of the peak.\n    \"\"\"\n\n    prominence: float = 1e-5\n    distance: int = 100\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.PreProcessParameters","title":"<code>PreProcessParameters</code>  <code>dataclass</code>","text":"<p>Parameters for preprocessing the data before analysis.</p> <p>Attributes:</p> Name Type Description <code>infer_heat</code> <code>bool</code> <p>If True, the heat flow is inferred from the data. This is useful for data that does not have a heat flow column.</p> Source code in <code>calocem/processparams.py</code> <pre><code>@dataclass\nclass PreProcessParameters:\n    \"\"\"\n    Parameters for preprocessing the data before analysis.\n\n    Attributes\n    ----------\n    infer_heat: bool\n        If True, the heat flow is inferred from the data. This is useful for data that does not have a heat flow column.\n    \"\"\"\n\n    infer_heat: bool = False\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.ProcessingParameters","title":"<code>ProcessingParameters</code>  <code>dataclass</code>","text":"<p>A data class for storing all processing parameters for calorimetry data.</p> <p>This class aggregates various processing parameters, including cutoff criteria, time constants for the Tian correction, and parameters for peak detection and gradient peak detection.</p> <p>Attributes:</p> Name Type Description <code>cutoff</code> <code>CutOffParameters</code> <p>Parameters defining the cutoff criteria for the analysis. Currently only cutoff_min is implemented, which defines the minimum time in minutes for the analysis. The default value is defined in the CutOffParameters class.</p> <code>time_constants</code> <code>TianCorrectionParameters</code> <p>Parameters related to time constants used in Tian's correction method for thermal analysis. he default values are defined in the TianCorrectionParameters class.</p> <code>peakdetection</code> <code>PeakDetectionParameters</code> <p>Parameters for detecting peaks in the thermal analysis data. This includes settings such as the minimum prominence and distance between peaks. The default values are defined in the PeakDetectionParameters class.</p> <code>gradient_peakdetection</code> <code>GradientPeakDetectionParameters</code> <p>Parameters for detecting peaks based on the gradient of the thermal analysis data. This includes more nuanced settings such as prominence, distance, width, relative height, and the criteria for selecting peaks (e.g., first peak, largest width). The default values are defined in the GradientPeakDetectionParameters class.</p> <code>downsample</code> <code>DownSamplingParameters</code> <p>Parameters for adaptive downsampling of the thermal analysis data. This includes settings such as the number of points, smoothing factor, and baseline weight. The default values are defined in the DownSamplingParameters class.</p> <code>spline_interpolation</code> <code>SplineInterpolationParameters</code> <p>Parameters which control the interpolation of the first and second derivative of the data. If no smoothing is applied the derivatives often become very noisy.</p> <code>slope_analysis</code> <code>SlopeAnalysisParameters</code> <p>Parameters for slope analysis of the heat flow data. This includes settings which control the identification of the mean slope of the main hydration peak.</p> <p>Examples:</p> <p>Define a set of processing parameters for thermal analysis data.</p> <pre><code>&gt;&gt;&gt; processparams = ProcessingParameters()\n&gt;&gt;&gt; processparams.cutoff.cutoff_min = 30\n&gt;&gt;&gt; processparams.spline_interpolation.apply = True\n</code></pre> Source code in <code>calocem/processparams.py</code> <pre><code>@dataclass\nclass ProcessingParameters:\n    \"\"\"\n    A data class for storing all processing parameters for calorimetry data.\n\n    This class aggregates various processing parameters, including cutoff criteria, time constants for the Tian correction, and parameters for peak detection and gradient peak detection.\n\n    Attributes\n    ----------\n\n    cutoff :\n        Parameters defining the cutoff criteria for the analysis.\n        Currently only cutoff_min is implemented, which defines the minimum time in minutes for the analysis. The default value is defined in the CutOffParameters class.\n\n    time_constants : TianCorrectionParameters\n        Parameters related to time constants used in Tian's correction method for thermal analysis. he default values are defined in the\n        TianCorrectionParameters class.\n\n    peakdetection : PeakDetectionParameters\n        Parameters for detecting peaks in the thermal analysis data. This includes settings such as the minimum\n        prominence and distance between peaks. The default values are defined in the PeakDetectionParameters class.\n\n    gradient_peakdetection : GradientPeakDetectionParameters\n        Parameters for detecting peaks based on the gradient of the thermal analysis data. This includes more\n        nuanced settings such as prominence, distance, width, relative height, and the criteria for selecting peaks\n        (e.g., first peak, largest width). The default values are defined in the GradientPeakDetectionParameters class.\n\n    downsample : DownSamplingParameters\n        Parameters for adaptive downsampling of the thermal analysis data. This includes settings such as the number of points,\n        smoothing factor, and baseline weight. The default values are defined in the DownSamplingParameters class.\n\n    spline_interpolation: SplineInterpolationParameters\n        Parameters which control the interpolation of the first and second derivative of the data. If no smoothing is applied the derivatives often become very noisy.\n\n    slope_analysis: SlopeAnalysisParameters\n        Parameters for slope analysis of the heat flow data. This includes settings which control the identification of the mean slope of the main hydration peak.\n\n\n    Examples\n    --------\n\n    Define a set of processing parameters for thermal analysis data.\n\n    &gt;&gt;&gt; processparams = ProcessingParameters()\n    &gt;&gt;&gt; processparams.cutoff.cutoff_min = 30\n    &gt;&gt;&gt; processparams.spline_interpolation.apply = True\n    \"\"\"\n\n    cutoff: CutOffParameters = field(default_factory=CutOffParameters)\n    time_constants: TianCorrectionParameters = field(\n        default_factory=TianCorrectionParameters\n    )\n\n    # peak detection params\n    peakdetection: PeakDetectionParameters = field(\n        default_factory=PeakDetectionParameters\n    )\n    gradient_peakdetection: GradientPeakDetectionParameters = field(\n        default_factory=GradientPeakDetectionParameters\n    )\n\n    # smoothing params\n    rolling_mean: RollingMeanParameters = field(default_factory=RollingMeanParameters)\n    median_filter: MedianFilterParameters = field(\n        default_factory=MedianFilterParameters\n    )\n    nonlin_savgol: NonLinSavGolParameters = field(\n        default_factory=NonLinSavGolParameters\n    )\n    spline_interpolation: SplineInterpolationParameters = field(\n        default_factory=SplineInterpolationParameters\n    )\n    # preprocessing params\n    downsample: DownSamplingParameters = field(default_factory=DownSamplingParameters)\n    preprocess: PreProcessParameters = field(default_factory=PreProcessParameters)\n    # slope analysis params\n    slope_analysis: SlopeAnalysisParameters = field(\n        default_factory=SlopeAnalysisParameters\n    )\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.SlopeAnalysisParameters","title":"<code>SlopeAnalysisParameters</code>  <code>dataclass</code>","text":"<p>Parameters for slope analysis of heat flow data.</p> <p>Attributes:</p> Name Type Description <code>flank_fraction_start</code> <code>float</code> <p>The start fraction of the window for averaging the slope of the main hydration peak. Example: 0.35 (the default value) means that the slope is calculated starting from 35% of the peak height measured relative to the minimum of the dormant period heat flow.</p> <code>flank_fraction_end</code> <code>float</code> <p>The end fraction of the window for averaging the slope of the main hydration peak. Example: 0.55 (the default value) means that the slope is calculated up to 55% of the peak height measured relative to the minimum of the dormant period heat flow.</p> <code>window_size</code> <code>float</code> <p>The size of the window for averaging the slope, given as a fraction of the total number of data points. Example: 0.1 (the default value) means that the slope is averaged over a window that is 10% of the total number of data points.</p> Source code in <code>calocem/processparams.py</code> <pre><code>@dataclass\nclass SlopeAnalysisParameters:\n    \"\"\"\n    Parameters for slope analysis of heat flow data.\n\n    Attributes\n    ----------\n    flank_fraction_start: float\n        The start fraction of the window for averaging the slope of the main hydration peak. Example: 0.35 (the default value) means that the slope is calculated starting from 35% of the peak height measured relative to the minimum of the dormant period heat flow.\n    flank_fraction_end: float\n        The end fraction of the window for averaging the slope of the main hydration peak. Example: 0.55 (the default value) means that the slope is calculated up to 55% of the peak height measured relative to the minimum of the dormant period heat flow.\n    window_size: float\n        The size of the window for averaging the slope, given as a fraction of the total number of data points. Example: 0.1 (the default value) means that the slope is averaged over a window that is 10% of the total number of data points.\n    \"\"\"\n\n    flank_fraction_start: float = 0.35\n    flank_fraction_end: float = 0.55\n    window_size: float = 0.1  # as fraction of total data points\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.SplineInterpolationParameters","title":"<code>SplineInterpolationParameters</code>  <code>dataclass</code>","text":"<p>Parameters for spline interpolation of heat flow data.</p> <p>Parameters:</p> Name Type Description Default <code>apply</code> <code>bool</code> <p>Flag indicating whether spline interpolation should be applied to the heat flow data. The default value is False.</p> <code>False</code> <code>smoothing_1st_deriv</code> <code>float</code> <p>Smoothing parameter for the first derivative of the heat flow data. The default value is 1e-9.</p> <code>1e-09</code> <code>smoothing_2nd_deriv</code> <code>float</code> <p>Smoothing parameter for the second derivative of the heat flow data. The default value is 1e-9.</p> <code>1e-09</code> Source code in <code>calocem/processparams.py</code> <pre><code>@dataclass\nclass SplineInterpolationParameters:\n    \"\"\"Parameters for spline interpolation of heat flow data.\n\n    Parameters\n    ----------\n\n    apply :\n        Flag indicating whether spline interpolation should be applied to the heat flow data. The default value is False.\n\n    smoothing_1st_deriv :\n        Smoothing parameter for the first derivative of the heat flow data. The default value is 1e-9.\n\n    smoothing_2nd_deriv :\n        Smoothing parameter for the second derivative of the heat flow data. The default value is 1e-9.\n\n    \"\"\"\n\n    apply: bool = False\n    smoothing_1st_deriv: float = 1e-9\n    smoothing_2nd_deriv: float = 1e-9\n</code></pre>"},{"location":"reference.html#calocem.tacalorimetry.TianCorrectionParameters","title":"<code>TianCorrectionParameters</code>  <code>dataclass</code>","text":"<p>Parameters related to time constants used in Tian's correction method for thermal analysis. The default values are defined in the TianCorrectionParameters class.</p> <p>Parameters:</p> Name Type Description Default <code>tau1</code> <code>int</code> <p>Time constant for the first correction step in Tian's method. The default value is 300.</p> <code>300</code> <code>tau2</code> <code>int</code> <p>Time constant for the second correction step in Tian's method. The default value is 100.</p> <code>100</code> Source code in <code>calocem/processparams.py</code> <pre><code>@dataclass\nclass TianCorrectionParameters:\n    \"\"\"\n    Parameters related to time constants used in Tian's correction method for thermal analysis. The default values are defined in the TianCorrectionParameters class.\n\n    Parameters\n    ----------\n    tau1 : int\n        Time constant for the first correction step in Tian's method. The default value is 300.\n\n    tau2 : int\n        Time constant for the second correction step in Tian's method. The default value is 100.\n    \"\"\"\n\n    tau1: int = 300\n    tau2: int = 100\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement","title":"<code>Measurement</code>","text":"<p>Class for handling and processing isothermal heat flow calorimetry data.</p> <p>This class coordinates file I/O, data processing, analysis, and visualization operations while maintaining the same API as the original implementation.</p> Source code in <code>calocem/measurement.py</code> <pre><code>class Measurement:\n    \"\"\"\n    Class for handling and processing isothermal heat flow calorimetry data.\n\n    This class coordinates file I/O, data processing, analysis, and visualization\n    operations while maintaining the same API as the original implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        folder: Optional[Union[str, pathlib.Path]] = None,\n        show_info: bool = True,\n        regex: Optional[str] = None,\n        auto_clean: bool = False,\n        cold_start: bool = True,\n        processparams: Optional[ProcessingParameters] = None,\n        new_code: bool = False,\n        processed: bool = False,\n    ):\n        \"\"\"\n        Initialize measurements from folder or existing data.\n\n        Parameters\n        ----------\n        folder : str or pathlib.Path, optional\n            Path to folder containing experimental files\n        show_info : bool, optional\n            Whether to print informative messages, by default True\n        regex : str, optional\n            Regex pattern to filter files, by default None\n        auto_clean : bool, optional\n            Whether to clean data automatically, by default False\n        cold_start : bool, optional\n            Whether to read from files or use cached data, by default True\n        processparams : ProcessingParameters, optional\n            Processing parameters, by default None. If None, the default parameters will be used\n        new_code : bool, optional\n            Flag for new code features, by default False\n        processed : bool, optional\n            Whether data is already processed, i.e., if a .csv file is used which was processed  by Calocem. By default False\n        \"\"\"\n        # Initialize attributes\n        self._data = pd.DataFrame()\n        self._info = pd.DataFrame()\n        self._data_unprocessed = pd.DataFrame()\n        self._metadata = pd.DataFrame()\n        self._metadata_id = \"\"\n\n        # Store configuration\n        self._new_code = new_code\n        self._processed = processed\n\n        # Setup processing parameters\n        if not isinstance(processparams, ProcessingParameters):\n            self.processparams = ProcessingParameters()\n        else:\n            self.processparams = processparams\n\n        # Initialize components\n        self._folder_loader = FolderDataLoader(processed=processed)\n        self._data_persistence = DataPersistence()\n        self._data_cleaner = DataCleaner()\n        self._plotter = SimplePlotter()\n\n        # Load data if folder provided\n        if folder:\n            try:\n                if cold_start:\n                    self._load_from_folder(folder, regex, show_info)\n                else:\n                    self._load_from_cache()\n\n                if auto_clean:\n                    self._auto_clean_data()\n\n            except Exception as e:\n                if show_info:\n                    print(f\"Error during initialization: {e}\")\n                if auto_clean:\n                    raise AutoCleanException()\n                if not cold_start:\n                    raise ColdStartException()\n                raise\n\n        # Apply downsampling if requested\n        if self.processparams.downsample.apply:\n            self._apply_adaptive_downsampling()\n\n        # Information message\n        if show_info:\n            print(\"================\")\n            print(\n                \"Are you missing some samples? Try rerunning with auto_clean=True and cold_start=True.\"\n            )\n            print(\"================\")\n\n    def _load_from_folder(\n        self, folder: Union[str, pathlib.Path], regex: Optional[str], show_info: bool\n    ):\n        \"\"\"Load data from folder using file loader.\"\"\"\n        try:\n            self._data, self._info = self._folder_loader.load_from_folder(\n                folder, regex, show_info\n            )\n            self._data_unprocessed = self._data.copy()\n\n            # Save to cache\n            self._data_persistence.save_data(self._data, self._info)\n\n        except Exception as e:\n            raise DataProcessingException(\"load_from_folder\", e)\n\n    def _load_from_cache(self):\n        \"\"\"Load data from cached pickle files.\"\"\"\n        try:\n            if not self._data_persistence.pickle_files_exist():\n                raise FileNotFoundError(\"No pickle files found for cold start\")\n\n            self._data, self._info = self._data_persistence.load_data()\n            self._data_unprocessed = self._data.copy()\n\n        except Exception as e:\n            raise ColdStartException() from e\n\n    def _auto_clean_data(self):\n        \"\"\"Apply automatic data cleaning.\"\"\"\n        try:\n            self._data = self._data_cleaner.auto_clean_data(self._data)\n        except Exception as e:\n            raise AutoCleanException() from e\n\n    def _apply_adaptive_downsampling(self):\n        \"\"\"Apply adaptive downsampling if configured.\"\"\"\n        # TODO: Implement downsampling logic\n        logger.info(\"Downsampling requested but not yet implemented\")\n\n    # Data access methods\n    def get_data(self) -&gt; pd.DataFrame:\n        \"\"\"Get the processed calorimetry data.\"\"\"\n        return self._data\n\n    def get_information(self) -&gt; pd.DataFrame:\n        \"\"\"Get the measurement information/metadata.\"\"\"\n        return self._info\n\n    def get_metadata(self) -&gt; tuple:\n        \"\"\"Get added metadata and the ID column name.\"\"\"\n        return self._metadata, self._metadata_id\n\n    def get_sample_names(self) -&gt; list:\n        \"\"\"Get list of sample names.\"\"\"\n        return [\n            pathlib.Path(str(sample)).stem\n            for sample, _ in SampleIterator.iter_samples(self._data)\n        ]\n\n    # Plotting methods\n    def plot(\n        self,\n        t_unit: str = \"h\",\n        y: str = \"normalized_heat_flow_w_g\",\n        y_unit_milli: bool = True,\n        regex: Optional[str] = None,\n        show_info: bool = True,\n        ax=None,\n    ):\n        \"\"\"Plot the calorimetry data.\"\"\"\n        return self._plotter.plot_data(\n            self._data, t_unit, y, y_unit_milli, regex, show_info, ax\n        )\n\n    def plot_by_category(\n        self,\n        categories: str,\n        t_unit: str = \"h\",\n        y: str = \"normalized_heat_flow_w_g\",\n        y_unit_milli: bool = True,\n    ):\n        \"\"\"Plot data by metadata categories.\"\"\"\n        # Simplified implementation - would need full metadata integration\n        logger.warning(\n            \"plot_by_category requires metadata integration - not fully implemented\"\n        )\n        yield from []\n\n    # Analysis methods\n    def get_peaks(\n        self,\n        processparams: Optional[ProcessingParameters] = None,\n        target_col: str = \"normalized_heat_flow_w_g\",\n        regex: Optional[str] = None,\n        cutoff_min: Optional[float] = None,  # Deprecated parameter\n        show_plot: bool = True,\n        plt_right_s: float = 2e5,\n        plt_top: float = 1e-2,\n        ax=None,\n        xunit: str = \"s\",\n        plot_labels: Optional[bool] = None,\n        xmarker: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get DataFrame of peak characteristics.\"\"\"\n        if cutoff_min is not None:\n            warnings.warn(\n                \"The cutoff_min parameter is deprecated. Use ProcessingParameters instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        params = processparams or self.processparams\n        analyzer = PeakAnalyzer(params)\n        peaks_df = analyzer.get_peaks(self._data, target_col, regex)\n\n        if show_plot and not peaks_df.empty:\n            # Simple plotting implementation\n            for sample, sample_data in SampleIterator.iter_samples(self._data, regex):\n                sample_peaks = peaks_df[\n                    peaks_df[\"sample_short\"] == pathlib.Path(str(sample)).stem\n                ]\n                if not sample_peaks.empty:\n                    # Get peak indices relative to sample data\n                    import numpy as np\n\n                    peak_indices = np.array(sample_peaks.index.tolist())\n                    self._plotter.plot_peaks(\n                        sample_data, peak_indices, str(sample), ax, \"time_s\", target_col\n                    )\n\n        return peaks_df\n\n    def get_peak_onsets(\n        self,\n        target_col: str = \"normalized_heat_flow_w_g\",\n        age_col: str = \"time_s\",\n        time_discarded_s: float = 900,\n        rolling: int = 1,\n        gradient_threshold: float = 0.0005,\n        show_plot: bool = False,\n        exclude_discarded_time: bool = False,\n        regex: Optional[str] = None,\n        ax=None,\n    ):\n        \"\"\"Get peak onsets based on gradient threshold.\"\"\"\n        analyzer = OnsetAnalyzer(self.processparams)\n        return analyzer.get_peak_onsets(\n            self._data,\n            target_col,\n            age_col,\n            time_discarded_s,\n            rolling,\n            gradient_threshold,\n            exclude_discarded_time,\n            regex,\n        )\n\n    def get_maximum_slope(\n        self,\n        processparams: Optional[ProcessingParameters] = None,\n        target_col: str = \"normalized_heat_flow_w_g\",\n        age_col: str = \"time_s\",\n        time_discarded_s: float = 900,\n        show_plot: bool = False,\n        show_info: bool = True,\n        exclude_discarded_time: bool = False,\n        regex: Optional[str] = None,\n        read_start_c3s: bool = False,\n        ax=None,\n        save_path: Optional[pathlib.Path] = None,\n        xscale: str = \"linear\",\n        xunit: str = \"s\",\n    ):\n        \"\"\"Find the point in time of the maximum slope.\"\"\"\n        params = processparams or self.processparams\n\n        time_discarded_s = (\n            params.cutoff.cutoff_min * 60 if params.cutoff.cutoff_min else 0\n        )\n        analyzer = SlopeAnalyzer(params)\n\n        result = analyzer.get_maximum_slope(\n            self._data,\n            target_col,\n            age_col,\n            time_discarded_s,\n            exclude_discarded_time,\n            regex,\n            # read_start_c3s,\n            # self._metadata,\n        )\n\n        if show_plot and not result.empty:\n            for sample, sample_data in SampleIterator.iter_samples(self._data, regex):\n                sample_short = pathlib.Path(str(sample)).stem\n                sample_result = result[result[\"sample_short\"] == sample_short]\n                sample_result = sample_result[\n                    sample_result[age_col] &gt;= time_discarded_s\n                ]\n                if not sample_result.empty:\n                    self._plotter.plot_slopes(\n                        sample_data,\n                        sample_result,\n                        str(sample_short),\n                        ax,\n                        age_col,\n                        target_col,\n                    )\n\n        return result\n\n    def get_mainpeak_params(\n        self,\n        processparams: Optional[ProcessingParameters] = None,\n        target_col: str = \"normalized_heat_flow_w_g\",\n        age_col: str = \"time_s\",\n        show_plot: bool = False,\n        plot_type: str = \"mean\",\n        regex: Optional[str] = None,\n        plotpath: Optional[pathlib.Path] = None,\n        ax=None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Unified method that calculates BOTH maximum and mean slope onset analyses.\n\n        This method performs both slope-based analysis approaches simultaneously:\n        - Maximum slope: Uses single point with maximum gradient for onset determination\n        - Mean slope: Uses averaged slope over flank windows for onset determination\n\n        Both results are returned in a single DataFrame with all slope values and onsets.\n\n        Parameters\n        ----------\n        processparams : ProcessingParameters, optional\n            Processing parameters, by default None\n        target_col : str\n            Column containing heat flow data. The default is 'normalized_heat_flow_w_g'.\n        age_col : str\n            Column containing time data. The default is 'time_s'.\n        show_plot : bool\n            Whether to plot the results\n        plot_type : str\n            Type of plot to show: 'max', 'mean',\n            - 'max': Shows only maximum slope analysis plot\n            - 'mean': Shows only mean slope (flank tangent) analysis plot\n        regex : str, optional\n            Regex to filter samples\n        plotpath : pathlib.Path, optional\n            Path to save plots\n        ax : matplotlib.axes.Axes, optional\n            Matplotlib axes to plot on\n\n        Returns\n        -------\n        pd.DataFrame\n            Comprehensive DataFrame with both max and mean slope results including:\n            - Gradients and curvatures at max slope\n            - Gradients of mean slope\n            - Onset times from both methods\n            - Normalized heat flow and heat values at key points\n            - Dormant period heat flow values\n            - ASTM C1679 characteristic values\n\n        Examples\n        --------\n        &gt;&gt;&gt; measurement = Measurement(folder=\"data/\")\n        &gt;&gt;&gt; mainpeak_params = measurement.get_mainpeak_params(\n        ...     processparams=ProcessingParameters(),\n        ...     show_plot=False,\n        ...     plot_type=\"mean\",\n        ... )\n        \"\"\"\n        params = processparams or self.processparams\n\n        max_slope_results = self._calculate_max_slope_analysis(\n            params,\n            target_col,\n            age_col,\n            regex,\n        )\n\n        mean_slope_results = self._calculate_mean_slope_analysis(\n            params,\n            target_col,\n            age_col,\n            regex,\n        )\n\n        dormant_minimum_heatflow = self.get_dormant_period_heatflow(\n            params, regex, show_plot=False\n        )\n\n        astm_values = self.get_astm_c1679_characteristics(params, individual=True, show_plot=False, regex=regex)\n\n        # Merge results into comprehensive DataFrame\n        combined_results = self._merge_slope_results(\n            max_slope_results, mean_slope_results, dormant_minimum_heatflow, astm_values\n        )\n\n        # Plot if requested\n        if show_plot and not (mean_slope_results.empty or max_slope_results.empty):\n            self._plot_combined_slope_analysis(\n                combined_results,\n                params,\n                target_col,\n                age_col,\n                plot_type,\n                regex,\n                plotpath,\n                ax,\n            )\n            if not ax:\n                plt.show()\n        elif mean_slope_results.empty:\n            # logger.warning(\"No slope analysis results to plot.\")\n            print(\"No mean slope analysis obtained - check the processing parameters.\")\n\n        elif max_slope_results.empty:\n            print(\n                \"No maximum slope analysis obtained - check the processing parameters.\"\n            )\n\n        return combined_results\n\n    def _calculate_max_slope_analysis(\n        self,\n        params: ProcessingParameters,\n        target_col: str,\n        age_col: str,\n        regex: Optional[str],\n    ) -&gt; pd.DataFrame:\n        \"\"\"Calculate maximum slope analysis and return structured results.\"\"\"\n        # Get required data\n        max_slope_analyzer = SlopeAnalyzer(params)\n        max_slopes = max_slope_analyzer.get_maximum_slope(\n            self._data,\n            target_col,\n            age_col,\n            regex,\n        )\n\n        if max_slopes.empty:\n            logger.warning(\"No maximum slopes found. Check processing parameters.\")\n            return pd.DataFrame()\n\n        dormant_hfs = self.get_dormant_period_heatflow(params, regex, show_plot=False)\n        if dormant_hfs.empty:\n            logger.warning(\"No dormant period heat flows found.\")\n            return pd.DataFrame()\n\n        # Calculate onsets\n        analyzer = OnsetAnalyzer(params)\n        onsets = analyzer.get_peak_onset_via_max_slope(\n            self._data,\n            max_slopes,\n            dormant_hfs,  # intersection, xunit\n        )\n\n        # Structure results with consistent naming\n        results = []\n        for _, slope_row in max_slopes.iterrows():\n            sample = slope_row.get(\"sample\", slope_row.get(\"sample_short\", \"\"))\n            sample_short = slope_row.get(\"sample_short\", slope_row.get(\"sample\", \"\"))\n\n            onset_row = (\n                onsets[onsets[\"sample_short\"] == sample_short]\n                if not onsets.empty\n                else pd.DataFrame()\n            )\n            onset_time = (\n                onset_row.iloc[0][\"onset_time_s\"] if not onset_row.empty else None\n            )\n\n            # get normalized_heat_j_g at onset_time\n            if onset_time and not pd.isna(onset_time):\n                onset_j_g = np.interp(\n                    onset_time,\n                    self._data[age_col],\n                    self._data[\"normalized_heat_j_g\"],\n                )\n            else:\n                onset_j_g = None\n\n            result_data = {\n                \"sample\": sample,\n                \"sample_short\": sample_short,\n                \"gradient_from_max_slope\": slope_row.get(\"gradient\", 0),\n                \"curvature_at_max_slope\": slope_row.get(\"curvature\", 0),\n                \"max_slope_time_s\": slope_row.get(\"time_s\", 0),\n                \"normalized_heat_flow_w_g_at_max_slope\": slope_row.get(\n                    \"normalized_heat_flow_w_g\", 0\n                ),\n                \"normalized_heat_j_g_at_max_slope\": slope_row.get(\"normalized_heat_j_g\", 0),\n                \"normalized_heat_j_g_at_onset_time_max_slope\": onset_j_g,\n                \"onset_time_s_from_max_slope\": onset_time,\n                \"onset_time_min_max_slope\": onset_time / 60 if onset_time else None,\n                \"onset_time_s_max_slope_abscissa\": (\n                    onset_row.iloc[0][\"onset_time_s_abscissa\"]\n                    if not onset_row.empty\n                    else None\n                ),\n            }\n            results.append(result_data)\n\n        return pd.DataFrame(results)\n\n    def _calculate_mean_slope_analysis(\n        self,\n        params: ProcessingParameters,\n        target_col: str,\n        age_col: str,\n        regex: Optional[str],\n    ) -&gt; pd.DataFrame:\n        \"\"\"Calculate mean slope (flank tangent) analysis and return structured results.\"\"\"\n        analyzer = FlankTangentAnalyzer(params)\n\n        # Get flank tangent results\n        tangent_results = analyzer.get_ascending_flank_tangent(\n            self._data,\n            target_col,\n            age_col,\n            regex,\n        )\n\n        if tangent_results.empty:\n            logger.warning(\"No flank tangent results found.\")\n            return pd.DataFrame()\n\n        results = []\n        for _, row in tangent_results.iterrows():\n            sample = row.get(\"sample\", row.get(\"sample_short\", \"\"))\n            sample_short = row.get(\"sample_short\", row.get(\"sample\", \"\"))\n\n            # onset by intersection with tangent to dormant period\n            onset_time = row.get(\"x_intersection_dormant\", row.get(\"tangent_time_s\", 0))\n\n            result_data = {\n                \"sample\": sample,\n                \"sample_short\": sample_short,\n                \"gradient_of_mean_slope\": row.get(\"tangent_slope\", 0),\n                \"mean_slope_time_s\": row.get(\"tangent_time_s\", 0),\n                \"normalized_heat_flow_w_g_at_mean_slope\": row.get(\"tangent_value\", 0),\n                \"normalized_heat_j_g_at_mean_slope\": row.get(\"tangent_j_g\", 0),\n                \"onset_time_s_from_mean_slope\": onset_time,\n                \"onset_time_min_from_mean_slope\": onset_time / 60 if onset_time else None,\n                \"onset_time_s_from_mean_slope_abscissa\": row.get(\"x_intersection\", 0),\n                \"normalized_heat_at_onset_time_mean_slope_abscissa_j_g\": row.get(\"x_intersection_j_g\", 0),\n                \"normalized_heat_at_onset_time_mean_slope_dormant_j_g\": row.get(\"x_intersection_dormant_j_g\", 0),\n                \"flank_start_value\": row.get(\"flank_start_value\", 0),\n                \"flank_end_value\": row.get(\"flank_end_value\", 0),\n                \"peak_time_s\": row.get(\"peak_time_s\", 0),\n                \"normalized_heat_flow_w_g_at_peak\": row.get(\"peak_value\", 0),\n                \"normalized_heat_j_g_at_peak\": row.get(\"peak_j_g\", 0),\n            }\n            results.append(result_data)\n\n        return pd.DataFrame(results)\n\n    def _merge_slope_results(\n        self,\n        max_slope_results: pd.DataFrame,\n        mean_slope_results: pd.DataFrame,\n        dormant_hf_results: pd.DataFrame,\n        astm_results: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Merge max slope and mean slope results into comprehensive DataFrame.\"\"\"\n        if (\n            max_slope_results.empty\n            and mean_slope_results.empty\n            and dormant_hf_results.empty\n            and astm_results.empty\n        ):\n            return pd.DataFrame()\n\n        # Use outer join to combine results by sample\n        if max_slope_results.empty:\n            return mean_slope_results\n        if mean_slope_results.empty:\n            return max_slope_results\n\n        combined = pd.merge(\n            max_slope_results,\n            mean_slope_results,\n            on=[\"sample\", \"sample_short\"],\n            how=\"outer\",\n            suffixes=(\"\", \"_duplicate\"),\n        )\n\n        combined = pd.merge(\n            combined,\n            dormant_hf_results,\n            on=[\"sample\", \"sample_short\"],\n            how=\"outer\",\n            suffixes=(\"\", \"_duplicate\"),\n        )\n\n        combined = pd.merge(\n            combined,\n            astm_results,\n            on=[\"sample\", \"sample_short\"],\n            how=\"outer\",\n            suffixes=(\"\", \"_duplicate\"),\n        )\n\n        duplicate_cols = [col for col in combined.columns if col.endswith(\"_duplicate\")]\n        combined = combined.drop(columns=duplicate_cols)\n\n        return combined\n\n    def _plot_combined_slope_analysis(\n        self,\n        results: pd.DataFrame,\n        params: ProcessingParameters,\n        target_col: str,\n        age_col: str,\n        plot_type: str,\n        regex: Optional[str],\n        plotpath: Optional[pathlib.Path],\n        ax,\n    ):\n        \"\"\"\n        Plot combined slope analysis results based on plot_type parameter.\n\n        Parameters\n        ----------\n        results : pd.DataFrame\n            Combined results containing both max and mean slope data\n        target_col : str\n            Column name for heat flow data\n        age_col : str\n            Column name for time data\n        plot_type : str\n            Type of plot to show: 'max', 'mean', or 'both'\n            - 'max': Shows only maximum slope analysis plot\n            - 'mean': Shows only mean slope (flank tangent) analysis plot\n            - 'both': Shows both analysis types (separate plots for each)\n        regex : str, optional\n            Regex to filter samples\n        plotpath : pathlib.Path, optional\n            Path to save plots\n        cutoff_min : float, optional\n            Cutoff time in minutes\n        ax : matplotlib.axes.Axes, optional\n            Matplotlib axes to plot on\n        \"\"\"\n        # Validate plot_type parameter\n        valid_plot_types = [\"max\", \"mean\", \"both\"]\n        cutoff_min = params.cutoff.cutoff_min\n\n        if plot_type not in valid_plot_types:\n            raise ValueError(\n                f\"plot_type must be one of {valid_plot_types}, got '{plot_type}'\"\n            )\n\n        # For now, plot using the existing unified plotting approach\n        # This could be enhanced to show both slope methods simultaneously\n        for _, result_row in results.iterrows():\n            sample = result_row[\"sample\"]\n            sample_short = result_row[\"sample_short\"]\n\n            # Get sample data\n            sample_data = self._get_filtered_sample_data(\n                sample, age_col, cutoff_time_min=cutoff_min\n            )\n            if sample_data.empty:\n                continue\n\n            if not pd.isna(\n                result_row.onset_time_s_from_mean_slope or result_row.onset_time_s_from_max_slope\n            ):\n                self._plotter.plot_tangent_analysis(\n                    sample_data,\n                    sample_short,\n                    params,\n                    ax=ax,\n                    age_col=age_col,\n                    target_col=target_col,\n                    cutoff_time_min=cutoff_min,\n                    analysis_type=plot_type,  # Use correct analysis type\n                    results=result_row.to_frame().T,\n                    figsize=(7, 5),\n                )\n            self._save_and_show_plot(\n                plotpath, f\"{plot_type}_slope_{sample_short}.png\", ax\n            )\n\n\n    # Backward compatibility methods\n    def get_peak_onset_via_max_slope(\n        self,\n        processparams: Optional[ProcessingParameters] = None,\n        show_plot: bool = False,\n        ax=None,\n        regex: Optional[str] = None,\n        age_col: str = \"time_s\",\n        target_col: str = \"normalized_heat_flow_w_g\",\n        time_discarded_s: float = 900,\n        save_path: Optional[pathlib.Path] = None,\n        xscale: str = \"linear\",\n        xunit: str = \"s\",\n        intersection: str = \"dormant_hf\",\n    ):\n        \"\"\"\n        Get reaction onset via maximum slope intersection method.\n\n        This is a wrapper around get_peak_onset_via_slope for backward compatibility.\n        Returns only the max slope related columns for compatibility.\n        \"\"\"\n        full_results = self.get_mainpeak_params(\n            processparams=processparams,\n            target_col=target_col,\n            age_col=age_col,\n            show_plot=show_plot,\n            regex=regex,\n            ax=ax,\n            plot_type=\"max\",\n\n            #time_discarded_s=time_discarded_s,\n            #intersection=intersection,\n            #xunit=xunit,\n        )\n\n        if full_results.empty:\n            return full_results\n\n        # Extract only max slope related columns for backward compatibility\n        # max_slope_cols = [\n        #     col\n        #     for col in full_results.columns\n        #     if col.startswith(\"max_slope_\") or col in [\"sample\", \"sample_short\"]\n        # ]\n\n        # result = full_results[max_slope_cols].copy()\n\n        # Rename columns to match old API\n        # column_mapping = {\n        #     \"onset_time_s_from_max_slope\": \"onset_time_s\",\n        #     \"max_slope_onset_time_min\": \"onset_time_min\",\n        #     \"max_slope_value\": \"maximum_slope\",\n        #     \"max_slope_time_s\": \"maximum_slope_time_s\",\n        # }\n\n        # for old_name, new_name in column_mapping.items():\n        #     if old_name in result.columns:\n        #         result = result.rename(columns={old_name: new_name})\n\n        return full_results\n\n\n    def get_ascending_flank_tangent(\n        self,\n        processparams: Optional[ProcessingParameters] = None,\n        target_col: str = \"normalized_heat_flow_w_g\",\n        age_col: str = \"time_s\",\n        flank_fraction_start: float = 0.2,\n        flank_fraction_end: float = 0.8,\n        window_size: float = 0.1,\n        cutoff_min: Optional[float] = None,\n        show_plot: bool = False,\n        regex: Optional[str] = None,\n        plotpath: Optional[pathlib.Path] = None,\n        ax=None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Determine tangent to ascending flank of peak by averaging over sections.\n\n        This is a wrapper around get_peak_onset_via_slope for backward compatibility.\n        Returns only the mean slope related columns for compatibility.\n        \"\"\"\n        full_results = self.get_peak_onset_via_slope(\n            processparams=processparams,\n            target_col=target_col,\n            age_col=age_col,\n            cutoff_min=cutoff_min,\n            show_plot=show_plot,\n            regex=regex,\n            plotpath=plotpath,\n            ax=ax,\n            flank_fraction_start=flank_fraction_start,\n            flank_fraction_end=flank_fraction_end,\n            window_size=window_size,\n        )\n\n        if full_results.empty:\n            return full_results\n\n        # Extract only mean slope related columns for backward compatibility\n        mean_slope_cols = [\n            col\n            for col in full_results.columns\n            if col.startswith(\"mean_slope_\")\n            or col in [\"sample\", \"sample_short\", \"peak_time_s\", \"peak_value\"]\n        ]\n\n        result = full_results[mean_slope_cols].copy()\n\n        # Rename columns to match old API\n        column_mapping = {\n            \"mean_slope_onset_time_s\": \"x_intersection\",\n            \"mean_slope_value\": \"tangent_slope\",\n            \"mean_slope_time_s\": \"tangent_time_s\",\n        }\n\n        for old_name, new_name in column_mapping.items():\n            if old_name in result.columns:\n                result = result.rename(columns={old_name: new_name})\n\n        return result\n\n    def get_dormant_period_heatflow(\n        self,\n        processparams: Optional[ProcessingParameters] = None,\n        regex: Optional[str] = None,\n        cutoff_min: int = 5,\n        upper_dormant_thresh_w_g: float = 0.002,\n        plot_right_boundary: float = 2e5,\n        prominence: float = 1e-3,\n        show_plot: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get dormant period heat flow characteristics.\"\"\"\n        params = processparams or self.processparams\n\n        # Get peaks first\n        peaks = self.get_peaks(params, regex=regex, show_plot=False)\n\n        # Analyze dormant period\n        analyzer = DormantPeriodAnalyzer(params)\n        dorm_hf = analyzer.get_dormant_period_heatflow(\n            self._data, peaks, regex, upper_dormant_thresh_w_g\n        )\n\n        if not dorm_hf.empty:\n            return dorm_hf\n        else:\n            return pd.DataFrame()\n\n    def get_astm_c1679_characteristics(\n        self,\n        processparams: Optional[ProcessingParameters] = None,\n        individual: bool = True,\n        show_plot: bool = False,\n        ax=None,\n        regex: Optional[str] = None,\n        xscale: str = \"log\",\n        xunit: str = \"s\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get characteristics according to ASTM C1679.\"\"\"\n        params = processparams or self.processparams\n\n        # Get peaks first\n        peaks = self.get_peaks(params, regex=regex, show_plot=False)\n\n        # Analyze ASTM characteristics\n        analyzer = ASTMC1679Analyzer(params)\n        df = analyzer.get_astm_c1679_characteristics(\n            self._data, peaks, individual, regex\n        )\n        return df\n\n    def get_cumulated_heat_at_hours(\n        self,\n        processparams: Optional[ProcessingParameters] = None,\n        target_h: float = 4,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get cumulated heat flow at specific age.\"\"\"\n        if \"cutoff_min\" in kwargs:\n            cutoff_min = kwargs[\"cutoff_min\"]\n            warnings.warn(\n                \"The cutoff_min parameter is deprecated. Use ProcessingParameters instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        else:\n            params = processparams or self.processparams\n            cutoff_min = params.cutoff.cutoff_min\n\n        return HeatCalculator.get_cumulated_heat_at_hours(\n            self._data, target_h, cutoff_min\n        )\n\n    def get_average_slope(\n        self,\n        processparams: Optional[ProcessingParameters] = None,\n        target_col: str = \"normalized_heat_flow_w_g\",\n        age_col: str = \"time_s\",\n        regex: Optional[str] = None,\n        show_plot: bool = False,\n        ax=None,\n        save_path: Optional[pathlib.Path] = None,\n        xscale: str = \"log\",\n        xunit: str = \"s\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Calculate average slope between onset and heat flow maximum.\"\"\"\n        params = processparams or self.processparams\n\n        # Get required data\n        max_slopes = self.get_maximum_slope(\n            params, target_col, age_col, regex=regex, show_plot=False\n        )\n        onsets = self.get_peak_onset_via_max_slope(params, regex=regex, show_plot=False)\n\n        if max_slopes.empty or onsets.empty:\n            logger.warning(\"Cannot calculate average slopes - missing required data\")\n            return pd.DataFrame()\n\n        analyzer = AverageSlopeAnalyzer(params)\n        result = analyzer.get_average_slope(\n            self._data, max_slopes, onsets, target_col, age_col, regex\n        )\n        return result\n\n    def _plot_tangent_analysis_unified(\n        self,\n        results: pd.DataFrame,\n        analysis_type: str,\n        target_col: str,\n        age_col: str,\n        regex: Optional[str] = None,\n        plotpath: Optional[pathlib.Path] = None,\n        cutoff_time_min: Optional[float] = None,\n        intersection: str = \"dormant_hf\",\n        xunit: str = \"s\",\n        time_discarded_s: float = 900,\n        ax=None,\n        # Additional data for onset intersection analysis\n        max_slopes: Optional[pd.DataFrame] = None,\n        dormant_hfs: Optional[pd.DataFrame] = None,\n        onsets: Optional[pd.DataFrame] = None,\n    ):\n        \"\"\"\n        Unified plotting method for tangent-based analysis results.\n\n        This method handles both flank tangent and onset intersection analysis,\n        with the main difference being how the slope is determined:\n        - Flank tangent: Uses averaged slope over a window\n        - Max slope: Uses single point with maximum gradient\n\n        Parameters\n        ----------\n        results : pd.DataFrame\n            Results from the analysis (tangent results for flank, onsets for max slope)\n        analysis_type : str\n            Either 'flank_tangent' or 'max_slope_onset'\n        target_col : str\n            Column name for heat flow data\n        age_col : str\n            Column name for time data\n        regex : str, optional\n            Regex to filter samples\n        plotpath : pathlib.Path, optional\n            Path to save plots\n        cutoff_time_min : float, optional\n            Cutoff time in minutes\n        intersection : str\n            Type of intersection for onset analysis ('dormant_hf' or 'abscissa')\n        xunit : str\n            Time unit for plotting\n        time_discarded_s : float\n            Time to discard for onset analysis\n        ax : matplotlib.axes.Axes, optional\n            Matplotlib axes to plot on\n        max_slopes : pd.DataFrame, optional\n            Required for onset intersection analysis\n        dormant_hfs : pd.DataFrame, optional\n            Required for onset intersection analysis with dormant_hf\n        onsets : pd.DataFrame, optional\n            Required for onset intersection analysis\n        \"\"\"\n        try:\n            if analysis_type == \"flank_tangent\":\n                self._plot_flank_tangent_unified(\n                    results, target_col, age_col, regex, plotpath, cutoff_time_min, ax\n                )\n            elif analysis_type == \"max_slope_onset\":\n                self._plot_onset_intersection_unified(\n                    results,\n                    max_slopes,\n                    dormant_hfs,\n                    target_col,\n                    age_col,\n                    regex,\n                    intersection,\n                    xunit,\n                    time_discarded_s,\n                    ax,\n                )\n            else:\n                raise ValueError(f\"Unknown analysis_type: {analysis_type}\")\n\n        except Exception as e:\n            logger.error(f\"Error plotting tangent analysis results: {e}\")\n            print(f\"Plotting failed: {e}\")\n\n    def _plot_flank_tangent_unified(\n        self,\n        results: pd.DataFrame,\n        target_col: str,\n        age_col: str,\n        regex: Optional[str] = None,\n        plotpath: Optional[pathlib.Path] = None,\n        cutoff_time_min: Optional[float] = None,\n        ax=None,\n    ):\n        \"\"\"Plot flank tangent analysis results using unified SimplePlotter.\"\"\"\n        for _, result_row in results.iterrows():\n            sample = result_row[\"sample\"]\n            sample_short = result_row[\"sample_short\"]\n\n            # Get sample data\n            sample_data = self._get_filtered_sample_data(\n                sample, age_col, cutoff_time_min=cutoff_time_min\n            )\n            if sample_data.empty:\n                continue\n\n            # Create a DataFrame with just this result for plotting\n            single_result = pd.DataFrame([result_row])\n\n            # Use unified plotting method\n            self._plotter.plot_tangent_analysis(\n                sample_data,\n                sample_short,\n                ax=ax,\n                age_col=age_col,\n                target_col=target_col,\n                cutoff_time_min=cutoff_time_min,\n                analysis_type=\"flank_tangent\",\n                tangent_results=single_result,\n                figsize=(7, 5),\n            )\n\n            self._save_and_show_plot(plotpath, f\"flank_tangent_{sample_short}.png\", ax)\n\n    def _plot_onset_intersection_unified(\n        self,\n        onsets: pd.DataFrame,\n        max_slopes: Optional[pd.DataFrame],\n        dormant_hfs: Optional[pd.DataFrame],\n        target_col: str,\n        age_col: str,\n        regex: Optional[str] = None,\n        intersection: str = \"dormant_hf\",\n        xunit: str = \"s\",\n        time_discarded_s: float = 900,\n        ax=None,\n    ):\n        \"\"\"Plot onset intersection analysis results using unified SimplePlotter.\"\"\"\n        if max_slopes is None:\n            raise ValueError(\"max_slopes required for onset intersection analysis\")\n\n        for _, onset_row in onsets.iterrows():\n            sample = onset_row[\"sample\"]\n\n            # Get sample data\n            sample_data = self._get_filtered_sample_data(\n                sample, age_col, time_discarded_s=time_discarded_s\n            )\n            if sample_data.empty:\n                continue\n\n            # Use unified plotting method\n            self._plotter.plot_tangent_analysis(\n                sample_data,\n                sample,\n                ax=ax,\n                age_col=age_col,\n                target_col=target_col,\n                analysis_type=\"onset_intersection\",\n                max_slopes=max_slopes,\n                dormant_hfs=dormant_hfs,\n                onsets=onsets,\n                intersection=intersection,\n                xunit=xunit,\n                figsize=(12, 8),\n            )\n\n            # Note: plotpath not available in this context, only show plot\n            self._save_and_show_plot(None, f\"onset_intersection_{sample}.png\", ax)\n\n    def _get_filtered_sample_data(\n        self,\n        sample: str,\n        age_col: str,\n        cutoff_time_min: Optional[float] = None,\n        time_discarded_s: Optional[float] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Get sample data with appropriate filtering applied.\n\n        This consolidates the common data filtering logic used in both analysis types.\n        \"\"\"\n        # Get sample data - handle both 'sample' and 'sample_short' columns\n        sample_data = self._data[\n            (self._data[\"sample\"] == sample)\n            | (self._data.get(\"sample_short\", \"\") == sample)\n        ]\n\n        if sample_data.empty:\n            return sample_data\n\n        # Apply cutoff time filtering\n        if cutoff_time_min is not None:\n            cutoff_seconds = cutoff_time_min * 60\n            sample_data = sample_data[sample_data[age_col] &gt;= cutoff_seconds]\n\n        # Apply time discarded filtering (for onset analysis)\n        if time_discarded_s is not None and time_discarded_s &gt; 0:\n            sample_data = sample_data[sample_data[age_col] &gt;= time_discarded_s]\n\n        return sample_data\n\n    def _save_and_show_plot(self, plotpath: Optional[pathlib.Path], filename: str, ax):\n        \"\"\"Handle plot saving and showing - common logic for both analysis types.\"\"\"\n        if plotpath:\n            plot_file = plotpath / filename\n            import matplotlib.pyplot as plt\n\n            plt.savefig(plot_file, dpi=300, bbox_inches=\"tight\")\n\n        import matplotlib.pyplot as plt\n\n        if not ax:\n            plt.show()\n\n    def _plot_flank_tangent_results(\n        self,\n        results: pd.DataFrame,\n        target_col: str,\n        age_col: str,\n        regex: Optional[str] = None,\n        plotpath: Optional[pathlib.Path] = None,\n        cutoff_time_min: Optional[float] = None,\n        ax=None,\n    ):\n        \"\"\"\n        Plot flank tangent analysis results using SimplePlotter.\n\n        This is a wrapper around the unified plotting method for backward compatibility.\n        \"\"\"\n        return self._plot_tangent_analysis_unified(\n            results=results,\n            analysis_type=\"flank_tangent\",\n            target_col=target_col,\n            age_col=age_col,\n            regex=regex,\n            plotpath=plotpath,\n            cutoff_time_min=cutoff_time_min,\n            ax=ax,\n        )\n\n    def _plot_onset_intersections(\n        self,\n        onsets: pd.DataFrame,\n        max_slopes: pd.DataFrame,\n        dormant_hfs: pd.DataFrame,\n        target_col: str,\n        age_col: str,\n        regex: Optional[str] = None,\n        intersection: str = \"dormant_hf\",\n        xunit: str = \"s\",\n        time_discarded_s: float = 900,\n        ax=None,\n    ):\n        \"\"\"\n        Plot onset intersection analysis results using SimplePlotter.\n\n        This is a wrapper around the unified plotting method for backward compatibility.\n        \"\"\"\n        return self._plot_tangent_analysis_unified(\n            results=onsets,\n            analysis_type=\"max_slope_onset\",\n            target_col=target_col,\n            age_col=age_col,\n            regex=regex,\n            intersection=intersection,\n            xunit=xunit,\n            time_discarded_s=time_discarded_s,\n            ax=ax,\n            max_slopes=max_slopes,\n            dormant_hfs=dormant_hfs,\n            onsets=onsets,\n        )\n\n    # Data manipulation methods\n    def normalize_sample_to_mass(\n        self, sample_short: str, mass_g: float, show_info: bool = True\n    ):\n        \"\"\"Normalize heat flow values to a specific mass.\"\"\"\n        self._data = DataNormalizer.normalize_sample_to_mass(\n            self._data, sample_short, mass_g, show_info\n        )\n\n    def add_metadata_source(self, file: str, sample_id_column: str):\n        \"\"\"Add metadata from external source.\"\"\"\n        # TODO: Implement metadata loading\n        logger.warning(\"add_metadata_source not yet implemented in refactored version\")\n\n    def remove_pickle_files(self):\n        \"\"\"Remove pickle cache files.\"\"\"\n        self._data_persistence.remove_pickle_files()\n\n    # Private utility methods\n    def _iter_samples(self, regex: Optional[str] = None):\n        \"\"\"Iterate over samples - compatibility method.\"\"\"\n        return SampleIterator.iter_samples(self._data, regex)\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.__init__","title":"<code>__init__(folder=None, show_info=True, regex=None, auto_clean=False, cold_start=True, processparams=None, new_code=False, processed=False)</code>","text":"<p>Initialize measurements from folder or existing data.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str or Path</code> <p>Path to folder containing experimental files</p> <code>None</code> <code>show_info</code> <code>bool</code> <p>Whether to print informative messages, by default True</p> <code>True</code> <code>regex</code> <code>str</code> <p>Regex pattern to filter files, by default None</p> <code>None</code> <code>auto_clean</code> <code>bool</code> <p>Whether to clean data automatically, by default False</p> <code>False</code> <code>cold_start</code> <code>bool</code> <p>Whether to read from files or use cached data, by default True</p> <code>True</code> <code>processparams</code> <code>ProcessingParameters</code> <p>Processing parameters, by default None. If None, the default parameters will be used</p> <code>None</code> <code>new_code</code> <code>bool</code> <p>Flag for new code features, by default False</p> <code>False</code> <code>processed</code> <code>bool</code> <p>Whether data is already processed, i.e., if a .csv file is used which was processed  by Calocem. By default False</p> <code>False</code> Source code in <code>calocem/measurement.py</code> <pre><code>def __init__(\n    self,\n    folder: Optional[Union[str, pathlib.Path]] = None,\n    show_info: bool = True,\n    regex: Optional[str] = None,\n    auto_clean: bool = False,\n    cold_start: bool = True,\n    processparams: Optional[ProcessingParameters] = None,\n    new_code: bool = False,\n    processed: bool = False,\n):\n    \"\"\"\n    Initialize measurements from folder or existing data.\n\n    Parameters\n    ----------\n    folder : str or pathlib.Path, optional\n        Path to folder containing experimental files\n    show_info : bool, optional\n        Whether to print informative messages, by default True\n    regex : str, optional\n        Regex pattern to filter files, by default None\n    auto_clean : bool, optional\n        Whether to clean data automatically, by default False\n    cold_start : bool, optional\n        Whether to read from files or use cached data, by default True\n    processparams : ProcessingParameters, optional\n        Processing parameters, by default None. If None, the default parameters will be used\n    new_code : bool, optional\n        Flag for new code features, by default False\n    processed : bool, optional\n        Whether data is already processed, i.e., if a .csv file is used which was processed  by Calocem. By default False\n    \"\"\"\n    # Initialize attributes\n    self._data = pd.DataFrame()\n    self._info = pd.DataFrame()\n    self._data_unprocessed = pd.DataFrame()\n    self._metadata = pd.DataFrame()\n    self._metadata_id = \"\"\n\n    # Store configuration\n    self._new_code = new_code\n    self._processed = processed\n\n    # Setup processing parameters\n    if not isinstance(processparams, ProcessingParameters):\n        self.processparams = ProcessingParameters()\n    else:\n        self.processparams = processparams\n\n    # Initialize components\n    self._folder_loader = FolderDataLoader(processed=processed)\n    self._data_persistence = DataPersistence()\n    self._data_cleaner = DataCleaner()\n    self._plotter = SimplePlotter()\n\n    # Load data if folder provided\n    if folder:\n        try:\n            if cold_start:\n                self._load_from_folder(folder, regex, show_info)\n            else:\n                self._load_from_cache()\n\n            if auto_clean:\n                self._auto_clean_data()\n\n        except Exception as e:\n            if show_info:\n                print(f\"Error during initialization: {e}\")\n            if auto_clean:\n                raise AutoCleanException()\n            if not cold_start:\n                raise ColdStartException()\n            raise\n\n    # Apply downsampling if requested\n    if self.processparams.downsample.apply:\n        self._apply_adaptive_downsampling()\n\n    # Information message\n    if show_info:\n        print(\"================\")\n        print(\n            \"Are you missing some samples? Try rerunning with auto_clean=True and cold_start=True.\"\n        )\n        print(\"================\")\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.add_metadata_source","title":"<code>add_metadata_source(file, sample_id_column)</code>","text":"<p>Add metadata from external source.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def add_metadata_source(self, file: str, sample_id_column: str):\n    \"\"\"Add metadata from external source.\"\"\"\n    # TODO: Implement metadata loading\n    logger.warning(\"add_metadata_source not yet implemented in refactored version\")\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_ascending_flank_tangent","title":"<code>get_ascending_flank_tangent(processparams=None, target_col='normalized_heat_flow_w_g', age_col='time_s', flank_fraction_start=0.2, flank_fraction_end=0.8, window_size=0.1, cutoff_min=None, show_plot=False, regex=None, plotpath=None, ax=None)</code>","text":"<p>Determine tangent to ascending flank of peak by averaging over sections.</p> <p>This is a wrapper around get_peak_onset_via_slope for backward compatibility. Returns only the mean slope related columns for compatibility.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_ascending_flank_tangent(\n    self,\n    processparams: Optional[ProcessingParameters] = None,\n    target_col: str = \"normalized_heat_flow_w_g\",\n    age_col: str = \"time_s\",\n    flank_fraction_start: float = 0.2,\n    flank_fraction_end: float = 0.8,\n    window_size: float = 0.1,\n    cutoff_min: Optional[float] = None,\n    show_plot: bool = False,\n    regex: Optional[str] = None,\n    plotpath: Optional[pathlib.Path] = None,\n    ax=None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Determine tangent to ascending flank of peak by averaging over sections.\n\n    This is a wrapper around get_peak_onset_via_slope for backward compatibility.\n    Returns only the mean slope related columns for compatibility.\n    \"\"\"\n    full_results = self.get_peak_onset_via_slope(\n        processparams=processparams,\n        target_col=target_col,\n        age_col=age_col,\n        cutoff_min=cutoff_min,\n        show_plot=show_plot,\n        regex=regex,\n        plotpath=plotpath,\n        ax=ax,\n        flank_fraction_start=flank_fraction_start,\n        flank_fraction_end=flank_fraction_end,\n        window_size=window_size,\n    )\n\n    if full_results.empty:\n        return full_results\n\n    # Extract only mean slope related columns for backward compatibility\n    mean_slope_cols = [\n        col\n        for col in full_results.columns\n        if col.startswith(\"mean_slope_\")\n        or col in [\"sample\", \"sample_short\", \"peak_time_s\", \"peak_value\"]\n    ]\n\n    result = full_results[mean_slope_cols].copy()\n\n    # Rename columns to match old API\n    column_mapping = {\n        \"mean_slope_onset_time_s\": \"x_intersection\",\n        \"mean_slope_value\": \"tangent_slope\",\n        \"mean_slope_time_s\": \"tangent_time_s\",\n    }\n\n    for old_name, new_name in column_mapping.items():\n        if old_name in result.columns:\n            result = result.rename(columns={old_name: new_name})\n\n    return result\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_astm_c1679_characteristics","title":"<code>get_astm_c1679_characteristics(processparams=None, individual=True, show_plot=False, ax=None, regex=None, xscale='log', xunit='s')</code>","text":"<p>Get characteristics according to ASTM C1679.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_astm_c1679_characteristics(\n    self,\n    processparams: Optional[ProcessingParameters] = None,\n    individual: bool = True,\n    show_plot: bool = False,\n    ax=None,\n    regex: Optional[str] = None,\n    xscale: str = \"log\",\n    xunit: str = \"s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Get characteristics according to ASTM C1679.\"\"\"\n    params = processparams or self.processparams\n\n    # Get peaks first\n    peaks = self.get_peaks(params, regex=regex, show_plot=False)\n\n    # Analyze ASTM characteristics\n    analyzer = ASTMC1679Analyzer(params)\n    df = analyzer.get_astm_c1679_characteristics(\n        self._data, peaks, individual, regex\n    )\n    return df\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_average_slope","title":"<code>get_average_slope(processparams=None, target_col='normalized_heat_flow_w_g', age_col='time_s', regex=None, show_plot=False, ax=None, save_path=None, xscale='log', xunit='s')</code>","text":"<p>Calculate average slope between onset and heat flow maximum.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_average_slope(\n    self,\n    processparams: Optional[ProcessingParameters] = None,\n    target_col: str = \"normalized_heat_flow_w_g\",\n    age_col: str = \"time_s\",\n    regex: Optional[str] = None,\n    show_plot: bool = False,\n    ax=None,\n    save_path: Optional[pathlib.Path] = None,\n    xscale: str = \"log\",\n    xunit: str = \"s\",\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate average slope between onset and heat flow maximum.\"\"\"\n    params = processparams or self.processparams\n\n    # Get required data\n    max_slopes = self.get_maximum_slope(\n        params, target_col, age_col, regex=regex, show_plot=False\n    )\n    onsets = self.get_peak_onset_via_max_slope(params, regex=regex, show_plot=False)\n\n    if max_slopes.empty or onsets.empty:\n        logger.warning(\"Cannot calculate average slopes - missing required data\")\n        return pd.DataFrame()\n\n    analyzer = AverageSlopeAnalyzer(params)\n    result = analyzer.get_average_slope(\n        self._data, max_slopes, onsets, target_col, age_col, regex\n    )\n    return result\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_cumulated_heat_at_hours","title":"<code>get_cumulated_heat_at_hours(processparams=None, target_h=4, **kwargs)</code>","text":"<p>Get cumulated heat flow at specific age.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_cumulated_heat_at_hours(\n    self,\n    processparams: Optional[ProcessingParameters] = None,\n    target_h: float = 4,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Get cumulated heat flow at specific age.\"\"\"\n    if \"cutoff_min\" in kwargs:\n        cutoff_min = kwargs[\"cutoff_min\"]\n        warnings.warn(\n            \"The cutoff_min parameter is deprecated. Use ProcessingParameters instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    else:\n        params = processparams or self.processparams\n        cutoff_min = params.cutoff.cutoff_min\n\n    return HeatCalculator.get_cumulated_heat_at_hours(\n        self._data, target_h, cutoff_min\n    )\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_data","title":"<code>get_data()</code>","text":"<p>Get the processed calorimetry data.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_data(self) -&gt; pd.DataFrame:\n    \"\"\"Get the processed calorimetry data.\"\"\"\n    return self._data\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_dormant_period_heatflow","title":"<code>get_dormant_period_heatflow(processparams=None, regex=None, cutoff_min=5, upper_dormant_thresh_w_g=0.002, plot_right_boundary=200000.0, prominence=0.001, show_plot=False)</code>","text":"<p>Get dormant period heat flow characteristics.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_dormant_period_heatflow(\n    self,\n    processparams: Optional[ProcessingParameters] = None,\n    regex: Optional[str] = None,\n    cutoff_min: int = 5,\n    upper_dormant_thresh_w_g: float = 0.002,\n    plot_right_boundary: float = 2e5,\n    prominence: float = 1e-3,\n    show_plot: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Get dormant period heat flow characteristics.\"\"\"\n    params = processparams or self.processparams\n\n    # Get peaks first\n    peaks = self.get_peaks(params, regex=regex, show_plot=False)\n\n    # Analyze dormant period\n    analyzer = DormantPeriodAnalyzer(params)\n    dorm_hf = analyzer.get_dormant_period_heatflow(\n        self._data, peaks, regex, upper_dormant_thresh_w_g\n    )\n\n    if not dorm_hf.empty:\n        return dorm_hf\n    else:\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_information","title":"<code>get_information()</code>","text":"<p>Get the measurement information/metadata.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_information(self) -&gt; pd.DataFrame:\n    \"\"\"Get the measurement information/metadata.\"\"\"\n    return self._info\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_mainpeak_params","title":"<code>get_mainpeak_params(processparams=None, target_col='normalized_heat_flow_w_g', age_col='time_s', show_plot=False, plot_type='mean', regex=None, plotpath=None, ax=None)</code>","text":"<p>Unified method that calculates BOTH maximum and mean slope onset analyses.</p> <p>This method performs both slope-based analysis approaches simultaneously: - Maximum slope: Uses single point with maximum gradient for onset determination - Mean slope: Uses averaged slope over flank windows for onset determination</p> <p>Both results are returned in a single DataFrame with all slope values and onsets.</p> <p>Parameters:</p> Name Type Description Default <code>processparams</code> <code>ProcessingParameters</code> <p>Processing parameters, by default None</p> <code>None</code> <code>target_col</code> <code>str</code> <p>Column containing heat flow data. The default is 'normalized_heat_flow_w_g'.</p> <code>'normalized_heat_flow_w_g'</code> <code>age_col</code> <code>str</code> <p>Column containing time data. The default is 'time_s'.</p> <code>'time_s'</code> <code>show_plot</code> <code>bool</code> <p>Whether to plot the results</p> <code>False</code> <code>plot_type</code> <code>str</code> <p>Type of plot to show: 'max', 'mean', - 'max': Shows only maximum slope analysis plot - 'mean': Shows only mean slope (flank tangent) analysis plot</p> <code>'mean'</code> <code>regex</code> <code>str</code> <p>Regex to filter samples</p> <code>None</code> <code>plotpath</code> <code>Path</code> <p>Path to save plots</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>Matplotlib axes to plot on</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Comprehensive DataFrame with both max and mean slope results including: - Gradients and curvatures at max slope - Gradients of mean slope - Onset times from both methods - Normalized heat flow and heat values at key points - Dormant period heat flow values - ASTM C1679 characteristic values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; measurement = Measurement(folder=\"data/\")\n&gt;&gt;&gt; mainpeak_params = measurement.get_mainpeak_params(\n...     processparams=ProcessingParameters(),\n...     show_plot=False,\n...     plot_type=\"mean\",\n... )\n</code></pre> Source code in <code>calocem/measurement.py</code> <pre><code>def get_mainpeak_params(\n    self,\n    processparams: Optional[ProcessingParameters] = None,\n    target_col: str = \"normalized_heat_flow_w_g\",\n    age_col: str = \"time_s\",\n    show_plot: bool = False,\n    plot_type: str = \"mean\",\n    regex: Optional[str] = None,\n    plotpath: Optional[pathlib.Path] = None,\n    ax=None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Unified method that calculates BOTH maximum and mean slope onset analyses.\n\n    This method performs both slope-based analysis approaches simultaneously:\n    - Maximum slope: Uses single point with maximum gradient for onset determination\n    - Mean slope: Uses averaged slope over flank windows for onset determination\n\n    Both results are returned in a single DataFrame with all slope values and onsets.\n\n    Parameters\n    ----------\n    processparams : ProcessingParameters, optional\n        Processing parameters, by default None\n    target_col : str\n        Column containing heat flow data. The default is 'normalized_heat_flow_w_g'.\n    age_col : str\n        Column containing time data. The default is 'time_s'.\n    show_plot : bool\n        Whether to plot the results\n    plot_type : str\n        Type of plot to show: 'max', 'mean',\n        - 'max': Shows only maximum slope analysis plot\n        - 'mean': Shows only mean slope (flank tangent) analysis plot\n    regex : str, optional\n        Regex to filter samples\n    plotpath : pathlib.Path, optional\n        Path to save plots\n    ax : matplotlib.axes.Axes, optional\n        Matplotlib axes to plot on\n\n    Returns\n    -------\n    pd.DataFrame\n        Comprehensive DataFrame with both max and mean slope results including:\n        - Gradients and curvatures at max slope\n        - Gradients of mean slope\n        - Onset times from both methods\n        - Normalized heat flow and heat values at key points\n        - Dormant period heat flow values\n        - ASTM C1679 characteristic values\n\n    Examples\n    --------\n    &gt;&gt;&gt; measurement = Measurement(folder=\"data/\")\n    &gt;&gt;&gt; mainpeak_params = measurement.get_mainpeak_params(\n    ...     processparams=ProcessingParameters(),\n    ...     show_plot=False,\n    ...     plot_type=\"mean\",\n    ... )\n    \"\"\"\n    params = processparams or self.processparams\n\n    max_slope_results = self._calculate_max_slope_analysis(\n        params,\n        target_col,\n        age_col,\n        regex,\n    )\n\n    mean_slope_results = self._calculate_mean_slope_analysis(\n        params,\n        target_col,\n        age_col,\n        regex,\n    )\n\n    dormant_minimum_heatflow = self.get_dormant_period_heatflow(\n        params, regex, show_plot=False\n    )\n\n    astm_values = self.get_astm_c1679_characteristics(params, individual=True, show_plot=False, regex=regex)\n\n    # Merge results into comprehensive DataFrame\n    combined_results = self._merge_slope_results(\n        max_slope_results, mean_slope_results, dormant_minimum_heatflow, astm_values\n    )\n\n    # Plot if requested\n    if show_plot and not (mean_slope_results.empty or max_slope_results.empty):\n        self._plot_combined_slope_analysis(\n            combined_results,\n            params,\n            target_col,\n            age_col,\n            plot_type,\n            regex,\n            plotpath,\n            ax,\n        )\n        if not ax:\n            plt.show()\n    elif mean_slope_results.empty:\n        # logger.warning(\"No slope analysis results to plot.\")\n        print(\"No mean slope analysis obtained - check the processing parameters.\")\n\n    elif max_slope_results.empty:\n        print(\n            \"No maximum slope analysis obtained - check the processing parameters.\"\n        )\n\n    return combined_results\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_maximum_slope","title":"<code>get_maximum_slope(processparams=None, target_col='normalized_heat_flow_w_g', age_col='time_s', time_discarded_s=900, show_plot=False, show_info=True, exclude_discarded_time=False, regex=None, read_start_c3s=False, ax=None, save_path=None, xscale='linear', xunit='s')</code>","text":"<p>Find the point in time of the maximum slope.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_maximum_slope(\n    self,\n    processparams: Optional[ProcessingParameters] = None,\n    target_col: str = \"normalized_heat_flow_w_g\",\n    age_col: str = \"time_s\",\n    time_discarded_s: float = 900,\n    show_plot: bool = False,\n    show_info: bool = True,\n    exclude_discarded_time: bool = False,\n    regex: Optional[str] = None,\n    read_start_c3s: bool = False,\n    ax=None,\n    save_path: Optional[pathlib.Path] = None,\n    xscale: str = \"linear\",\n    xunit: str = \"s\",\n):\n    \"\"\"Find the point in time of the maximum slope.\"\"\"\n    params = processparams or self.processparams\n\n    time_discarded_s = (\n        params.cutoff.cutoff_min * 60 if params.cutoff.cutoff_min else 0\n    )\n    analyzer = SlopeAnalyzer(params)\n\n    result = analyzer.get_maximum_slope(\n        self._data,\n        target_col,\n        age_col,\n        time_discarded_s,\n        exclude_discarded_time,\n        regex,\n        # read_start_c3s,\n        # self._metadata,\n    )\n\n    if show_plot and not result.empty:\n        for sample, sample_data in SampleIterator.iter_samples(self._data, regex):\n            sample_short = pathlib.Path(str(sample)).stem\n            sample_result = result[result[\"sample_short\"] == sample_short]\n            sample_result = sample_result[\n                sample_result[age_col] &gt;= time_discarded_s\n            ]\n            if not sample_result.empty:\n                self._plotter.plot_slopes(\n                    sample_data,\n                    sample_result,\n                    str(sample_short),\n                    ax,\n                    age_col,\n                    target_col,\n                )\n\n    return result\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_metadata","title":"<code>get_metadata()</code>","text":"<p>Get added metadata and the ID column name.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_metadata(self) -&gt; tuple:\n    \"\"\"Get added metadata and the ID column name.\"\"\"\n    return self._metadata, self._metadata_id\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_peak_onset_via_max_slope","title":"<code>get_peak_onset_via_max_slope(processparams=None, show_plot=False, ax=None, regex=None, age_col='time_s', target_col='normalized_heat_flow_w_g', time_discarded_s=900, save_path=None, xscale='linear', xunit='s', intersection='dormant_hf')</code>","text":"<p>Get reaction onset via maximum slope intersection method.</p> <p>This is a wrapper around get_peak_onset_via_slope for backward compatibility. Returns only the max slope related columns for compatibility.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_peak_onset_via_max_slope(\n    self,\n    processparams: Optional[ProcessingParameters] = None,\n    show_plot: bool = False,\n    ax=None,\n    regex: Optional[str] = None,\n    age_col: str = \"time_s\",\n    target_col: str = \"normalized_heat_flow_w_g\",\n    time_discarded_s: float = 900,\n    save_path: Optional[pathlib.Path] = None,\n    xscale: str = \"linear\",\n    xunit: str = \"s\",\n    intersection: str = \"dormant_hf\",\n):\n    \"\"\"\n    Get reaction onset via maximum slope intersection method.\n\n    This is a wrapper around get_peak_onset_via_slope for backward compatibility.\n    Returns only the max slope related columns for compatibility.\n    \"\"\"\n    full_results = self.get_mainpeak_params(\n        processparams=processparams,\n        target_col=target_col,\n        age_col=age_col,\n        show_plot=show_plot,\n        regex=regex,\n        ax=ax,\n        plot_type=\"max\",\n\n        #time_discarded_s=time_discarded_s,\n        #intersection=intersection,\n        #xunit=xunit,\n    )\n\n    if full_results.empty:\n        return full_results\n\n    # Extract only max slope related columns for backward compatibility\n    # max_slope_cols = [\n    #     col\n    #     for col in full_results.columns\n    #     if col.startswith(\"max_slope_\") or col in [\"sample\", \"sample_short\"]\n    # ]\n\n    # result = full_results[max_slope_cols].copy()\n\n    # Rename columns to match old API\n    # column_mapping = {\n    #     \"onset_time_s_from_max_slope\": \"onset_time_s\",\n    #     \"max_slope_onset_time_min\": \"onset_time_min\",\n    #     \"max_slope_value\": \"maximum_slope\",\n    #     \"max_slope_time_s\": \"maximum_slope_time_s\",\n    # }\n\n    # for old_name, new_name in column_mapping.items():\n    #     if old_name in result.columns:\n    #         result = result.rename(columns={old_name: new_name})\n\n    return full_results\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_peak_onsets","title":"<code>get_peak_onsets(target_col='normalized_heat_flow_w_g', age_col='time_s', time_discarded_s=900, rolling=1, gradient_threshold=0.0005, show_plot=False, exclude_discarded_time=False, regex=None, ax=None)</code>","text":"<p>Get peak onsets based on gradient threshold.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_peak_onsets(\n    self,\n    target_col: str = \"normalized_heat_flow_w_g\",\n    age_col: str = \"time_s\",\n    time_discarded_s: float = 900,\n    rolling: int = 1,\n    gradient_threshold: float = 0.0005,\n    show_plot: bool = False,\n    exclude_discarded_time: bool = False,\n    regex: Optional[str] = None,\n    ax=None,\n):\n    \"\"\"Get peak onsets based on gradient threshold.\"\"\"\n    analyzer = OnsetAnalyzer(self.processparams)\n    return analyzer.get_peak_onsets(\n        self._data,\n        target_col,\n        age_col,\n        time_discarded_s,\n        rolling,\n        gradient_threshold,\n        exclude_discarded_time,\n        regex,\n    )\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_peaks","title":"<code>get_peaks(processparams=None, target_col='normalized_heat_flow_w_g', regex=None, cutoff_min=None, show_plot=True, plt_right_s=200000.0, plt_top=0.01, ax=None, xunit='s', plot_labels=None, xmarker=False)</code>","text":"<p>Get DataFrame of peak characteristics.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_peaks(\n    self,\n    processparams: Optional[ProcessingParameters] = None,\n    target_col: str = \"normalized_heat_flow_w_g\",\n    regex: Optional[str] = None,\n    cutoff_min: Optional[float] = None,  # Deprecated parameter\n    show_plot: bool = True,\n    plt_right_s: float = 2e5,\n    plt_top: float = 1e-2,\n    ax=None,\n    xunit: str = \"s\",\n    plot_labels: Optional[bool] = None,\n    xmarker: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Get DataFrame of peak characteristics.\"\"\"\n    if cutoff_min is not None:\n        warnings.warn(\n            \"The cutoff_min parameter is deprecated. Use ProcessingParameters instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    params = processparams or self.processparams\n    analyzer = PeakAnalyzer(params)\n    peaks_df = analyzer.get_peaks(self._data, target_col, regex)\n\n    if show_plot and not peaks_df.empty:\n        # Simple plotting implementation\n        for sample, sample_data in SampleIterator.iter_samples(self._data, regex):\n            sample_peaks = peaks_df[\n                peaks_df[\"sample_short\"] == pathlib.Path(str(sample)).stem\n            ]\n            if not sample_peaks.empty:\n                # Get peak indices relative to sample data\n                import numpy as np\n\n                peak_indices = np.array(sample_peaks.index.tolist())\n                self._plotter.plot_peaks(\n                    sample_data, peak_indices, str(sample), ax, \"time_s\", target_col\n                )\n\n    return peaks_df\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.get_sample_names","title":"<code>get_sample_names()</code>","text":"<p>Get list of sample names.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def get_sample_names(self) -&gt; list:\n    \"\"\"Get list of sample names.\"\"\"\n    return [\n        pathlib.Path(str(sample)).stem\n        for sample, _ in SampleIterator.iter_samples(self._data)\n    ]\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.normalize_sample_to_mass","title":"<code>normalize_sample_to_mass(sample_short, mass_g, show_info=True)</code>","text":"<p>Normalize heat flow values to a specific mass.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def normalize_sample_to_mass(\n    self, sample_short: str, mass_g: float, show_info: bool = True\n):\n    \"\"\"Normalize heat flow values to a specific mass.\"\"\"\n    self._data = DataNormalizer.normalize_sample_to_mass(\n        self._data, sample_short, mass_g, show_info\n    )\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.plot","title":"<code>plot(t_unit='h', y='normalized_heat_flow_w_g', y_unit_milli=True, regex=None, show_info=True, ax=None)</code>","text":"<p>Plot the calorimetry data.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def plot(\n    self,\n    t_unit: str = \"h\",\n    y: str = \"normalized_heat_flow_w_g\",\n    y_unit_milli: bool = True,\n    regex: Optional[str] = None,\n    show_info: bool = True,\n    ax=None,\n):\n    \"\"\"Plot the calorimetry data.\"\"\"\n    return self._plotter.plot_data(\n        self._data, t_unit, y, y_unit_milli, regex, show_info, ax\n    )\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.plot_by_category","title":"<code>plot_by_category(categories, t_unit='h', y='normalized_heat_flow_w_g', y_unit_milli=True)</code>","text":"<p>Plot data by metadata categories.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def plot_by_category(\n    self,\n    categories: str,\n    t_unit: str = \"h\",\n    y: str = \"normalized_heat_flow_w_g\",\n    y_unit_milli: bool = True,\n):\n    \"\"\"Plot data by metadata categories.\"\"\"\n    # Simplified implementation - would need full metadata integration\n    logger.warning(\n        \"plot_by_category requires metadata integration - not fully implemented\"\n    )\n    yield from []\n</code></pre>"},{"location":"reference.html#calocem.measurement.Measurement.remove_pickle_files","title":"<code>remove_pickle_files()</code>","text":"<p>Remove pickle cache files.</p> Source code in <code>calocem/measurement.py</code> <pre><code>def remove_pickle_files(self):\n    \"\"\"Remove pickle cache files.\"\"\"\n    self._data_persistence.remove_pickle_files()\n</code></pre>"},{"location":"tian.html","title":"Tian Correction","text":""},{"location":"tian.html#example-workflow","title":"Example Workflow","text":""},{"location":"tian.html#load-the-data","title":"Load the Data","text":"<p>For fast reactions, e.g., reactions occuring during the first minutes of cement hydration, the thermal inertia of the calorimeter significantly broadens the heat flow signal.  If the characteristic time constants are determined experimentally, a Tian correction can be applied to the heat flow data.</p> <p>First we load the data.</p> <pre><code>from pathlib import Path\n\nimport matplotlib.pyplot as plt\n\nimport calocem.tacalorimetry as ta\nfrom calocem.processparams import ProcessingParameters\n\ndatapath = Path(__file__).parent / \"calo_data\"\n\n# load experimental data\ntam = ta.Measurement(\n    folder=datapath,\n    regex=r\".*file1.csv\",\n    show_info=True,\n    auto_clean=False,\n    cold_start=True,\n)\n</code></pre> <p>In the next step, we need to define a few parameters which are necessary for the Tian correction.  Therefore we create a ProcessingParameters object which we call <code>processparams</code> in this case. The <code>processparams</code> object has a number of attributes which we can define. First, we define two time constants <code>tau1</code> and <code>tau2</code>. The numeric value needs to be determined experimentally.</p> <pre><code># Set Proceesing Parameters\nprocessparams = ProcessingParameters()\nprocessparams.time_constants.tau1 = 240\nprocessparams.time_constants.tau2 = 80\nprocessparams.median_filter.apply = True\nprocessparams.median_filter.size = 15\nprocessparams.spline_interpolation.apply = True\nprocessparams.spline_interpolation.smoothing_1st_deriv = 1e-10\nprocessparams.spline_interpolation.smoothing_2nd_deriv = 1e-10\n</code></pre> <p>Next we apply the Tian correction by calling the method <code>apply_tian_correction()</code>. We pass the <code>processparams</code> object defined above to the method.</p> <pre><code># apply tian correction\ntam.apply_tian_correction(\n    processparams=processparams,\n)\n</code></pre> <p>Finally, we can get the Pandas dataframe containing the calorimetric data by calling <code>get_data()</code>. Using the <code>df</code> DataFrame we can plot the calorimetry data using well-known Matplotlib methods.</p> <pre><code>df = tam.get_data()\n\n# plot corrected and uncorrected data\nfig, ax = plt.subplots()\nax.plot(\n    df[\"time_s\"] / 60,\n    df[\"normalized_heat_flow_w_g\"],\n    linestyle=\"--\",\n    label=\"sample\"\n    )\nax.plot(\n    df[\"time_s\"] / 60,\n    df[\"normalized_heat_flow_w_g_tian\"],\n    color=ax.get_lines()[-1].get_color(),\n    label=\"Tian corrected\"\n    )\nax.set_xlim(0, 15)\nax.set_xlabel(\"Time (min)\")\nax.set_ylabel(\"Normalized Heat Flow (W/g)\")\nax.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"tian.html#one-or-two-tau-values","title":"One or Two Tau Values","text":"<p>If only one Tau value is defined, the correction algorithm will only consider this \\(\\tau\\) value and the data will be corrected according to </p> \\[ P(t) = \\varepsilon \\left[ U(t) + \\tau \\frac{dU(t)}{dt} \\right] \\] <p>If two values for \\(\\tau\\) are provided, the data will be corrected considering both values.</p> \\[ P(t) = \\varepsilon \\left[ U(t) + (\\tau_1+\\tau_2) \\frac{dU(t)}{dt} + \\tau_1\\tau_2 \\frac{d^2U}{dt^2} \\right] \\] <p>The actual implementation of the correction algorithm is not based on the voltage \\(U\\) but on the heat flow.  In most cases, the exported data does not contain the raw voltage data but the heat flow data which has been obtained in the instrument software with the experimentally determined value for \\(\\varepsilon\\).</p> <p>Therefore, the second equation reads like</p> \\[ \\dot{Q}_{Tian}(t) =  \\dot{Q}(t) + (\\tau_1+\\tau_2) \\frac{\\dot{Q}}{dt} + \\tau_1\\tau_2 \\frac{d^2\\dot{Q}}{dt^2}  \\] <p>In pratical terms, if only the attribute tau1 is set, only the first derivative of the heat flow will be considered.</p> <pre><code># Set Proceesing Parameters\nprocessparams = ProcessingParameters()\nprocessparams.time_constants.tau1 = 300\n</code></pre> <p>The difference between having one or two tau constants can be seen in the following plot. In general, having two tau constants renders the signal even more narrow.</p> <p></p>"},{"location":"tian.html#smoothing-the-data","title":"Smoothing the Data","text":""},{"location":"tian.html#no-smoothing","title":"No smoothing","text":"<p>It is important to smoothen the data.  Otherwise, small noise in the raw heat flow data will lead to significant noise especially in the second derivative. By default the smoothing is no applied.  Here, we explicitly set the attributes to <code>False</code> and repeat the analysis as shown above. The results demonstrates the noise in the data which originates from tiny fluctuations which result in significant noise in the second derivative.</p> <pre><code># Set Proceesing Parameters\nprocessparams.median_filter.apply = False\nprocessparams.spline_interpolation.apply = False\n</code></pre> <p></p>"},{"location":"tian.html#only-median-filter","title":"Only Median Filter","text":"<p>Here is the result of only applying a median filter with a size of 15.</p> <p><pre><code># Set Proceesing Parameters\nprocessparams.median_filter.apply = True\nprocessparams.spline_interpolation.apply = 15\n</code></pre> </p>"},{"location":"tian.html#only-spline-smoothing","title":"Only Spline Smoothing","text":"<p>Here is the result of only applying a Univariate Spline with smmothing of 1e-10 for both the first and the second derivative. The combination of a median filter and spline smoothing reliably delivers smooth data without introducing artifacts or significant line broadening.</p> <p><pre><code># Set Proceesing Parameters\nprocessparams.median_filter.apply = False\nprocessparams.spline_interpolation.apply = True\nprocessparams.spline_interpolation.smoothing_1st_deriv = 1e-10\nprocessparams.spline_interpolation.smoothing_2nd_deriv = 1e-10\n</code></pre> </p>"}]}